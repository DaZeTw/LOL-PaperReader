[
  {
    "query": "What is the main goal of the DeepWideSearch benchmark introduced in this paper?",
    "section": "Abstract comprehension",
    "answer": "The main goal of DeepWideSearch is to evaluate and benchmark the ability of intelligent agents to perform both **deep reasoning** (multi-hop retrieval) and **wide-scale information collection** simultaneously. It fills a major gap in current benchmarks that only test one of these dimensions separately.",
    "mysystem_ans": "The main goal of the DeepWideSearch benchmark is to evaluate the capability of information-seeking agents in performing both deep reasoning and wide-scale information collection. This benchmark addresses the critical gap in current evaluation methodologies, which have hindered the development of agents that can effectively handle complex real-world information-seeking tasks. By introducing this benchmark, the authors aim to facilitate research and improve agent architectures that can integrate depth and width in their search processes, as demonstrated by the significant challenges highlighted in their experiments, where state-of-the-art agents achieved only a 2.39% average success rate [c1][c4][c5]."
},
  {
    "query": "Briefly describe the two dataset conversion methods used to construct DeepWideSearch.",
    "section": "Methodology",
    "answer": "The authors design two complementary dataset construction methods:\n\n**Deep2Wide Conversion** – expands deep reasoning datasets (like GAIA, BrowseComp) by adding table schemas and annotations for broader information collection.\n\n**Wide2Deep Conversion** – adds multi-hop reasoning complexity to wide search datasets (like WideSearch) by replacing explicit entities with synthesized, reasoning-based sub-questions.",
    "mysystem_ans": "The two dataset conversion methods used to construct DeepWideSearch are:\n\n1. **Deep2Wide Conversion**: This method extends existing deep search benchmarks (like GAIA and BrowseComp) by augmenting their information scope through human-annotated table schemas. It aims to create questions that require wide-scale information collection while maintaining deep reasoning capabilities [c1][c2][c3].\n\n2. **Wide2Deep Conversion**: This method enhances wide search queries by replacing explicit entities with synthesized complex sub-questions that necessitate multi-hop search steps. This approach is designed to integrate deep reasoning into wide search tasks, ensuring that the questions reflect the combinatorial complexity of real-world information-seeking scenarios [c2][c3][c4]. \n\nBoth methods incorporate rigorous human validation protocols to ensure data quality and result in a curated collection of 220 questions across 15 diverse domains [c2][c3]."
},
  {
    "query": "How do current state-of-the-art agents perform on DeepWideSearch, and which model performs best?",
    "section": "Experiment",
    "answer": "Overall performance is very low, with an average success rate of only **2.39%** across systems. The **Gemini 2.5 Pro** model achieves the best performance, with a 45.27% Column-F1, 73.98% Core Entity Accuracy, and 1.82% Pass@4 success rate, outperforming most other agents including GPT-5 and Claude Sonnet 4.",
    "mysystem_ans": "Current state-of-the-art agents perform poorly on the DeepWideSearch benchmark, achieving only a 2.39% average success rate. This highlights the significant challenges they face in integrating deep reasoning and wide-scale information collection, which are essential for real-world applications such as market analysis and business development [c1][c2][c4]. \n\nAmong the agents tested, OWL (GPT-5) and WebSailor (Claude Sonnet 4) are noted for their high operational costs per query, with OWL (GPT-5) incurring an average cost of approximately $2.75 per question. Despite this high cost, many queries remain unresolved, indicating that even the most advanced models struggle with the demands of the DeepWideSearch tasks [c3][c4]. \n\nOverall, while there are improvements in core entity identification, the agents exhibit limited effectiveness in wide-scale information collection, often underperforming compared to their LLM counterparts that rely on internal knowledge [c4]."
  },
  {
    "query": "List and explain the four failure patterns identified by the authors in agent performance.",
    "section": "Error Analysis",
    "answer": "The four key failure modes are:\n\n**Lack of Reflection** – agents cannot correct or rethink after failed search steps.\n\n**Overreliance on Internal Knowledge** – they use outdated internal facts instead of retrieving fresh data.\n\n**Insufficient Retrieval** – agents miss relevant information even when visiting correct pages.\n\n**Context Overflow** – long reasoning chains exceed model context limits, truncating important details.",
    "mysystem_ans": "Current state-of-the-art agents perform poorly on the DeepWideSearch benchmark, achieving only a 2.39% average success rate. This highlights the significant challenges in integrating deep reasoning and wide-scale information collection in their operations [c1][c2][c4]. Among the agents tested, OWL (GPT-5) and WebSailor (Claude Sonnet 4) are noted for their high operational costs per query, with OWL (GPT-5) incurring an average cost of approximately $2.75 per question, yet still struggling to resolve many queries [c3][c4]. Overall, while there are improvements in core entity identification, the agents exhibit limited effectiveness in wide-scale information collection [c4]."
  },
  {
    "query": "What are the main limitations of DeepWideSearch mentioned by the authors?",
    "section": "Multi-turn reasoning test (Turn 1)",
    "answer": "They identify three main limitations:\n\n**Uneven difficulty** – Wide2Deep produces easier tasks than Deep2Wide.\n\n**Dataset–real-world gap** – the benchmark does not fully match real-world solution paths.\n\n**Manual dependency** – dataset construction still relies heavily on human annotation.",
    "mysystem_ans": "The main limitations of DeepWideSearch mentioned by the authors are:\n\n1. **Question Complexity**: The Wide2Deep construction method produces significantly easier questions compared to the Deep2Wide method, as indicated by higher Core Entity Accuracy. The authors plan to refine sub-questions to increase complexity while maintaining natural language quality [c4].\n\n2. **Alignment with Real-World Applications**: The current dataset exhibits slight differences from actual deep and wide search questions regarding solution paths. Future work aims to refine the dataset to better align with real-world scenarios [c4].\n\n3. **Scalability of Dataset Construction**: The dataset construction relies heavily on human annotation, which limits its scalability. This reliance poses challenges for expanding the dataset efficiently [c4]. \n\nThese limitations highlight areas for improvement in the DeepWideSearch benchmark to enhance its effectiveness in evaluating information-seeking agents."
  },
  {
    "query": "Based on those limitations, what improvements do the authors plan for future work?",
    "section": "Multi-turn reasoning test (Turn 2 – Follow-up Question)",
    "answer": "The authors plan to:\n\n**Refine Wide2Deep sub-questions** to increase difficulty.\n\n**Adjust dataset design** to better mirror real-world search workflows.\n\n**Develop automated data generation** and **reference-free evaluation metrics** to improve scalability and reduce human labor.",
    "mysystem_ans": "The authors plan several improvements for future work based on the limitations identified in DeepWideSearch:\n\n1. **Increase Question Complexity**: They aim to iteratively refine sub-questions in the Wide2Deep construction method to enhance question complexity while maintaining the quality of natural language. This is in response to the observation that the current method produces significantly easier questions compared to the Deep2Wide method [c2].\n\n2. **Align with Real-World Applications**: The authors intend to refine the DeepWideSearch dataset to better align with real-world deep and wide search questions, addressing the discrepancies in solution paths observed in the current dataset [c2].\n\n3. **Enhance Scalability**: Recognizing that the dataset construction heavily relies on human annotation, which limits scalability, the authors plan to explore methods to improve the scalability of dataset creation in future iterations [c2].\n\nThese improvements aim to enhance the reliability and adaptability of LLM agents in real-world applications [c1]."
  }
]