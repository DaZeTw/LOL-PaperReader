[
  {
    "doc_id": "CiteRead",
    "title": "ABSTRACT",
    "page": 1,
    "text": "When reading a scholarly paper, scientists oftentimes wish to un-\nderstand how follow-on work has built on or engages with what\nthey are reading. While a paper itself can only discuss prior work,\nsome scientific search engines can provide a list of all subsequent\nciting papers; unfortunately, they are undifferentiated and discon-\nnected from the contents of the original reference paper. In this\nwork, we introduce a novel paper reading experience that integrates\nrelevant information about follow-on work directly into a paper,\nallowing readers to learn about newer papers and see how a paper\nis discussed by its citing papers in the context of the reference\npaper. We built a tool, called CiteRead, that implements the fol-\nlowing three contributions: 1) automated techniques for selecting\nimportant citing papers, building on results from a formative study\nwe conducted, 2) an automated process for localizing commentary\nprovided by citing papers to a place in the reference paper, and\n3) an interactive experience that allows readers to seamlessly al-\nternate between the reference paper and information from citing\npapers (e.g., citation sentences), placed in the margins. Based on\na user stud",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "ABSTRACT",
    "page": 1,
    "text": "eractive experience that allows readers to seamlessly al-\nternate between the reference paper and information from citing\npapers (e.g., citation sentences), placed in the margins. Based on\na user study with 12 scientists, we found that in comparison to\nhaving just a list of citing papers and their citation sentences, the\nuse of CiteRead while reading allows for better comprehension\nand retention of information about follow-on work.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "INTRODUCTION",
    "page": 1,
    "text": "Scientific progress involves a cumulative endeavor across many\nscientists that are building on and discussing each other’s work.\nThrough the use of citations, authors of research papers situate\ntheir novel contributions in the context of previously published\nscientific literature. Citations can also be used to trace forward in\ntime to understand how a paper relates to work that came afterward.\nCiting papers can sometimes describe newer work that directly\nimproves on or uses the work described in a reference paper. Citing\npapers also provide commentary in the form of citation sentences,\nor “citances” [ 19 ], from other scientists about how the work in a\nreference paper was received by the field or where it is situated in\nlight of contemporary work. This information is valuable when a\nscientist is trying to understand the state of the art in a field or has\nencountered a relevant paper while conducting a literature review.\nWhile readers of a paper can easily make use of citations provided\nby the paper’s authors to determine prior work, few tools exist for\nunderstanding and exploring the follow-on work that came after\na reference paper was published. One set of tools is provided by\nsci",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "INTRODUCTION",
    "page": 1,
    "text": "\nby the paper’s authors to determine prior work, few tools exist for\nunderstanding and exploring the follow-on work that came after\na reference paper was published. One set of tools is provided by\nscientific search engines, such as Google Scholar, that list all the\npapers that cite a reference paper. However, these lists can be long\nand are undifferentiated, with little information about how the\nciting paper makes use of or refers to the reference paper, requiring\nthe reader to dig into each citing paper for details. Other scientific\nsearch engines, such as Semantic Scholar and Scite, 1 go further\nto extract and present the relevant citance from the citing paper\nand also categorize citances according to how they discuss the\nreference paper. For instance, Semantic Scholar groups citances\nby intent (citing as background, method, or result) [ 5 ], while Scite\ncharacterizes citances as supporting, contrasting, or just mentioning\nthe reference paper’s findings. However, the presentation of these\ncitances remains a standalone list, disconnected from the experience\nof reading the reference paper. Thus, someone reading a passage",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "INTRODUCTION",
    "page": 1,
    "text": "1 https://scite.ai",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "INTRODUCTION",
    "page": 1,
    "text": "707",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Research",
    "page": 3,
    "text": "Citations provide an important way for authors of research papers\nto characterize how they are contributing to a body of knowledge\nwithin a field. As a result, much research has analyzed and ap-\nplied citation information towards supporting researchers and the\nresearch process.\nSome researchers have analyzed citation sentences, or “citances,”\nwithin citing papers [ 19 ] as well as the broader context around\na citation. Analyzing these citation contexts can help researchers\ndetermine what other researchers think about the paper in question\nand learn about considerations that might not be discussed in the\nreference paper (e.g., limitations of the approach used). In their\nanalysis, Elkiss et al. [ 10 ] found that while many citations overlap\nto some extent with the abstracts of the papers, some citations focus\non aspects of these papers different from the abstracts. In fact, the\nset of all citances of a paper contain 20% more concepts compared\nto the abstract of the reference paper [ 9 ]. Other researchers note\nthat citances state facts more concisely [21].\nIn addition, citation contexts as well as citation network infor-\nmation can be used to analyze papers towards specific applicati",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Research",
    "page": 3,
    "text": " Other researchers note\nthat citances state facts more concisely [21].\nIn addition, citation contexts as well as citation network infor-\nmation can be used to analyze papers towards specific applications\nbeneficial to researchers, including paper summarization, paper\nrecommendation, automated review paper generation, and finding\nemerging trends.\nWhile researchers have explored applications of citation infor-\nmation to support determining whether to read a paper or getting\nthe gist of a paper without reading it, prior work has not yet focused\non using citation information to enhance the reading experience\nonce a person has decided to read a paper. In this work, we focused\non how citations can enhance the reading experience of those who\nhave already opened and are scrolling through the pages of a paper.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Localizing Citation Contexts",
    "page": 3,
    "text": "A body of NLP work has studied the problem of selecting important\ncitations [ 14 , 16 , 20 , 23 ]. A small amount of NLP work has studied\nhow to identify spans of text in the reference paper corresponding\nto the citation sentences in citing papers. In NLP, the goal of this\nwork has been to summarize the reference paper via the citation\ncontexts from citing papers; different from our work, localization\nis used in service of summarization. For example, Cohan & Go-\nharian [ 8 ] evaluate on the TAC 2014 Biomedical Summarization\nbenchmark 2 , and show that identifying spans in the reference paper\nimproves citation context-based summarization. The CL-SciSumm\nshared task [ 4 ] has run for several years, and also focuses on local-\nization in service of summarization by citing papers. Unlike our\nwork, this line of work views localization as a subtask for automatic\nsummarization; in contrast, our localization approach seeks to op-\ntimize usefulness for immediate human consumption in a novel\npaper reading interface. Thus, for example, our approach seeks to\nmatch visually salient attributes, or localize to the section level, as\nfrequently preferred by formative study participants; in contrast,",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Localizing Citation Contexts",
    "page": 3,
    "text": "2 http://www.nist.gov/tac/2014/BiomedSumm/",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Localizing Citation Contexts",
    "page": 3,
    "text": "the NLP approaches match text in the reference paper only up to a\nfiner granularity of five sentences [4].",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Digital Documents",
    "page": 3,
    "text": "A range of interfaces have been developed to aid readers of scientific\npapers and other types of digital documents. Many systems have\nexplored various augmentation techniques to improve active read-\ning experiences, such as embedding word-scale visualizations [ 12 ],\nlayering dynamic content on static images [ 17 ], and generating\non-demand visualizations [ 1 ]. Several scientific paper reading in-\nterfaces incorporate social annotation by readers, including Nota\nBene [ 26 ] and Fermat’s Library. 3 In addition, ScholarPhi [ 15 ] au-\ntomatically augments scientific papers with term definitions and\nequation diagrams, and offers navigation aides within a paper. Dif-\nferent from user-generated annotations or automated definitions\nand navigation aides, our work seeks to repurpose author-written\npaper text as a new form of paper annotations. This approach is\ncomplementary to these earlier efforts, and can also be seen as a\nbootstrapping method for social annotation, which could help to\nattract users to a platform prior to the existence of user-generated\nannotations.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "FORMATIVE STUDY",
    "page": 3,
    "text": "To understand how citations and citances might be beneficial to re-\nsearchers, we conducted a small formative study. Seven researchers\n(four doctoral students and three professional researchers) were\nrecruited through mailing lists and Slack channels to participate in\nparticipatory design sessions. Three participants identified as NLP\nresearchers, and five participants identified as HCI researchers.\nBefore the session, we assigned one of two papers for partici-\npants to read. The papers were “Generalized Fisheye Views” [ 11 ]\nand “Scientific Article Summarization Using Citation-Context and\nArticle’s Discourse Structure” [ 7 ]. We chose these two papers as\nthey cover disciplines our participants had expertise in (HCI and\nNLP, respectively) and are well-cited. Assignment was counterbal-\nanced such that some participants had to read a paper from their\nspecific discipline, while others had to read a paper outside their\ndiscipline.\nThe studies were conducted remotely via Zoom, a video con-\nferencing platform, and Figma, a collaborative design tool. Each\nsession began with a semi-structured interview about the partici-\npant’s background and reading experiences, with a focus on how\nthey u",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "FORMATIVE STUDY",
    "page": 3,
    "text": "ferencing platform, and Figma, a collaborative design tool. Each\nsession began with a semi-structured interview about the partici-\npant’s background and reading experiences, with a focus on how\nthey utilized citations and citances. Participants then took part in a\n40-minute participatory design session with the experimenter. We\nprovided the paper that they came to the session having already\nread, together with some selected citing papers, and participants\nwere asked to select citations or citances that they found useful and\nsketch how they wanted to present them on the reference paper.\nAfter the session, they were asked to organize their designs into\ngroups and rate them according to the benefits. We asked partic-\nipants to think aloud and share their screen during the sessions,\nand we recorded the audio, video, and screen share.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "FORMATIVE STUDY",
    "page": 3,
    "text": "3 https://fermatslibrary.com",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "FORMATIVE STUDY",
    "page": 3,
    "text": "709",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Findings",
    "page": 4,
    "text": "All participants mentioned that they usually use search engines\n(e.g., Google Scholar and Semantic Scholar) to look for papers that\ncite the reference paper. Their stated aim was mostly to find broadly\nsimilar but more recent papers (instead of looking for a particular\naspect or answer in the papers). Participants usually relied on the\ntitle, citation count, venue, and year to filter the list of citing papers.\nThey then used the abstract of the citing paper (and sometimes its\nintroduction) to gauge its relevance. Only one of them reported\nscanning the citances provided in Semantic Scholar as a way to\nselect relevant papers.\nParticipants also gave feedback on what kinds of citances they\nfound interesting versus uninteresting:",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Findings",
    "page": 4,
    "text": "• Participants expressed that citances were useful to under-\nstand how other researchers frame the reference paper. They\nfurther used this information to verify that they correctly\nunderstood the main point of the reference paper. Citances\nthat discuss result comparisons were noted to be particularly\nuseful.\n• Citances that discuss the limitations of the reference paper\nwere also highly appreciated. One participant noted: “Au-\nthors [of the reference paper] usually put an emphasis on\nthe novelty but not the other aspects of the work, like lim-\nitations ...” Participants also mentioned that they found it\nuseful when a citing paper claims to have a solution to the\nissue.\n• Generic citations were not judged to be particularly inter-\nesting or useful. If there was a high information overlap\nbetween the citance and the reference paper’s abstract, then\nit was likely to be unhelpful.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Findings",
    "page": 4,
    "text": "Finally, participants gave insights into what information they\nneeded to better understand a citation, including information from\nthe reference paper and from the citing paper:",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Findings",
    "page": 4,
    "text": "• Participants considered or looked for specific locations in\nthe reference paper that would be the best place to present a\ngiven citance, but when asked, they said a precise location\nwas unnecessary and maybe even a distraction—aligning at\nthe section level often seemed sufficient.\n• Several types of information about a citing paper were iden-\ntified as useful for contextualizing the citance and deciding\nwhether it was important. Particpants wanted an abstract\nor TLDR summary [ 3 ] to understand the gist of the citing\npaper. They wanted to know when each citing paper was\npublished, with the most recent work first. Publication venue\nand citation count were also deemed useful as a signal of\nthe credibility of the citing paper. Interestingly, the names\nof citing authors were not judged useful—participants felt\nthey were unlikely to recognize most names.\n• Sometimes the citing sentence did not contain the requisite\ninformation that the participant needed to understand the\ncitation. Participants declared an interest in the opportunity\nto see the whole paragraph (or maybe section) of the citing\npaper, in order to understand it. Sometimes, related figures\nor tables were also needed.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "OUR APPROACH",
    "page": 4,
    "text": "Guided by insights from our formative study, we developed\nCiteRead, a scientific paper reading interface that enhances the\nreading experience by annotating citation contexts from follow-\non work directly on the paper, thereby allowing users to seam-\nlessly alternate between the reference paper and citation con-\ntexts. CiteRead consists of three main components: an automated\nmethod for selecting citation contexts, a method for localizing se-\nlected citation contexts in the reference paper, and a web-based\nclient.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Selection Method",
    "page": 4,
    "text": "Inspired by features proposed by prior work for identifying impor-\ntant citations in scientific papers [ 16 , 20 , 23 ], we selected a similar\nfeature set to use in our selection method. Unlike prior work, we\ncalculated a score of each citance using a linear combination of\nfeatures and hand-tuned parameters instead of training a binary\nclassifier. While a learned classifier might achieve higher precision,\nthe need for a large training dataset was a barrier. Unfortunately,\nexisting datasets used by prior work were not applicable for us due\nto differences in objectives. For example, the dataset used by Valen-\nzuela et al. [ 23 ] did not consider result comparison as an important\ncitation, but our formative study had shown that researchers found\nthose highly useful. When it came to choice of features, however,\nwe relied heavily on previous machine learning models (e.g., SciB-\nert [2], SPECTER [6], and Cohan’s citation intent classifier [5]).\nIn the end, our selection approach used the following features\n(we annotate whether the feature is a positive or negative signal):",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Selection Method",
    "page": 4,
    "text": "• (F1) Total number of direct citations : The total number\nof times the citing paper cited the reference paper. (positive)\n• (F2) Small co-citations : A Boolean feature, set to one if\nthere are fewer than three other citations in the citance.\n(positive)\n• (F3) Citation appears in table or caption : A Boolean fea-\nture, set to one if at least one citation appears in a table or a\ncaption. (positive)\n• (F4) Sentence length : An ordinal feature based on the word\ncount in the citance. (positive)\n• (F5) Similarity between papers : Cosine similarity be-\ntween the SPECTER embedding of the citing paper and the\nreference paper. (positive)\n• (F6) Similarity to abstract content : The highest cosine\nsimilarity between the SciBert embedding of the citance and\nsentences in the reference paper’s abstract. (negative)\n• (F7) Age : An ordinal feature based on the age of citing paper.\n(negative)\n• (F8) Contains cue words : A Boolean feature, set to one if\nthe citance contains a signaling keyword, such as ‘however’\nor ‘whereas.’ (positive)\n• (F9) In generic sections : A Boolean feature, set to one if\nthe citance is from (or subsection of) generic sections, such\nas ‘Introduction’ or ‘Background.’ (negat",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Selection Method",
    "page": 4,
    "text": "‘however’\nor ‘whereas.’ (positive)\n• (F9) In generic sections : A Boolean feature, set to one if\nthe citance is from (or subsection of) generic sections, such\nas ‘Introduction’ or ‘Background.’ (negative)\n• (F10) In important sections : A Boolean feature, set to one\nif the citance is from (or subsection of) important sections,\nsuch as ‘Method’ or ‘Results.’ (positive)",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Selection Method",
    "page": 4,
    "text": "710",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Localization Method",
    "page": 5,
    "text": "Similarly to the selection method, we adopted an approach that\nleverages heuristics and existing pretrained machine learning mod-\nels instead of training a new model due to mismatch between\navailable datasets and our new task. We ran each selected citance\nthrough our localization method to localize them on the reference\npaper. Fig. 2 illustrates the procedure. First, if the citance contains\na number, we tried to search if the target paper also contains the\nsame number, and localized the citance there if it exists. If not, we\ntried to find the most relevant section for citances using a combina-\ntion of category classification and similarities between key terms\nand section titles. After we found the most relevant section, we\nsearched for the best sentence in the section using a combination\nof similarities between the citance’s key terms and each word in\nthe sentence and a similarity between the citance and the sentence.\nIf none of the sentences in the section passed the threshold, we\nlocalized the citance near the section title instead.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Interface",
    "page": 5,
    "text": "CiteRead’s user interface was implemented as an extension of\nthe ScholarPhi reader [ 15 ] using the React framework. Based on\nthe insights from our formative study, we designed CiteRead to\nemphasize: (1) reducing context switching, (2) providing sufficient\ndetails about the citation context, and (3) minimizing intrusiveness.\nWe envision that CiteRead can help researchers in both linear read-\ning experiences and information foraging scenarios. CiteRead’s\ninterface consists of three main components:\nAnnotation . CiteRead annotates selected citances from our se-\nlection and localization method in the margin of the reference paper\nin the reading interface (see Fig. 3a). The icon inside the annotation\nrepresents the type of associated citation. For example, a citation\nthat is comparing its own results to the results from the reference\npaper will be shown as a chart icon. The citance of the citation will\nbe shown as a tooltip when a reader hovers over an annotation,\nand the dedicated citation sidebar will be opened when a reader\nclicks on an annotation. When there are multiple citations of the\nsame type located in close proximity, CiteRead aggregates them\ninto a single stacked annotation",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Interface",
    "page": 5,
    "text": "n sidebar will be opened when a reader\nclicks on an annotation. When there are multiple citations of the\nsame type located in close proximity, CiteRead aggregates them\ninto a single stacked annotation.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Interface",
    "page": 5,
    "text": "Sidebar. Each selected citation appears as a card in the sidebar\ngrouped by the page in the reference paper it is located in. The cita-\ntion card design was inspired by our formative study and includes\nthe information that participants deemed useful. A reader can click\na citation card to scroll the paper to the annotation, and similarly\nclick an annotation to scroll the sidebar to the associated citation\ncard. To make the card as compact as possible, lengthy text in the",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Interface",
    "page": 5,
    "text": "card is truncated, but a reader can hover over the truncated text to\nsee the full text in a tooltip. The citation number (or other citation\nformat) is highlighted to help readers quickly identify where in\nthe citance is it talking about the reference paper. If a reader needs\nmore context to understand a citation, they can click the title of a\ncard to navigate to a detailed card.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Interface",
    "page": 5,
    "text": "Detailed Card. The detailed card provides more context to a\nreader by including sentences or paragraphs near the citance (i.e.,\nsentences with 50 words around the citance). The detailed card\nalso includes a TLDR [ 3 ], or an auto-generated short summary, of\nthe citing paper to help readers understand the overview context.\nIn addition to the same information shown in a citation card in\nthe sidebar, a detailed card includes a list of authors and other\nrelated metrics, such as reference count and citation velocity. When\na reader wants to explore the citing paper further, they can use\nlinks in the card to arrive at Semantic Scholar’s paper detail page\nor the paper’s PDF.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Additional Implementation Details",
    "page": 5,
    "text": "We used VILA [ 22 ] to extract text from source PDFs. We used a\npretrained SPECTER model [ 6 ] to compute embeddings through\nthe Hugging Face Sentence Transformer interface. We used Tex-\ntRank [ 18 ], a graph-based ranking model, to retrieve key phrases.\nCiteRead was implemented within the React framework as an\nextension of the open-source ScholarPhi reader [15].",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "USER STUDY",
    "page": 5,
    "text": "We conducted a lab evaluation of CiteRead to explore the potential\nutility of integrating localized citation contexts into the reading\nexperience. More specifically, we considered the use case of a sci-\nentist who is reading a paper and is interested in learning about\nrelevant follow-on papers, including how those citing papers re-\nlate to and provide commentary about the paper they are reading.\nWe conducted a within-subjects experiment where we compared\nusing CiteRead for reading a paper with citation contexts in the\nmargins to a baseline where readers have access to the paper and\na companion list of citing papers, representing the standard way\nreaders find out about follow-on work today. We also examined\nthe overall usability of CiteRead while participants were using\nthe tool during the experiment. Finally, after the study, we asked\nparticipants to discuss how they could see themselves using the tool\nin their work. We obtained Internal Review Board (IRB) approval\nprior to conducting our study.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Study Design",
    "page": 5,
    "text": "5.1.1\nParticipants. In order to reduce potential variation across\nparticipants due to differing research topics and expertise, we fo-\ncused on researchers who are current doctoral graduate students\nand working in the field of Human-Computer Interaction (HCI). We\nrecruited a total of 12 graduate students as participants, through\nuniversity and industry-affiliated mailing lists and Slack channels.\nAll participants had experience with reading and writing research\npapers in HCI as well as understanding of technical topics and\nterminology in the research papers used for this study. Participants\nwere compensated $30 (USD) via PayPal. Study sessions were one\nhour long and held remotely over Zoom.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Study Design",
    "page": 5,
    "text": "711",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Quantitative Experiment Results",
    "page": 8,
    "text": "Quantitative results from the lab experiment were strongly positive.\nAs shown in Figure 5a, participants using CiteRead scored signifi-\ncantly higher on the comprehension test (M=62.5, SD=24.2) com-\npared to Baseline (M=43.3, SD=13.7), based on a two-tailed paired\nt-test ( p = 0 . 029). There were 3 out of 12 participants who achieved\na higher score with the baseline interface, but these participants\nalso performed worse on average compared to the participants who\nachieved a better score with CiteRead (38% vs 64%). Additionally,\nparticipants reported lower cognitive load for all NASA-TLX cri-\nteria, as shown in Table 2 (2.05 points lower on average, range of\n1–20), and this difference was significant for the Performance work-\nload dimension (10.00 vs. 12.85, p = 0 . 039; lower value indicates\nincreased sense of success).\nParticipants rated the system high in terms of usability, with\naverage and median SUS scores of 78.54 and 80.0, respectively.\nFigure 6 illustrates the distribution of the participants’ answers,\nwhich was positive for all questions. Generally, all participants\nsuccessfully used CiteRead to explore the selected citations and\nnavigate around the interface with ease. W",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Quantitative Experiment Results",
    "page": 8,
    "text": "the participants’ answers,\nwhich was positive for all questions. Generally, all participants\nsuccessfully used CiteRead to explore the selected citations and\nnavigate around the interface with ease. We observed a minor\ndisruption in the system where the sidebar did not scroll to the\nassociated card when participants clicked on the same annotation\ntwice in a row, but most participants quickly noticed the issue and\ncontinued smoothly with the session.\nMore detailed analysis of test results by category (Figure 5b)\nshows that participants using CiteRead achieved higher test scores",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Quantitative Experiment Results",
    "page": 8,
    "text": "Table 2: Mean cognitive load (NASA-TLX) scores by interface,\nwith standard deviation shown as uncertainty. Lower values\nindicate lower cognitive load (better). The range of possible\nvalues is 1–20. The asterisk indicates 0.05-significance.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Quantitative Experiment Results",
    "page": 8,
    "text": "CiteRead\nBaseline\np",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Quantitative Experiment Results",
    "page": 8,
    "text": "Mental\n14.52 ± 4.12\n16.43 ± 3.25\n0.244\nTemporal\n13.33 ± 4.77\n15.24 ± 4.28\n0.305\nPerformance\n10.00 ± 3.76\n12.86 ± 3.55\n*0.039\nEffort\n12.86 ± 4.64\n14.05 ± 4.79\n0.558\nFrustration\n7.38 ± 4.30\n9.76 ± 5.64\n0.193",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Quantitative Experiment Results",
    "page": 8,
    "text": "not just overall, but also on each sub-category of questions. Un-\nexpectedly, participants using CiteRead even scored higher on\nthe Local questions (78% vs 63%), whose answers can be found in\nthe reference paper text itself without the use of citation contexts.\nUnlike the overall accuracy comparison, sub-category accuracy\ndifferences were not statistically significant.\nOverall performance aggregated across both conditions did not\nmeaningfully differ between the two papers; average test scores\nwere 53 . 37 ± 20 . 26 and 52 . 52 ± 23 . 81 for Papers 1 and 2, respec-\ntively ( p = 0 . 921). We note, however, that participants reported\nhigher cognitive load for Paper 2 for all NASA-TLX criteria, and\nthis difference is significant for the Temporal criterion (5.67 vs 4.33,\np = 0 . 047), indicating that participants felt increased time pressure.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "DISCUSSION",
    "page": 10,
    "text": "Our study results overall provide evidence of the positive benefit of\nlocalizing citation contexts of citing papers directly into the reading\nexperience of a reference paper. Not only does CiteRead make it\neasier for readers to find relevant follow-on work, the use of anno-\ntations in the margins of a reader application allows for seamless\nexploration of a reference paper along with follow-on work. From\nboth the user study as well as the formative study, we found that\nparticipants expressed a desire to answer the kinds of questions\nposed in the user study around how follow-on work discusses or\nbuilds on a reference paper. For instance, one participant in the\nuser study said:",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "DISCUSSION",
    "page": 10,
    "text": "“I definitely had those moments of wondering about\nthese types of questions, but going through the list\nin Google Scholar with only abstracts does not help\nwith the specific questions.” –P9",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "DISCUSSION",
    "page": 10,
    "text": "Unfortunately, typical methods for answering these questions\ntoday are extremely cumbersome, involving manually scanning\nthrough a long list of potentially hundreds of papers, finding the\ncitation contexts, and then interpreting them in light of the parts\nof the reference paper they discuss. Even our baseline condition\nin the user study is a step up from typical methods as we used\nour selection method to reduce the list to a more relevant and\nmanageable set. For instance, when asked how long it would take\nto answer the questions posed in our user study using existing tools,\none participant said:",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "DISCUSSION",
    "page": 10,
    "text": "“It could be very, very long, especially if it is a domain\nthat I am not familiar with. I might spend 3–5 days to\ndo a lit review to understand these aspects.” –P1",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "DISCUSSION",
    "page": 10,
    "text": "Another participant expressed that the task of answering ques-\ntions about follow-on work is so difficult without appropriate tools\nthat it may not even make sense to try:",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "DISCUSSION",
    "page": 10,
    "text": "“They might not ask these types of questions because\nthey do not have the tools to answer...The kind of\nquestions I can ask will also change depending on the\ntools accessible to me.” –P8",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "DISCUSSION",
    "page": 10,
    "text": "Thus, while we note that participants in our study found even the\nCiteRead condition to be somewhat mentally taxing and were not\ncompletely successful in answering the questions, we are bringing\nthe effort associated with a nearly impossible, potentially multi-day\ntask down to an approachable level. Through our pipeline of selec-\ntion, localization, and integration into the reading experience, we\ncan significantly reduce the manual effort required for researchers\nto tackle these important questions.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Design Considerations and Implications",
    "page": 10,
    "text": "6.1.1\nSelection versus Scale. We devised a selection process that\nsignificantly whittled down the number of citing papers to provide\nas annotations within a reference paper through artificially limiting\nto 15. This had the benefit of leading to a reading experience that\nwas not overwhelming due to a high volume of annotations in the\nmargins. While our sidebar interface provides the capability for",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Design Considerations and Implications",
    "page": 10,
    "text": "716",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "LIMITATIONS AND SOCIETAL IMPACTS",
    "page": 11,
    "text": "Our evaluation was limited in several ways. First, our controlled\nlab experiment evaluated the benefits of CiteRead for a specific\nslice of scientists (HCI researchers) on a small set of papers. While\nour results are positive, more studies are needed to determine the\ngenerality of our findings. More extensive evaluation on a large\nrange of papers is also required to determine the general effective-\nness of our automated selection and localization techniques. Our\nend-to-end system evaluation provided support for its effectiveness\nin terms of strong quantitative and qualitative results corroborating\nthe value of these features, but evaluations of each individual tech-\nnique on many papers would provide additional evidence. We also\nnote the need for longitudinal studies to better understand how\ntools like CiteRead are used outside of laboratory settings. In this\nwork, we used existing extracted data from Semantic Scholar for\nselection and localization, which were pre-processed before study\nsessions. However, in the real world, it is impossible to exhaustively\npre-process all publications, and readers are likely to encounter\ndocuments that have not been processed. Moreover, this proces",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "LIMITATIONS AND SOCIETAL IMPACTS",
    "page": 11,
    "text": "y\nsessions. However, in the real world, it is impossible to exhaustively\npre-process all publications, and readers are likely to encounter\ndocuments that have not been processed. Moreover, this process-\ning includes parsing a document to get both layout and content\ninformation, the quality of which may depend on the format of the\naccessed paper (e.g., images, PDFs, and TeX files). A potential alter-\nnative to pre-processing is to process the document in real-time,\nbut further investigation into its feasibility would be necessary.\nWe also note several potential risks associated with our approach.\nAs with any discovery tool that redirects user attention, we must be\ncognizant of possible harms this may produce. For example, draw-\ning attention to a subset of citing papers risks unfairly promoting\nsome follow-on work over others. Adding annotations to a paper\nmay divert attention away from the paper. CiteRead’s choice of\nlocalization and citation context to show may also misrepresent\nthe citing paper author’s initial intent. We believe CiteRead’s ben-\nefits merit these risks, and note several mitigating factors. While\nCiteRead does select a subset of follow-on work to supply, we\nbeliev",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "LIMITATIONS AND SOCIETAL IMPACTS",
    "page": 11,
    "text": "iting paper author’s initial intent. We believe CiteRead’s ben-\nefits merit these risks, and note several mitigating factors. While\nCiteRead does select a subset of follow-on work to supply, we\nbelieve that lowering the barrier to finding follow-on work while\nreading will result overall in increased discovery. With respect to\ndiverting attention away from the reference paper, we are encour-\naged by the lower cognitive load scores reported in the user study,",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "LIMITATIONS AND SOCIETAL IMPACTS",
    "page": 11,
    "text": "717",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "CONCLUSION AND FUTURE WORK",
    "page": 12,
    "text": "In this work, we present a system, CiteRead, that integrates infor-\nmation from follow-on work directly in the scientific paper reading\nexperience. Through a formative study, we discovered what types\nof information from citing papers scientists are interested in con-\nsuming while reading. Based on these findings, we developed novel\ntechniques to select and localize citation contexts in ways that\nsupport these discovered information needs. CiteRead provides a\nseamless interface for alternating between reading the paper and\ncommentary from follow-on work. Our quantitative and qualitative\nevaluation of CiteRead demonstrates the benefits of this approach\nfor understanding follow-on work, while reducing cognitive load.\nFinally, we synthesize a set of design considerations and implica-\ntions for future tools that integrate commentary into the reading\nexperience, through automated selection and localization.\nWe intend ultimately to deploy our system on a broad range of\npapers. Towards this end, we plan to improve the robustness of our\ncitation context selection and localization approaches by collecting\na new annotated dataset and tuning our models using supervised\nlearning. We also plan t",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "CONCLUSION AND FUTURE WORK",
    "page": 12,
    "text": "d, we plan to improve the robustness of our\ncitation context selection and localization approaches by collecting\na new annotated dataset and tuning our models using supervised\nlearning. We also plan to conduct field studies with a wider range\nof scientists, and investigate the potential engagement benefits of\nCiteRead. Finally, we are also excited to study how to integrate\nuser commentary with automatically-localized author commentary.\nWhile we have tested our approach in the scientific literature\ndomain, we are excited about the possibilities of localized discus-\nsion to augment reading experiences across a range of domains.\nFor example, news articles are often accompanied by discussion\nthreads. However, these discussion threads are not localized to rele-\nvant sections in the article; instead, they are typically located below\nthe article, in a sidebar (e.g., New York Times), or on a separate\nweb page altogether (e.g., Hacker News 4 ). We envision the appli-\ncation of integrated reading experiences like CiteRead to enable\nseamless switching between articles and automatically-localized\ncommentary for a broad range of domains.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "ACKNOWLEDGMENTS",
    "page": 12,
    "text": "We thank Marti Hearst, Andrew Head, Dongyeop Kang, Arman\nCohan, Kyle Lo, Matt Latzke, Shannon Shen, Tal August, and Ray-\nmond Fok for helpful early discussions, as well as the anonymous\nreviewers for useful feedback on the manuscript. We also thank the\nresearchers who participated in our user studies and assisted with\npiloting the system.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "[1] Sriram Karthik Badam, Zhicheng Liu, and Niklas Elmqvist. 2018. Elastic docu-\nments: Coupling text and tables through contextual visualizations for enhanced\ndocument reading. IEEE transactions on visualization and computer graphics 25, 1\n(2018), 661–671.\n[2] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language\nModel for Scientific Text. In In Proceedings of the Conference on Empirical Methods\nin Natural Language Processing .",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "4 https://news.ycombinator.com/",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "[3] Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel S. Weld. 2020. TLDR: Ex-\ntreme Summarization of Scientific Documents. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 .\n[4] Muthu Kumar Chandrasekaran, Guy Feigenblat, Eduard H. Hovy, Abhilasha\nRavichander, Michal Shmueli-Scheuer, and Anita de Waard. 2020. Overview\nand Insights from the Shared Tasks at Scholarly Document Processing 2020:\nCL-SciSumm, LaySumm and LongSumm. In Proceedings of the First Workshop on\nScholarly Document Processing .\n[5] Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019.\nStructural Scaffolds for Citation Intent Classification in Scientific Publications. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics .\n[6] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld.\n2020.\nSPECTER: Document-level Representation Learning using Citation-\ninformed Transformers. arXiv:2004.07180 [cs.CL]\n[7] Arman Cohan and Nazli Goharian. 2015. Scientific Article Summarization Us-\ning Citation-Context and Article’s Discourse Structure. In In Proceedings of the\nConference on Empirical Methods in Natural",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "] Arman Cohan and Nazli Goharian. 2015. Scientific Article Summarization Us-\ning Citation-Context and Article’s Discourse Structure. In In Proceedings of the\nConference on Empirical Methods in Natural Language Processing .\n[8] Arman Cohan and Nazli Goharian. 2017. Contextualizing citations for scientific\nsummarization using word embeddings and domain knowledge. In Proceedings\nof the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 1133–1136.\n[9] Anna Divoli, Preslav Nakov, and Marti A. Hearst. 2012. Do Peers See More in a\nPaper Than Its Authors? Advances in Bioinformatics 2012 (2012).\n[10] Aaron Elkiss, Siwei Shen, Anthony Fader, Günes Erkan, David J. States, and\nDragomir R. Radev. 2008. Blind men and elephants: What do citation summaries\ntell us about a research article? Journal of the Association for Information Science\nand Technology 59 (2008), 51–62.\n[11] George W. Furnas. 1986. Generalized fisheye views. In Proceedings of the 1986\nCHI Conference on Human Factors in Computing Systems .\n[12] Pascal Goffin, Tanja Blascheck, Petra Isenberg, and Wesley Willett. 2020. Interac-\ntion techniques for visual exploration using embedded word-sc",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "Conference on Human Factors in Computing Systems .\n[12] Pascal Goffin, Tanja Blascheck, Petra Isenberg, and Wesley Willett. 2020. Interac-\ntion techniques for visual exploration using embedded word-scale visualizations.\nIn Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems .\n1–13.\n[13] S. G. Hart and Lowell E. Staveland. 1988. Development of NASA-TLX (Task Load\nIndex): Results of Empirical and Theoretical Research. Advances in psychology\n52 (1988), 139–183.\n[14] Saeed-Ul Hassan, Anam Akram, and Peter Haddawy. 2017. Identifying Important\nCitations Using Contextual Information from Full Text. 2017 ACM/IEEE Joint\nConference on Digital Libraries (JCDL) (2017), 1–8.\n[15] Andrew Head, Kyle Lo, Dongyeop Kang, Raymond Fok, Sam Skjonsberg, Daniel S.\nWeld, and Marti A. Hearst. 2021. Augmenting Scientific Papers with Just-in-Time,\nPosition-Sensitive Definitions of Terms and Symbols. Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems .\n[16] Petr Knoth, Phil Gooch, and Kris Jack. 2017. What Others Say About This Work?\nScalable Extraction of Citation Contexts from Research Papers. In International\nConference on Theory and Practice of Digital Li",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "Knoth, Phil Gooch, and Kris Jack. 2017. What Others Say About This Work?\nScalable Extraction of Citation Contexts from Research Papers. In International\nConference on Theory and Practice of Digital Libraries .\n[17] Damien Masson, Sylvain Malacria, Edward Lank, and Géry Casiez. 2020.\nChameleon: Bringing Interactivity to Static Digital Documents. In Proceedings of\nthe 2020 CHI Conference on Human Factors in Computing Systems . 1–13.\n[18] Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Text. In In\nProceedings of the Conference on Empirical Methods in Natural Language Process-\ning .\n[19] Preslav Nakov, Ariel S. Schwartz, and Marti A. Hearst. 2004. Citances: Citation\nSentences for Semantic Analysis of Bioscience Text. In SIGIR’04 workshop on\nSearch and Discovery in Bioinformatics .\n[20] David Pride and Petr Knoth. 2017. Incidental or Influential? - Challenges in\nAutomatically Detecting Citation Importance Using Publication Full Texts. In\nInternational Conference on Theory and Practice of Digital Libraries .\n[21] Ariel S. Schwartz and Marti A. Hearst. 2006. Summarizing Key Concepts using\nCitation Sentences. In Proceedings of the Workshop on Linking Natural Language\nProc",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "and Practice of Digital Libraries .\n[21] Ariel S. Schwartz and Marti A. Hearst. 2006. Summarizing Key Concepts using\nCitation Sentences. In Proceedings of the Workshop on Linking Natural Language\nProcessing and Biology: Towards Deeper Biological Literature Analysis .\n[22] Zejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl, Daniel S. Weld, and Doug\nDowney. 2022. VILA: Improving Structured Content Extraction from Scientific\nPDFs Using Visual Layout Groups. arXiv:2106.00676 [cs.CL]\n[23] Marco Valenzuela, Vu A. Ha, and Oren Etzioni. 2015. Identifying Meaningful\nCitations. In AAAI Workshop: Scholarly Big Data .\n[24] Hongyi Wen, Julian Ramos Rojas, and Anind K. Dey. 2016. Serendipity: Finger\nGesture Recognition using an Off-the-Shelf Smartwatch. Proceedings of the 2016\nCHI Conference on Human Factors in Computing Systems .\n[25] Anbang Xu, Zhe Liu, Yufan Guo, Vibha Sinha, and Rama Akkiraju. 2017. A\nNew Chatbot for Customer Service on Social Media. Proceedings of the 2017 CHI\nConference on Human Factors in Computing Systems .\n[26] Sacha Zyto, David Karger, Mark Ackerman, and Sanjoy Mahajan. 2012. Successful\nclassroom deployment of a social document annotation system. In Proceedings of\nthe 2",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "n Factors in Computing Systems .\n[26] Sacha Zyto, David Karger, Mark Ackerman, and Sanjoy Mahajan. 2012. Successful\nclassroom deployment of a social document annotation system. In Proceedings of\nthe 2012 CHI Conference on Human Factors in Computing Systems . 1883–1892.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "REFERENCES",
    "page": 12,
    "text": "718",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Service on Social Media”",
    "page": 13,
    "text": "Q1: How do alternative approaches compare against the approach\nused in this paper? Select one or more statements that were made,\neither in this paper or in a citing paper.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Service on Social Media”",
    "page": 13,
    "text": "• GRU models outperform the current work’s (baseline) LSTM\nmodel with respect to BLEU score\n• Convolutional Neural Network yields similar result to the\ncurrent work’s LSTM model for BLEU score\n• Bidirectional Long Short-term Memory encoder with\nattention-based architecture gets better results compared\nto plain LSTM encoder-decoder used in this paper\n• Human responses still outperform generated responses on\nappropriateness in this paper, but the responses generated\nby a tone-aware chatbot are perceived as appropriate as the\nresponses by human agents\n• There was no statistically significant difference between\nthis paper’s approach and human agents on empathy for\nemotional requests\n• Chatbots based on GRU models have shown better evaluation\nresults than human’s responses on attentiveness.\nQ2: In contrast to the dataset used in this paper to train chatbots,\nwhat have other papers tried to use as datasets instead? Select one\nor more answers.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Service on Social Media”",
    "page": 13,
    "text": "• Twitter conversations\n• Mail threads of DBpedia\n• Extracted data from mobile apps\n• Ubuntu dialogue corpus\n• The NPS Chat corpus\nQ3: In contrast to the factors used in this paper for human evalua-\ntion, what factors do other papers use to do human evaluation of\nchatbots? Select one or more answers.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Service on Social Media”",
    "page": 13,
    "text": "• Humor\n• Helpfulness\n• Flexibility\n• Attentiveness\n• Perceived humanness",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Smartwatch”",
    "page": 13,
    "text": "Q1: In contrast to the sensors used in this paper, what other types\nof sensors have been used in gesture recognition systems? Select\none or more answers, and specify whether the the sensors are in\nthis paper or in a citing paper.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Smartwatch”",
    "page": 13,
    "text": "• PPG\n• Sonar Sensors\n• EKG\n• EMG\n• Electrical Impedance Tomography",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Smartwatch”",
    "page": 13,
    "text": "Q2: What are the limitations of this paper’s approach? Select one or\nmore statements that were made, either in this paper or in a citing\npaper.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Smartwatch”",
    "page": 13,
    "text": "• While the average accuracy is high (87%), it might be imprac-\ntical as a commercial solution to require users to provide a\nlot of instances upfront to train the model.\n• This paper only explored the feasibility of using motion\nsensors when the user is not moving.\n• The gesture set is fixed with limited gestural vocabulary and\nnot easily modifiable.\n• Leveraging around-device position information from acous-\ntic processing techniques could induce noisy feedback that\ninterferes with gesture detection.\n• Accelerometers can only be used for coarse hand gestures.\nTraining data is extremely difficult to collect.\nQ3: Which of these statements have been made about evaluation\nof gesture recognition? Select one or more statements that were\nmade, either in this paper or in a citing paper.",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Smartwatch”",
    "page": 13,
    "text": "• Gesture sets vary extensively between systems, so it is chal-\nlenging to compare their results.\n• It is very common to use multiple different gesture sets in\nevaluation.\n• From the perspective of new system design and evaluation,\nit is difficult to evaluate what does the new system add in\nterms of class of gesture.\n• Besides the set of five gestures evaluated in the current work,\nsome works also examine a bending gesture.\n• False negative is more important than false positive in ges-\nture recognition evaluation",
    "images": null
  },
  {
    "doc_id": "CiteRead",
    "title": "Smartwatch”",
    "page": 13,
    "text": "719",
    "images": null
  }
]