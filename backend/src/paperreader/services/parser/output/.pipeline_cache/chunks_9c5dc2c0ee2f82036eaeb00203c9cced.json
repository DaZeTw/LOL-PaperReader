[
  {
    "doc_id": "2510.20168v1",
    "title": "Page 1",
    "page": 1,
    "text": "2025-10-24",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Width in Agentic Information Seeking",
    "page": 2,
    "text": "Tian Lan, Bin Zhu, Qianghuai Jia, Junyang Ren, Haijun Li, Longyue Wang ‚àó , Zhao Xu, Weihua Luo, Kaifu Zhang",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Alibaba International Digital Commerce",
    "page": 3,
    "text": "‚àó Corresponding Author: Longyue Wang",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Current search agents fundamentally lack the ability to simultaneously perform deep reasoning",
    "page": 4,
    "text": "over multi-hop retrieval and wide -scale information collection‚Äîa critical deficiency for real-world",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "of data, each requiring deep reasoning over multi-hop retrieval paths. Specifically, we propose two",
    "page": 5,
    "text": "methods to converse established datasets, resulting in a curated collection of 220 questions spanning 15",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "depth and width search in information-seeking tasks. Furthermore, our error analysis reveals four",
    "page": 6,
    "text": "failure modes: lack of reflection, overreliance on internal knowledge, insufficient retrieval, and context\noverflow‚Äîexposing key limitations in current agent architectures. We publicly release DeepWideSearch",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "1. Introduction",
    "page": 7,
    "text": "0\n100\n200\n300\n400\n500",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Width (Table Volume)",
    "page": 8,
    "text": "1",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Width (Table Volume)",
    "page": 8,
    "text": "2",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Width (Table Volume)",
    "page": 8,
    "text": "3",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Width (Table Volume)",
    "page": 8,
    "text": "4",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Width (Table Volume)",
    "page": 8,
    "text": "5",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Width (Table Volume)",
    "page": 8,
    "text": "6",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Width (Table Volume)",
    "page": 8,
    "text": "7",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Width (Table Volume)",
    "page": 8,
    "text": "8",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Depth (Avg. Search Steps)",
    "page": 9,
    "text": "GAIA, WebWalkQA,\nBrowseComp...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Depth (Avg. Search Steps)",
    "page": 9,
    "text": "WideSearch",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Search Depth (Avg. Search Steps)",
    "page": 9,
    "text": "TriviaQA,\nHotpotQA",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWideSearch",
    "page": 10,
    "text": "High Width\nHigh Depth\nLow Width\nHigh Depth",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWideSearch",
    "page": 10,
    "text": "High Width",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWideSearch",
    "page": 10,
    "text": "Low Depth\nLow Width\nLow Depth",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWideSearch",
    "page": 10,
    "text": "Figure 1 | The comparison of existing bench-\nmarks on search width and depth.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWideSearch",
    "page": 10,
    "text": "Large Language Models (LLMs) with advanced reason-\ning capabilities [ Achiam et al. , 2023 , Liu et al. , 2024 ,\nGuo et al. , 2025 ] have driven substantial progress across\na wide range of natural language tasks. Building on\nthese advances, LLM-based agents that equipped with\nplanning, tool use, and multi-step reasoning capabili-\nties [ Xi et al. , 2025 , Gao et al. , 2025 ]‚Äîhave achieved\nstrong performance on complex real-world challenges,\nincluding computer operation [ Wang et al. , 2025 ], deep\nresearch [ Du et al. , 2025 ], and information seeking [ Mi-\nalon et al. , 2023 , Wei et al. , 2025 ].",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWideSearch",
    "page": 10,
    "text": "5o far, existing benchmarks for evaluating agents\ncan be systematically categorized along two critical\ndimensions (Figure 1 ): search width (measured by the\nnumber of information units to be searched) and search\ndepth (measured by average search steps for each unit),\nrevealing four distinct categories: (1) Low width, high\ndepth benchmarks (e.g., GAIA [ Mialon et al. , 2023 ],",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWideSearch",
    "page": 10,
    "text": "¬© 2025 Alibaba. All rights reserved",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Dimension\nDeep Search\nWide Search\nDeepWideSearch (Ours)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Task\nFind specific, hidden\nanswers, facts or entities\nBroadly collect target\nentities and attributes",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Broadly collect hidden target entities\nand attributes that required multi-hop\ndeep web browsing and deep reasoning",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Output\nA single answer\nA structured table of\nentities and attributes\nA structured table of\nentities and attributes",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Challenge\nDeep reasoning over\nmulti-step retrieval",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Accurately collect a large\nvolume of information\nabout specific entities.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Accurately collect a large volume\nof information that needed deep\nreasoning over multi-step retrieval",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Example",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "What is the best-selling EV\nmade by the top company\nbased on MoM sales growth\nin August 2025?",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Collect the Top-3 best-selling\nnew EV cars (price and range)\nof Geely Inc.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Collect Top-10 EV marker in China\nby MoM sales growth (Aug 2025)\nand its Top-3 best-selling new\nEV cars (price and range).",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Human\nDifficulty\nHigh\nMedium\nVery High",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "Figure 2 | The detailed comparison among deep search, wide search benchmarks and our proposed\nDeepWideSearch.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "BrowseComp [ Wei et al. , 2025 ]), which focus on intricate deep reasoning over multi-hop retrieval for\nsearching target answers; (2) Low width, low depth benchmarks (e.g., TriviaQA, HotpotQA), which\naddress simple fact-finding tasks; (3) High width, low depth benchmarks (e.g., WideSearch [ Wong\net al. , 2025 ] and PaSa [ He et al. , 2025 ]), which emphasize broad information collection about specific\nquestions; and critically, (4) High width, high depth tasks, which collect extensive information that\nrequired deep reasoning‚Äîa critical capability for real-world applications like comprehensive market\nanalysis and business development but entirely unaddressed by current benchmarks. For instance,\nas shown in Figure 2 , the case ‚Äúidentifying the Top-10 EV maker in China by MoM sales growth (Aug\n2025) and its Top-3 best-selling new EV cars (price and range)‚Äù exemplifies this challenge. It requires\nagent to gather a large volume of candidates, i.e. , EV makers , to fill the result table through wide-scale\nsearch, and verify each candidate by performing deep reasoning, a combinatorial complexity that\nexceeds both the scope of width-focused evaluations and the scale of depth-focused benchm",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "ugh wide-scale\nsearch, and verify each candidate by performing deep reasoning, a combinatorial complexity that\nexceeds both the scope of width-focused evaluations and the scale of depth-focused benchmarks.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "To address this critical evaluation gap, we introduce DeepWideSearch, the first benchmark\nexplicitly designed to evaluate the capability of agents in deep and wide information seeking. Since it\nis challenging to construct deep and wide search instances even with human annotation, we develop\ntwo methods for conversing established datasets: (1) Deep2Wide Conversion , which extends deep\nsearch benchmarks (e.g., GAIA and BrowseComp) by augmenting their information scope through\nhuman-annotated table schemas; and (2) Wide2Deep Conversion , which enhances wide search queries\nby replacing explicit entities with synthesized complex sub-questions that necessitate multi-hop\nsearch steps. Both approaches integrate rigorous human validation protocols to ensure data quality\nwhile maintaining the combinatorial complexity inherent in real-world information-seeking scenarios.\nThe final benchmark comprises 220 meticulously curated questions spanning 15 diverse domains,\nfeaturing both Chinese and English queries with human-verified ground truths, with 85 instances\nderived from Deep2Wide and 135 from Wide2Deep construction methods.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "We conduct comprehensive experiments across state-of-the-art LLMs and agent systems on Deep-\nWideSearch. Our results demonstrate that even the most advanced agent systems achieve only\n2.39% average success rate on DeepWideSearch, highlighting the substantial difficulty of this kind of\ninformation seeking task. Notably, while agent frameworks consistently improve core entity iden-\ntification (e.g., +15.91 absolute percentage points in Core Entity Accuracy), they exhibit limited\nefficacy in wide-scale information collection, frequently underperforming their LLMs counterparts\nusing internal knowledge. Through systematic error analysis, we identify four fundamental failure",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 2",
    "page": 11,
    "text": "2",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 3",
    "page": 12,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 3",
    "page": 12,
    "text": "modes: (1) lack of effective reflection mechanisms when encountering problematic search trajectories;\n(2) overreliance on parametric internal knowledge leading to outdated or inaccurate information;\n(3) insufficient retrieval despite accessing relevant webpages; and (4) context overflow exceeding\ncurrent agent architectue limitations. These empirical findings expose key limitations of current agent\narchitecture for the deep and wide information-seeking task. To facilitate further research in this\ncritical domain, we have publicly released the DeepWideSearch benchmark, including datasets and\nevaluation codebase.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "2.1. LLM-based Search Agents",
    "page": 13,
    "text": "The emergence of LLM-based agent systems has enabled sophisticated information-seeking capabilities,\nwith frameworks ranging from closed-source implementations (e.g., OpenAI Deep Research) to open-\nsource platforms (e.g., WebAgent [ Wu et al. , 2025b ] and Cognitive Kernel-Pro [ Fang et al. , 2025b ]).\nThese systems have demonstrated proficiency in numerous application domains, including computer-\nuse agents, deep research for complex problem investigation [ Han et al. , 2025 ], and multi-step\ninformation retrieval through tool use [ Xi et al. , 2025 ]. Among these applications, information-\nseeking agents represent a critical frontier impact real-world utility. Current research in this domain\nprimarily addresses five technical challenges: (1) agentic system architecture design [ Zhang et al. ,\n2025a , Zhou et al. , 2025a , Xia et al. , 2025 , Fang et al. , 2025a ], (2) synthetic data generation for\ncomplex scenarios [ Wu et al. , 2025a , Li et al. , 2025 , Tao et al. , 2025 ], (3) optimization techniques\nfor retrieval efficiency [ Zhang et al. , 2025b , Fan et al. , 2025 , Sun et al. , 2025 ], (4) knowledge\nmanagement for multi-hop reasoning [ Zhang et al. , 2025a , Xu et al. , 20",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "2.1. LLM-based Search Agents",
    "page": 13,
    "text": "imization techniques\nfor retrieval efficiency [ Zhang et al. , 2025b , Fan et al. , 2025 , Sun et al. , 2025 ], (4) knowledge\nmanagement for multi-hop reasoning [ Zhang et al. , 2025a , Xu et al. , 2025 ], and (5) evaluation\nmethodologies for performance assessment [ Zhuge et al. , 2025 , Gou et al. , 2025 ].",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "2.2. Benchmarks for LLM-based Agents",
    "page": 14,
    "text": "Existing evaluation frameworks for information-seeking agents primarily target two distinct capabil-\nities: (1) Depth in multi-hop reasoning, measured by benchmarks like GAIA [ Mialon et al. , 2023 ]\nand BrowseComp [ Wei et al. , 2025 ] for general complex reasoning, and domain-specific variants in\nhealthcare [ Chen et al. , 2025 ] and E-commerce [ Lyu et al. , 2025 ]; (2) Width in information collection,\nassessed by WideSearch [ Wong et al. , 2025 ] for comprehensive retrieval of atomic information, and\nPaSa [ He et al. , 2025 ] and SPAR [ Shi et al. , 2025 ] for academic literature retrieval. Crucially, no\nexisting benchmark captures the combinatorial complexity inherent in real-world information-seeking\ntasks that simultaneously demand extensive exploration (width) and intricate multi-step reasoning\n(depth). This fundamental gap in evaluation methodology has prevented meaningful progress toward\nagents capable of handling the complex real-world information-seeking. To address this limitation,\nwe propose DeepWideSearch, the first benchmark explicitly designed to evaluate the capability of\nagents in the deep and wide information-seeking task.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "3. Task Formulation",
    "page": 15,
    "text": "As shown in Figure 3 , DeepWideSearch establishes an evaluation framework that explicitly captures\nthe combinatorial complexity of real-world information-seeking tasks‚Äîrequiring agents to perform\ndeep reasoning and wide -scale information collection. The evaluation metrics (Column F1, Row F1,\nItem F1, and Success Rate) illustrated in Figure 3 will be formally described in Section 4.4 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "3. Task Formulation",
    "page": 15,
    "text": "3",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 4",
    "page": 16,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Input",
    "page": 17,
    "text": "Formally, each task in DeepWideSearch is defined as a tuple ( ùëÑ, ùê∂ ): (1) Question ùë∏ represents\na complex natural language query for deep and wide information seeking; and (2) Columns ùë™ = { ùíÑ ùíä } ùëµ\nùíä = 1\ndefine the table schema as a set of ùëÅ attributes and constraints need to be collected and verified, such\nas EV price and MoM scales growth in Figure 3 (right).",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Output",
    "page": 18,
    "text": "As shown in Figure 3 (medium), agents are required to generate a structured tabular\nresponse ùëÖ by performing wide search for gathering numerous candidates and deep search for the\nverification of each candidate.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Output",
    "page": 18,
    "text": "Web\nPages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Output",
    "page": 18,
    "text": "Web\nPages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Output",
    "page": 18,
    "text": "Web\nPages\nWeb\nPages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Xiaomi\nBYD\n...\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Wide Search for Candidates",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Company\nMoM\nCar\nPrice",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Geely\n...\n...\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "BYD\n...\n...\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "...\n...\n...\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Agents",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Please collect Top-10\nEV maker in China by",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "MoM sales growth\n(Aug 2025) and its",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Top-3 best-selling",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "EVs and prices.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Ground-truth Tables",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Web\nPages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Web\nPages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 19,
    "text": "Web\nPages\nWeb\nPages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Step 1\nStep 2\nStep N-1\nStep N",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Column",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Deep Search for Each Candidate",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Company\nMoM\nCar\nPrice",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Xiaomi\n...\n...\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "BYD\n...\n...\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "...\n...\n...\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Model-Generated Tables",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Row\nItem",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Success Rate\nEfficiency Metrics",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Complex DeepWide",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Search Question",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "...",
    "page": 20,
    "text": "Figure 3 | Task formulation of DeepWideSearch task. The evaluation metrics (highlighted in red) are\ndetailed in Section 4.4 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "4. Methodology of Dataset Construction",
    "page": 21,
    "text": "Constructing DeepWideSearch instances from scratch presents significant challenges due to the\nsubstantial human effort. To address this challenge while maintaining methodological rigor, we\npropose two methods to converse established datasets into deep and wide search questions: (1)\nDeep2Wide Conversion and (2) Wide2Deep Conversion. Both methodologies are complemented by\nhuman annotation procedures to ensure the quality.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "4.1. Convert Deep Search Datasets (Deep2Wide)",
    "page": 22,
    "text": "Existing deep search benchmarks such as GAIA [ Mialon et al. , 2023 ], BrowseComp [ Wei et al. , 2025 ]\nand BrowseComp-zh [ Zhou et al. , 2025b ] require agents to employ multi-hop web browsing and\ndeep reasoning to identify target answers. Building upon these resources, we develop the Deep2Wide\nconversion methodology by expanding the scope of searched information. As illustrated in Figure 4\n(Top), our approach follows a three-stage pipeline inspired by WideSearch [ Wong et al. , 2025 ]:\n(1) Core Entity Filtering : We sample 80 Chinese questions from BrowseComp-zh and 20 English\nquestions from BrowseComp, filtering out instances where answers are unsuitable as core entities ( e.g. ,\ndates and numerical values). For example, as shown in Figure 5 , Dan Lin is the core entity of the deep\nsearch question; (2) Table Schema Definition : Human annotators design structured table schemas\nby defining relevant information about the core entities; (3) Comprehensive Annotation : Annotators",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "4.1. Convert Deep Search Datasets (Deep2Wide)",
    "page": 22,
    "text": "4",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Deep Sub-",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Question\nsynthesis",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "DeepSearch",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Dataset",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Table\nDefinition",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Human\nAnnotation",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Deep2Wide",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Dataset",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Web\npages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "WideSearch",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Dataset",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Core\nEntity",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Filter",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Core Entity",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Extraction",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Agent-based",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Webpage\nExtension",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Human\nAnnotation",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Wied2Deep",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Dataset",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Deep2Wide Dataset Construction Pipeline",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Web\npages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Web\npages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Web\npages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Web\npages",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Question",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Fusion",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Wide2Deep Dataset Construction Pipeline",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "Figure 4 | The pipelines of our proposed Deep2Wide and Wide2Deep data construction methods.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 5",
    "page": 23,
    "text": "perform exhaustive web searches to populate the tables. Each instance requires approximately 30\nminutes of human annotation time, ensuring high-quality and verified data. Following a design\nsimilar to that of the WideSearch benchmark [ Wong et al. , 2025 ], we incorporated timestamps into\neach question to ensure that the answers remain invariant over time.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Deep Search Question : There is a Chinese athlete who has achieved remarkable success in a",
    "page": 24,
    "text": "ball sport. He was the first player in his discipline to successfully defend his title at a major\ncompetition and has won multiple world championship titles. His sport underwent rule changes\nin the early 21st century, and he became the first men‚Äôs singles Olympic champion under the\nnew rules.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Table Schema : Competition records of Dan Lin from 2010 to 2020 including the following",
    "page": 25,
    "text": "columns: Date, Tournament Name, Level, Event, Result, and Match Details (including opponent,\nscore, and win/loss outcome).",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Table Schema : Competition records of Dan Lin from 2010 to 2020 including the following",
    "page": 25,
    "text": "Figure 5 | One deep search question in BrowseComp-ZH.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "4.2. Convert Wide Search Datasets (Wide2Deep)",
    "page": 26,
    "text": "Given that WideSearch [ Wong et al. , 2025 ] represents the publicly available dataset providing human-\nannotated tabular answers for wide-scale information-seeking, we develop the Wide2Deep conversion\nmethodology to transform these wide search queries by introducing complexity in entity identification.\nThis approach reuse the valuable human-annotated table in WideSearch while enhancing the deep\nreasoning requirements. Inspired by WebWalker [ Wu et al. , 2025b ], we implement a human-in-the-\nloop pipeline (Figure 3, bottom) comprising five stages: (1) Entity Extraction : Advanced LLMs\nidentify core entities in 160 English and Chinese WideSearch questions, similar to the core entity in\nthe deep search benchmark (Figure 5 ); (2) Deep Sub-Question Synthesis : Following prior work [ Li\net al. , 2025 , Tao et al. , 2025 ], a web search agent are implemented to recursively traverse official\nwebsites about core entities and collecting their rich entity information. Then, a complex sub-question\nis generated based on these rich information, adhering to two critical constraints: (a) Uniqueness: The\nanswer to the question must be a single, well-defined entity; (b) Complexity: Direct derivatio",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "4.2. Convert Wide Search Datasets (Wide2Deep)",
    "page": 26,
    "text": "n\nis generated based on these rich information, adhering to two critical constraints: (a) Uniqueness: The\nanswer to the question must be a single, well-defined entity; (b) Complexity: Direct derivation of the",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "4.2. Convert Wide Search Datasets (Wide2Deep)",
    "page": 26,
    "text": "5",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 6",
    "page": 27,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 6",
    "page": 27,
    "text": "entity from the question must require at least one additional web search step; (3) Question Fusion :\nClaude-sonnet-4 fuses the deep sub-question with the original wide search query; and (4) Human\nAnnotation : A team of seven master‚Äôs-level annotators validates and refines the synthesized questions\nto ensure uniqueness, complexity, and linguistic naturalness. This process requires approximately 40\nminutes of human annotation per instance, maintaining the high-quality standards essential for a\nrigorous benchmark. The prompts of core entity extraction, deep sub-question synthesis and question\nfusion are placed at Appendix C .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "4.3. Data Statistics",
    "page": 28,
    "text": "Table 1 provides a comprehensive comparison of our DeepWideSearch benchmark against exist-\ning datasets across multiple dimensions. Our benchmark demonstrates significantly higher search\ncomplexity compared to prior work, with an average table volume of 414.10 information units, sub-\nstantially exceeding deep search benchmarks like GAIA and BrowseComp. Crucially, DeepWideSearch\nrequires 4.21 average search steps to identify core entities‚Äînearly 4 √ó more complex than WideSearch\n(1.24). The dataset spans 15 diverse domains, covering both English and Chinese queries, with 220\ncarefully curated instances (85 from Deep2Wide, 135 from Wide2Deep). These statistics empirically\nvalidate the deep and wide attributes of our proposed DeepWideSearch. Cases and more details about\nthe data in Table 1 can be found in Appendix A .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Lang.",
    "page": 29,
    "text": "TriviaQA [ Joshi et al. , 2017 ]\n-\n95K\n-\n1\n‚âà 1\nEN\nHotpotQA [ Yang et al. , 2018 ]\n-\n113K\n-\n1\n‚âà 2\nEN\nGAIA [ Mialon et al. , 2023 ]\n-\n103\n-\n1\n7.73\nEN\nBrowseComp [ Wei et al. , 2025 ]\n9\n1266\n126.6\n1\n-\nEN\nBrowseComp-zh [ Zhou et al. , 2025b ]\n11\n289\n26.27\n1\n-\nZH\nWideSearch [ Wong et al. , 2025 ]\n14\n200\n12.80\n450.67\n1.24\nEN,ZH",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Lang.",
    "page": 29,
    "text": "Our Proposed DeepWideSearch",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Lang.",
    "page": 29,
    "text": "Deep2Wide\n15\n85\n7.08\n247.74\n3.22\nEN,ZH\nWide2Deep\n13\n135\n10.38\n518.84\n4.55\nEN,ZH\nOverall\n15\n220\n14.67\n414.10\n4.21\nEN,ZH",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Lang.",
    "page": 29,
    "text": "Table 1 | Data statistics comparison across benchmarks. GAIA refers to the text-only split.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "4.4. Evaluation Metrics of DeepWideSearch",
    "page": 30,
    "text": "As shown in Figure 3 , we evaluate agent performance on DeepWideSearch along three complementary\naxes: Depth, Width, and Efficiency.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Depth Evaluation",
    "page": 31,
    "text": "The depth dimension evaluate the capability of agents to correctly identify target\nentities through deep reasoning over multi-hop retrieval. Following previous works [ Wei et al. , 2025 ,\nMialon et al. , 2023 ], we introduce the Column-F1 metric. As shown in Figure 3 , Column-F1 is\ncomputed as the F1 score over the unique columns in the table. These unique columns correspond to\nthe core attributes of entities ( i.e. , rows) that uniquely identify them. Therefore, Column-F1 can be\nseen as the extension of the accuracy used in established deep search benchmarks, computing the\nprecision of a group of entities (rows in the table). Higher Column-F1 scores indicate more precise\nentities identification across the entire table structure. Moreover, since our proposed two methods\ninclude the core entity of questions, we also introduce the Core Entity Accuracy (CE Acc.) , serving\nas an additional indicator of deep reasoning capability.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Depth Evaluation",
    "page": 31,
    "text": "6",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 7",
    "page": 32,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Width Evaluation",
    "page": 33,
    "text": "The width dimension measures how comprehensively and accurately the agent\nretrieves all associated information units for entities (rows in the table). Building upon the evaluation\nframework of WideSearch [ Wong et al. , 2025 ], we assess performance at three granularities: (1)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Success Rate : A binary metric indicating whether the agent‚Äôs output table exactly matches the",
    "page": 34,
    "text": "human-annotated ground truth (all rows, columns, and values identical); (2) Row-level F1 : Computes\nprecision, recall, and F1 scores at the row level (i.e., for each entity and its associated attributes),\ncapturing whether the agent retrieves complete contextual information per entity; (3) Item-level\nF1 : The finest-grained metric evaluating accuracy at the individual cell level, reflecting fidelity in\nretrieving atomic information units within the structured table.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Efficiency Evaluation",
    "page": 35,
    "text": "To address the substantial computational costs inherent in web-scale tool\nusage (including search, browsing APIs), we further evaluate system efficiency through two metrics:\n(1) Input/Output Token : The total tokens consumed during reasoning and tool calls; (2) Cost :\nEstimated cost expenditure based on standard model inference API pricing during query resolution.\nThese efficiency metrics are critical for real-world deployment considerations, particularly given the\ndemanding requirements for extensive multi-round search and browsing.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Efficiency Evaluation",
    "page": 35,
    "text": "To account for stochasticity in LLM-based agent behavior, we conduct four independent runs per\nquestion for each baseline system. For both depth and width metrics, we report three complementary\nstatistics: (1) Avg@4 : The mean performance across all four runs; (2) Max@4 : The best performance\nobserved across the four runs; and (3) Pass@4 : The proportion of questions solved successfully in at\nleast one run (only for Success Rate). This comprehensive evaluation protocol ensures robustness\nagainst sampling variance while also highlighting the system‚Äôs peak performance potential.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "5.1. Experimental Setup",
    "page": 36,
    "text": "We evaluate three kinds of baselines on our proposed DeepWideBenchmark: (1) Closed-source LLMs\n(without tool calls) : OpenAI o3-mini, GPT-4o, GPT-5, Claude-sonnet 4, Gemini 2.5 Pro and Qwen-Max;\n(2) Open-source LLMs (without tool calls) : DeepSeek-V3/R1 [ Guo et al. , 2025 , Liu et al. , 2024 ],\nKIMI-K2 [ Team et al. , 2025 ], Qwen3 series [ Yang et al. , 2025 ]; and (3) Open-source Agent Systems :\nWebSailor [ Li et al. , 2025 ], Smolagents [ Roucher et al. , 2025 ] and OWL [ Hu et al. , 2025 ] are equipped\nwith advanced GPT-5, Claude-sonnet-4 and Gemini-2.5-Pro backbone models. All agent systems\nutilized identical tools: (1) Google Search API; and (2) Webpage Visit tool. Since webpages in HTML\nformat are often very lengthy, we use the same LLM in the agents to summarize the HTML into a\nconcise summarization. The cost of this summarization process is also counted into the efficiency\nmetrics. We utilized the official API endpoints of these LLMs with their default decoding parameter\nsettings.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "5.2. Main Results",
    "page": 37,
    "text": "The complete results are presented in Table 2 . It can be found that most baselines demonstrate\nnear-zero success rates, with only WebSailor (Gemini 2.5 Pro) and WebSailor (Claude Sonnet 4)\nexceeding 1-2% in Success Rate (Avg@4), confirming the inherent complexity of simultaneously\nhandling deep reasoning and wide-scale information collection. Notably, Gemini 2.5 Pro emerges as\nthe top-performing LLM, achieving the highest Column F1 (45.27%, Avg@4), Core Entity Accuracy\n(73.98%, Avg@4), and Pass@4 Success Rate (1.82%), even outperforming several agent systems.\nThis exceptional performance indicates that Gemini 2.5 Pro possesses advanced reasoning capabilities",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "5.2. Main Results",
    "page": 37,
    "text": "7",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 8",
    "page": 38,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "CE Acc. (%)",
    "page": 39,
    "text": "Avg@4\nPass@4\nAvg@4\nMax@4\nAvg@4\nMax@4\nAvg@4\nMax@4\nAvg@4\nPass@4",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "CE Acc. (%)",
    "page": 39,
    "text": "Closed-source LLMs",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "CE Acc. (%)",
    "page": 39,
    "text": "OpenAI o3-mini\n0.0\n0.0\n3.35\n4.55\n13.59\n16.85\n27.36\n35.68\n61.59\n69.55\nGPT-5\n0.30\n1.36\n9.61\n13.42\n21.67\n28.21\n31.71\n41.05\n58.41\n72.72\nClaude Sonnet 4\n0.9\n0.9\n7.31\n8.97\n19.94\n23.38\n32.63\n40.16\n57.95\n64.09\nGemini 2.5 Pro\n0.9\n1.82\n15.42\n18.96\n32.06\n37.10",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "45.27",
    "page": 40,
    "text": "52.86",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "73.98",
    "page": 41,
    "text": "81.82\nQwen-Max\n0.0\n0.0\n4.16\n6.18\n14.32\n18.48\n28.81\n36.19\n56.02\n63.64\nGPT-4o\n0.0\n0.0\n4.18\n7.01\n11.86\n16.41\n19.66\n27.07\n54.20\n63.64",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "73.98",
    "page": 41,
    "text": "Open-source LLMs",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "73.98",
    "page": 41,
    "text": "DeepSeek-V3\n0.23\n0.45\n6.52\n9.99\n19.08\n24.32\n31.26\n39.56\n60.68\n69.09\nDeepSeek-R1\n0.28\n0.45\n10.72\n14.39\n25.01\n30.56\n38.42\n47.77\n66.93\n75.45\nKIMI-K2\n0.34\n0.91\n7.74\n11.92\n20.44\n27.54\n31.48\n41.83\n64.32\n73.18\nQwen3-235B-A22B\n0.0\n0.0\n2.94\n5.74\n12.38\n19.53\n22.03\n34.99\n52.39\n67.73\nQwen3-235B-A22B-Instruct\n0.0\n0.0\n3.50\n5.34\n13.28\n17.85\n24.64\n33.03\n56.82\n64.09\nQwen3-32B\n0.0\n0.0\n2.28\n3.67\n12.05\n16.26\n26.37\n35.97\n54.66\n66.36",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "73.98",
    "page": 41,
    "text": "Open-source Agent Framework with Advanced LLMs",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "73.98",
    "page": 41,
    "text": "OWL (Gemini 2.5 Pro)\n0.0\n0.0\n11.11\n16.93\n28.75\n41.70\n34.84\n50.39\n66.14\n81.82\nOWL (Claude sonnet 4)\n0.68\n1.36\n8.29\n14.81\n20.44\n31.65\n30.08\n45.50\n67.39\n81.82\nSmolagents (Gemini 2.5 Pro)\n0.11\n0.45\n9.01\n15.65\n18.53\n30.91\n27.39\n45.09\n60.00\n79.09\nSmolagents (Claude sonnet 4)\n0.91\n0.91\n5.06\n8.94\n14.49\n22.68\n21.60\n33.83\n62.95\n74.09\nSmolagents (GPT-5)\n0.45\n0.45\n8.18\n14.27\n20.26\n30.66\n31.83\n44.41\n66.48\n80.00\nWebSailor (Gemini 2.5 Pro)\n1.25\n2.73\n12.51\n20.49\n25.29\n39.11\n34.41\n52.69\n70.57\n81.36\nWebSailor (Claude Sonnet 4)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "42.35",
    "page": 42,
    "text": "42.01",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "54.01",
    "page": 43,
    "text": "70.91\n80.90\nWebSailor (GPT-5)\n0.34\n1.36\n10.97\n16.17\n25.96\n35.65\n37.18\n49.48\n74.32",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "85.00",
    "page": 44,
    "text": "Table 2 | Main results on our proposed DeepWideSearch benchmark.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "85.00",
    "page": 44,
    "text": "for entity identification and extensive internal knowledge for filling result tables without external\nsearch. Furthermore, we detail the performance of baselines from depth and width metrics as below.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Depth Metrics",
    "page": 45,
    "text": "Our analysis reveals that agent systems generally enhance the deep search capabilities\nof base LLMs, as evidenced by consistent improvements in Core Entity Accuracy (CE Acc.). For example,\nthe CE Acc. (Avg@4) of GPT-5 increases from 58.41% (base LLM) to 74.32% when integrated into\nWebSailor, representing a +15.91 percentage point gain. Similarly, Claude Sonnet 4 improves from\n57.95% to 70.91% under WebSailor, demonstrating the effectiveness of iterative tool calls and multi-\nstep reasoning in complex information retrieval. However, Gemini 2.5 Pro represents a notable\nexception to this trend. Upon close inspection of generated outputs, we find that Gemini 2.5 Pro in\nagent systems frequently fails due to three critical issues: (a) producing invalid markdown-formatted\ntables; (b) executing incorrect tool call APIs; and (c) incomplete task solving due to inference errors,\noccurring in 24.24% of cases on average‚Äîsubstantially higher than GPT-5 (16.36%) and Claude\nSonnet 4 (17.80%). This suggests that Gemini 2.5 Pro‚Äôs output formatting behavior becomes brittle\nwhen subjected to multi-step tool orchestration. Critically, while agent systems improve core entity\nidentification, they fail to",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Depth Metrics",
    "page": 45,
    "text": "gests that Gemini 2.5 Pro‚Äôs output formatting behavior becomes brittle\nwhen subjected to multi-step tool orchestration. Critically, while agent systems improve core entity\nidentification, they fail to consistently enhance column-level precision. For instance, the Column F1\n(Avg@4) of Claude Sonnet 4 model declines from 32.63% (base LLM) to 30.08% in OWL and 21.60%\nin Smolagents. This pattern highlights a fundamental limitation: even when agents successfully\nidentify core entities through multi-hop reasoning, current agent architectures cannot reliably collect\ncomplete entities, with their effectiveness often falling below the usage of internal knowledge in base\nLLMs.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Width Metrics",
    "page": 46,
    "text": "When evaluating width metrics that measure comprehensive information collection,\nwe observe that most agent frameworks do not significantly improve the base LLMs‚Äô wide search\ncapabilities. Only three combinations demonstrate consistent improvements across all width metrics:",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Width Metrics",
    "page": 46,
    "text": "8",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 9",
    "page": 47,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 9",
    "page": 47,
    "text": "OWL (Claude Sonnet 4), WebSailor (Claude Sonnet 4), and WebSailor (GPT-5). The remaining agents\nshow substantial performance degradation compared to their counterpart base LLMs. Beyond the\nissues specific to Gemini 2.5 Pro that described above, the Smolagents framework also consistently\nunderperforms across nearly all metrics. Our investigation reveals that Smolagents employs minimal\nreasoning before tool calls, which restricts the effectiveness of subsequent tool calls. This architectural\nconstraint prevents Smolagents from formulating precise search queries, resulting in inadequate\ninformation coverage and poor performance on width metrics.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "6. Analysis",
    "page": 48,
    "text": "In this section, we conduct several detailed analysis on Efficiency (Section 6.1 ), Tool Calls (Section 6.2 ),\nDifferences in Dataset Construction Methods (Section 6.3 ), Per-topic Performance (Section 6.4 ), and\nError Analysis (Section 6.5 ).",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "6.1. Efficiency Analysis",
    "page": 49,
    "text": "Table 3 | Average token usage and cost statistics for\nsome agents on DeepWideSearch questions.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Cost ($)",
    "page": 50,
    "text": "OWL (Gemini 2.5 Pro)\n65K\n2.5K\n‚âà 0 . 2\nOWL (GPT-5)\n1.8M\n50K\n‚âà 2 . 75\nSmolagents (Claude Sonnet 4)\n224K\n2.4K\n‚âà 2 . 14\nSmolagents (GPT-5)\n120K\n25K\n‚âà 0 . 90\nWebSailor (Gemini 2.5 Pro)\n65K\n2.5K\n‚âà 0 . 49\nWebSailor (Claude Sonnet 4)\n186.2K\n3.5K\n‚âà 1 . 40\nWebSailor (GPT-5)\n17.7K\n6.2K\n‚âà 0 . 36",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Cost ($)",
    "page": 50,
    "text": "Compared to deep search or wide search, Deep-\nWideSearch imposes significantly higher com-\nputational and operational overhead. As shown\nin Table 3 , even state-of-the-art agents incur sub-\nstantial resource costs per query. For instance,\nOWL (GPT-5) and WebSailor (Claude Sonnet 4)\nachieve average $2.75 and $1.40 per question\n‚Äî with many queries remaining unresolved de-\nspite this high cost. Due to unstable network\nconditions and tool call errors, agents often re-\nquire multiple retry attempts to complete tasks\nsuch as search, significantly increasing computational overhead‚Äîfor instance, OWL (GPT-5) incurs\nan average cost exceeding $6.8 under retry conditions. These results underscore a critical inefficiency\nin current agent architectures when tackling complex deep and wide information seeking tasks. This\nsuggests that existing systems are not yet scalable for real-world deployment of DeepWideSearch,\nmotivating future work on efficient planning, memory reuse, and adaptive resource allocation.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "6.2. Tool Calls Analysis",
    "page": 51,
    "text": "Table 4 | Average tool calls in the Web-\nSailor system.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Visit",
    "page": 52,
    "text": "WebSailor (Gemini 2.5 Pro)\n4.77\n2.68\nWebSailor (Claude Sonnet 4)\n23.23\n4.57\nWebSailor (GPT-5)\n8.72\n5.35",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Visit",
    "page": 52,
    "text": "Table 4 shows the average number of tool calls (Search and\nVisit tools) per sample across different backbone LLMs in\nWebSailor. Notably, WebSailor (Claude Sonnet 4) exhibits\na significantly higher Search tool calls (23.23) compared\nto Gemini 2.5 Pro (4.77) and GPT-5 (8.72). This aligns\nwith its superior performance (Table 2 ), suggesting that\nscaling the search tool calls improves the performance.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "6.3. Differences in Dataset Construction Methods",
    "page": 53,
    "text": "Table 5 demonstrates the average performance of advanced LLMs (GPT-5, Claude Sonnet 4 and Gemini\n2.5 Pro) with their counterpart agent systems. It can be found that the Deep2Wide construction\nmethod produces substantially more challenging data than Wide2Deep method. For example, LLMs\nand agents achieves nearly 0.0% success rate on Deep2Wide (Avg. LLMs: 0.0% Avg@4; Avg. Agents:",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "6.3. Differences in Dataset Construction Methods",
    "page": 53,
    "text": "9",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 10",
    "page": 54,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "Avg@4\nPass@4\nAvg@4\nMax@4\nAvg@4\nMax@4\nAvg@4\nMax@4\nAvg@4\nPass@4",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "Wide Search ‚Üí DeepWideSearch (Wide2Deep)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "Avg. LLMs\n1.17\n2.22\n17.23\n21.80\n38.04\n43.95\n50.94\n59.09\n90.12\n93.83\nAvg. Agents\n1.23\n2.13\n15.55\n24.13\n33.51\n46.98\n44.13\n60.70\n88.36\n96.76\nAvg. All\n1.21\n2.15\n16.00\n23.49\n34.75\n46.16\n45.96\n60.26\n88.84\n95.96",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "Deep Search ‚Üí DeepWideSearch (Deep2Wide)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "Avg. LLMs\n0.0\n0.0\n2.67\n3.92\n8.52\n13.25\n13.67\n21.81\n31.77\n46.27\nAvg. Agents\n0.15\n0.44\n3.25\n5.99\n9.21\n16.43\n13.75\n24.92\n33.86\n54.56\nAvg. All\n0.11\n0.32\n3.09\n5.42\n9.02\n15.56\n13.73\n24.07\n33.29\n52.30",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "Overall",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "Avg. LLMs\n0.72\n1.36\n11.60\n14.89\n26.64\n32.09\n36.54\n44.69\n67.58\n75.45\nAvg. Agents\n0.75\n1.25\n9.36\n14.88\n20.76\n30.60\n27.77\n40.74\n58.05\n69.89\nAvg. All\n0.74\n1.28\n9.97\n14.88\n22.36\n31.01\n30.16\n41.82\n60.65\n71.40",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "Table 5 | Performance comparison between Deep2Wide and Wide2Deep methods.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Entity Acc. (%)",
    "page": 55,
    "text": "0.15% Avg@4), compared to the Wide2Deep (Avg. LLMs: 1.17% Avg@4; Avg. Agents: 1.23% Avg@4).\nCritically, the overall Entity Accuracy on Deep2Wide is only 33.29% (vs. 88.84% on Wide2Deep).\nThis observation indicates that the synthesized deep sub-question in the Wide2Deep method is easier\nfor LLMs to solve. Nevertheless, the column-F1 of Wide2Deep remains below 51%, indicating that\ncomprehensively collecting entities is still challenging.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "6.4. Per-topic Performance Analysis",
    "page": 56,
    "text": "As shown in Figure 6 , we analyze topic-wise performance through bidirectional bar charts evaluating\ndepth metrics (Column-F1, CE Acc.) and width metrics (Item-F1, Row-F1), excluding domains with\nfewer than 5 samples. Four key patterns emerge: (1) The top-5 most frequent topics (sample count\n>20) are Film & Movies , Politics , Finance , Technology , and Sports ; (2) Politics achieves the highest\nitem- and row-level F1 scores (35% and 19%), indicating wide search are more tractable in this topic,\nwhile Politics and Finance attain the highest column F1 and CE accuracy, suggesting deep search are\ncomparatively easier here; (3) Despite strong depth performance in Finance , Travel , and Education\ntopics, the performance of baselines exhibit substantially lower width metrics on these three topics\n(e.g., Travel 20% item F1 and Finance 8% row F1), revealing that strong deep search capability does\nnot guarantee effective wide search capability; and (4) History and Games consistently underperform\nacross all metrics ( e.g. , 5% Column-F1 of History ), establishing them as the most challenging topics.\nThese findings highlight the heterogeneous nature of search complexity across topics.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "6.5. Error Analysis",
    "page": 57,
    "text": "As shown in Tables 2 , agent systems might underperform backbone LLMs on DeepWideSearch tasks.\nOur error analysis reveals four key failure patterns: (1) Lack of Reflection: agents often lack\neffective reflection mechanisms. When encountering wrong trajectories (Figures 13 ) or tool call errors\n(Figure 14 ), they prematurely conclude the task is unsolvable and output empty tables rather than\nanalyzing failure causes and exploring alternative paths; (2) Overreliance on Internal Knowledge:\nagents frequently overrely on internal knowledge. Even when correctly identifying core entities\n(Figure 15 ), they often generate tables solely using their internal parametric knowledge rather than\nperforming proper web queries, resulting in outdated or inaccurate information due to limited training\ndata scope; (3) Insufficient Retrieval: information retrieval is often insufficient. For example, despite",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "6.5. Error Analysis",
    "page": 57,
    "text": "10",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "Travel\nHistory\nMedicine",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "Games\nArt & Culture",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "Education",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "Music\nGeography",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "Sports\nTechnology",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "Politics\nFinance\nFilm & Movies",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "20%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "3%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "19%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "14%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "18%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "17%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "17%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "19%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "20%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "21%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "35%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "28%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 11",
    "page": 58,
    "text": "24%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "Item-F1",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "9%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "1%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "6%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "4%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "11%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "2%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "10%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "4%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "7%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "6%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "19%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "8%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 59,
    "text": "11%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "Row-F1",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "Travel\nHistory\nMedicine",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "Games\nArt & Culture",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "Education",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "Music\nGeography",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "Sports\nTechnology",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "Politics\nFinance\nFilm & Movies",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "27%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "5%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "27%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "24%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "24%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "28%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "24%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "29%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "28%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "29%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "43%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "49%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 60,
    "text": "32%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "Column-F1",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "85%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "11%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "56%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "26%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "55%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "81%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "57%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "53%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "57%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "68%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "86%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "88%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 61,
    "text": "59%",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 62,
    "text": "CE Acc.\nSample Count\nPerformance (%)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 62,
    "text": "Figure 6 | Per-topic analysis on two depth metrics (Column F1 and CE Acc.) and two width metrics\n(Item F1 and Row F1).",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "28",
    "page": 62,
    "text": "identifying relevant pages (Figure 17 ), agents frequently fail to properly access complete context\nthrough visit operations, leading to significant information omissions. Even when visit operations are\nexecuted correctly, summarized webpage data may still miss critical details. This limitation motivates\nthe design of a question-aware, customized webpage summarization process in agent systems; and (4)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Context Overflow: context overflow presents a fundamental challenge. Deep wide search requires",
    "page": 63,
    "text": "extensive multi-step reasoning and numerous search tool calls, significantly expanding context length\n(Figure 16 ). This issue occurred in 24.96% of cases, exceeding the context management capabilities of\ncurrent agent architectures; In summary, these four error patterns highlight that current agents face\nsubstantial limitations when addressing the challenges of depth and width in complex information-\nseeking tasks. Addressing these limitations requires specialized architecture for deep wide search\nscenarios.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "7. Conclusion",
    "page": 64,
    "text": "This paper addresses the critical gap in information-seeking agent evaluation by introducing Deep-\nWideSearch benchmark, the first benchmark designed to simultaneously assess deep reasoning and\nwide-scale information collection. Our experiments demonstrate that state-of-the-art agents achieve\nonly 2.39% average success rate on this challenging benchmark, revealing fundamental limitations\nfor current agents. These results underscore the combinatorial complexity of deep and wide search\nas a key frontier to guide future research toward more capable information-seeking agents.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "7. Conclusion",
    "page": 64,
    "text": "11",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 12",
    "page": 65,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "8. Limitations and Future Work",
    "page": 66,
    "text": "Despite our established DeepWideSearch benchmark, there are three key limitations remain to\nbe addressed in the future work: (1) As shown in Table 5 , the Wide2Deep construction method\nproduces significantly easier questions than Deep2Wide, as evidenced by the substantially higher CE\nAccuracy. We will iteratively refine sub-questions to increase question complexity while maintaining\nnatural language quality; (2) Our current dataset exhibits slight differences with real-world deep\nand wide search questions in terms of solution paths (Cases in Appendix B ). In future work, we\nwill iteratively refine the DeepWideSearch dataset to better align with real-world applications; and\n(3) Our dataset construction relies heavily on human annotation, limiting scalability. Future work\nshould explore automated data generation techniques and develop reference-free evaluation metrics\nthat avoid complex, human-verified tabular answers, enabling efficient dataset expansion and model\noptimization across diverse domains.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,\nS. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "S. Chen, P. Moreira, Y. Xiao, S. Schmidgall, J. Warner, H. Aerts, T. Hartvigsen, J. Gallifant, and D. S.\nBitterman. Medbrowsecomp: Benchmarking medical deep research and computer use, 2025. URL\nhttps://arxiv.org/abs/2505.14963 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "M. Du, B. Xu, C. Zhu, X. Wang, and Z. Mao. Deepresearch bench: A comprehensive benchmark for\ndeep research agents, 2025. URL https://arxiv.org/abs/2506.11763 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "Y. Fan, K. Zhang, H. Zhou, Y. Zuo, Y. Chen, Y. Fu, X. Long, X. Zhu, C. Jiang, Y. Zhang, L. Kang, G. Chen,\nC. Huang, Z. He, B. Wang, L. Bai, N. Ding, and B. Zhou. Ssrl: Self-search reinforcement learning,\n2025. URL https://arxiv.org/abs/2508.10874 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "T. Fang, H. Zhang, Z. Zhang, K. Ma, W. Yu, H. Mi, and D. Yu. Webevolver: Enhancing web agent\nself-improvement with coevolving world model, 2025a. URL https://arxiv.org/abs/2504.21024 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "T. Fang, Z. Zhang, X. Wang, R. Wang, C. Qin, Y. Wan, J.-Y. Ma, C. Zhang, J. Chen, X. Li, H. Zhang,\nH. Mi, and D. Yu. Cognitive kernel-pro: A framework for deep research agents and agent foundation\nmodels training, 2025b. URL https://arxiv.org/abs/2508.00414 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "H.-a. Gao, J. Geng, W. Hua, M. Hu, X. Juan, H. Liu, S. Liu, J. Qiu, X. Qi, Y. Wu, et al. A survey of\nself-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046 , 2025.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "B. Gou, Z. Huang, Y. Ning, Y. Gu, M. Lin, W. Qi, A. Kopanev, B. Yu, B. J. Guti√©rrez, Y. Shu, C. H.\nSong, J. Wu, S. Chen, H. N. Moussa, T. Zhang, J. Xie, Y. Li, T. Xue, Z. Liao, K. Zhang, B. Zheng,\nZ. Cai, V. Rozgic, M. Ziyadi, H. Sun, and Y. Su. Mind2web 2: Evaluating agentic search with\nagent-as-a-judge, 2025. URL https://arxiv.org/abs/2506.21506 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948 , 2025.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "R. Han, Y. Chen, Z. CuiZhu, L. Miculicich, G. Sun, Y. Bi, W. Wen, H. Wan, C. Wen, S. Ma√Ætre, G. Lee,\nV. Tirumalashetty, E. Xue, Z. Zhang, S. Haykal, B. Gokturk, T. Pfister, and C.-Y. Lee. Deep researcher\nwith test-time diffusion, 2025. URL https://arxiv.org/abs/2507.16075 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "References",
    "page": 67,
    "text": "12",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "Y. He, G. Huang, P. Feng, Y. Lin, Y. Zhang, H. Li, et al. Pasa: An llm agent for comprehensive academic\npaper search. arXiv preprint arXiv:2501.10120 , 2025.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "M. Hu, Y. Zhou, W. Fan, Y. Nie, B. Xia, T. Sun, Z. Ye, Z. Jin, Y. Li, Q. Chen, Z. Zhang, Y. Wang, Q. Ye,\nB. Ghanem, P. Luo, and G. Li. Owl: Optimized workforce learning for general multi-agent assistance\nin real-world task automation, 2025. URL https://arxiv.org/abs/2505.23885 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge\ndataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\npages 1601‚Äì1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147/ .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "K. Li, Z. Zhang, H. Yin, L. Zhang, L. Ou, J. Wu, W. Yin, B. Li, Z. Tao, X. Wang, et al. Websailor:\nNavigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592 , 2025.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v3\ntechnical report. arXiv preprint arXiv:2412.19437 , 2024.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "Y. Lyu, X. Zhang, L. Yan, M. de Rijke, Z. Ren, and X. Chen. Deepshop: A benchmark for deep research\nshopping agents, 2025. URL https://arxiv.org/abs/2506.02839 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "G. Mialon, C. Fourrier, C. Swift, T. Wolf, Y. LeCun, and T. Scialom. Gaia: a benchmark for general ai\nassistants, 2023. URL https://arxiv.org/abs/2311.12983 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "A. Roucher, A. V. del Moral, T. Wolf, L. von Werra, and E. Kaunism√§ki. ‚Äòsmolagents‚Äò: a smol library to\nbuild great agentic systems. https://github.com/huggingface/smolagents , 2025.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "X. Shi, Y. Li, Q. Kou, L. Yu, J. Xie, and H. Zhou. Spar: Scholar paper retrieval with llm-based agents\nfor enhanced academic search, 2025. URL https://arxiv.org/abs/2507.15245 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "H. Sun, Z. Qiao, J. Guo, X. Fan, Y. Hou, Y. Jiang, P. Xie, Y. Zhang, F. Huang, and J. Zhou. Zerosearch:\nIncentivize the search capability of llms without searching, 2025. URL https://arxiv.org/abs/2505.\n04588 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "Z. Tao, J. Wu, W. Yin, J. Zhang, B. Li, H. Shen, K. Li, L. Zhang, X. Wang, Y. Jiang, P. Xie, F. Huang, and\nJ. Zhou. Webshaper: Agentically data synthesizing via information-seeking formalization, 2025.\nURL https://arxiv.org/abs/2507.15061 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "K. Team, Y. Bai, Y. Bao, and G. C. et al. Kimi k2: Open agentic intelligence, 2025. URL https:\n//arxiv.org/abs/2507.20534 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "X. Wang, B. Wang, D. Lu, J. Yang, T. Xie, J. Wang, J. Deng, X. Guo, Y. Xu, C. H. Wu, Z. Shen, Z. Li,\nR. Li, X. Li, J. Chen, B. Zheng, P. Li, F. Lei, R. Cao, Y. Fu, D. Shin, M. Shin, J. Hu, Y. Wang, J. Chen,\nY. Ye, D. Zhang, D. Du, H. Hu, H. Chen, Z. Zhou, H. Yao, Z. Chen, Q. Gu, Y. Wang, H. Wang, D. Yang,\nV. Zhong, F. Sung, Y. Charles, Z. Yang, and T. Yu. Opencua: Open foundations for computer-use\nagents, 2025. URL https://arxiv.org/abs/2508.09123 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "J. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus, and\nA. Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents, 2025. URL\nhttps://arxiv.org/abs/2504.12516 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "R. Wong, J. Wang, J. Zhao, L. Chen, Y. Gao, L. Zhang, X. Zhou, Z. Wang, K. Xiang, G. Zhang, et al.\nWidesearch: Benchmarking agentic broad info-seeking. arXiv preprint arXiv:2508.07999 , 2025.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 13",
    "page": 68,
    "text": "13",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "J. Wu, B. Li, R. Fang, W. Yin, L. Zhang, Z. Tao, D. Zhang, Z. Xi, G. Fu, Y. Jiang, P. Xie, F. Huang,\nand J. Zhou. Webdancer: Towards autonomous information seeking agency, 2025a. URL https:\n//arxiv.org/abs/2505.22648 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "J. Wu, W. Yin, Y. Jiang, Z. Wang, Z. Xi, R. Fang, L. Zhang, Y. He, D. Zhou, P. Xie, and F. Huang.\nWebwalker: Benchmarking llms in web traversal, 2025b. URL https://arxiv.org/abs/2501.07572 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "Y. Xi, J. Lin, Y. Xiao, Z. Zhou, R. Shan, T. Gao, J. Zhu, W. Liu, Y. Yu, and W. Zhang. A survey of\nllm-based deep search agents: Paradigm, optimization, evaluation, and challenges. arXiv preprint\narXiv:2508.05668 , 2025.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "Y. Xia, J. Fan, W. Chen, S. Yan, X. Cong, Z. Zhang, Y. Lu, Y. Lin, Z. Liu, and M. Sun. AgentRM:\nEnhancing agent generalization with reward modeling. In W. Che, J. Nabende, E. Shutova, and\nM. T. Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 19277‚Äì19290, Vienna, Austria, July 2025. Association\nfor Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.945.\nURL https://aclanthology.org/2025.acl-long.945/ .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "W. Xu, K. Mei, H. Gao, J. Tan, Z. Liang, and Y. Zhang. A-mem: Agentic memory for llm agents, 2025.\nURL https://arxiv.org/abs/2502.12110 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "A. Yang, A. Li, and B. Y. et al. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A\ndataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/\n1809.09600 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "W. Zhang, C. Cui, Y. Zhao, R. Hu, Y. Liu, Y. Zhou, and B. An. Agentorchestra: A hierarchical multi-agent\nframework for general-purpose task solving. arXiv preprint arXiv:2506.12508 , 2025a.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "Z. Zhang, Z. Chen, M. Li, Z. Tu, and X. Li. Rlvmr: Reinforcement learning with verifiable meta-\nreasoning rewards for robust long-horizon agents, 2025b. URL https://arxiv.org/abs/2507.22844 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "H. Zhou, X. Wan, R. Sun, H. Palangi, S. Iqbal, I. Vuliƒá, A. Korhonen, and S. Arƒ±k. Multi-agent design:\nOptimizing agents with better prompts and topologies, 2025a. URL https://arxiv.org/abs/2502.\n02533 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "P. Zhou, B. Leon, X. Ying, C. Zhang, Y. Shao, Q. Ye, D. Chong, Z. Jin, C. Xie, M. Cao, Y. Gu, S. Hong,\nJ. Ren, J. Chen, C. Liu, and Y. Hua. Browsecomp-zh: Benchmarking web browsing ability of large\nlanguage models in chinese, 2025b. URL https://arxiv.org/abs/2504.19314 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "M. Zhuge, C. Zhao, D. R. Ashley, W. Wang, D. Khizbullin, Y. Xiong, Z. Liu, E. Chang, R. Krishnamoorthi,\nY. Tian, Y. Shi, V. Chandra, and J. Schmidhuber. Agent-as-a-judge: Evaluating agents with agents,\n2025. URL https://openreview.net/forum?id=DeVm3YUnpj .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 14",
    "page": 69,
    "text": "14",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 15",
    "page": 70,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "A. Details of Datasets",
    "page": 71,
    "text": "The table volume in Table 1 represents the number of the searched information in the DeepWideSearch\nquestions, which is defined as the product of rows and columns of the table. The average steps of\nthe search entities is counted as the number of the reasoning steps and tool calls. Specifically, the\naverage steps of GAIA is counted by the reference trajectories in the dataset, and the average steps\nof WideSearch is annotated by our three human raters. Besides, Figure 7 and Figure 8 present two\ncases in our proposed DeepWideSearch dataset.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "A. Details of Datasets",
    "page": 71,
    "text": "Case Deep2Wide_62 Instance (Core Entity is Nobel Proze)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Soviet physicist A received his Ph.D. at the age of 27 under the supervision",
    "page": 72,
    "text": "of the renowned Soviet physicist B. Physicist B was awarded the Nobel Prize at the age of 54\nand passed away six years later. In 2023, physicist A received a prestigious international prize\nin physics. Please provide the names, specific award titles, dates of birth (formatted as \"Mon\nDD, yyyy\"; if the exact date is unknown, use \"-, yyyy\"), and nationalities of the other scientists\nwho received awards in the same year as A. Present the results in a single Markdown table\nwith the following columns in order: Name, Award, Date of Birth, Nationality. All cells must\nbe filled according to the column requirements; do not omit any information arbitrarily. The\ntable must be output entirely in Chinese, and the final output should strictly follow the format:\n``` markdowntable content ``` .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Soviet physicist A received his Ph.D. at the age of 27 under the supervision",
    "page": 72,
    "text": "Ground-truth Table of Deep2Wide_62 Instance",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Nationality",
    "page": 73,
    "text": "Peter Agre\nChem\nJan 30, 1949\nUnited States\nRoderick MacKinnon\nChem\nFeb 19, 1956\nUnited States\nClive Granger\nEcon\nSep 04, 1934\nUnited Kingdom\nRobert Engle\nEcon\nNov 10, 1942\nUnited States\nAlexei Abrikosov\nPhys\nJun 25, 1928\nRussia/United States\nVitaly Ginzburg\nPhys\nOct 04, 1916\nRussia/United States\nAnthony Leggett\nPhys\nMar 26, 1938\nUnited Kingdom/United States\nJ. M. Coetzee\nLit\nFeb 09, 1940\nSouth Africa\nShirin Ebadi\nPeace\nJun 21, 1947\nIran\nPaul Lauterbur\nMed\nMay 06, 1929\nUnited States\nPeter Mansfield\nMed\nOct 09, 1933\nUnited Kingdom",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Nationality",
    "page": 73,
    "text": "Figure 7 | One case in DeepWideSearch dataset.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Nationality",
    "page": 73,
    "text": "15",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 16",
    "page": 74,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 16",
    "page": 74,
    "text": "Case Deep2Wide_26 Instance (Core Entity is Chen Yixun)",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : A pop song performed by a well-known Chinese singer, in which the song",
    "page": 75,
    "text": "title appears ten times in the lyrics, and the first two words of the album title start with the\nsame letter. The lyricist once served as a judge on a music variety show and later appeared on a\nlocal TV program alongside another famous lyricist. Could you please list the TV dramas the\nsinger has acted in, and summarize their titles, directors, and main cast? Present the results in\na Markdown table with the columns in the following order: Name, Director, Main Cast. Output\nonly the result in the format: ``` markdowntable content ``` .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : A pop song performed by a well-known Chinese singer, in which the song",
    "page": 75,
    "text": "Ground-truth Table of Deep2Wide_26 Instance",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Main Cast",
    "page": 76,
    "text": "Brief Marriage\nChen Zhifa\nEason Chan, Cecilia Yip\nToward Happiness\nLin Jianzhong\nNicky Wu, Tang Yuhong\nBloody Marriage\nJin Ge\nGu Shaohua\nSapphire Night Sky\nLai Hoi Shan\nCharlene Choi, Eason Chan\nLet‚Äôs Cheer Together\nZhu Ruibin\nSteven Cheung, Kenny Kwan\nTriumph in the Skies\nPoon Ka Tak\nAnthony Wong, Myolie Wu\nWhen Four-Leaf Clover\nMeets the Sword Tip\nZhang Yijie\nLai Lok Yi, Li Rilang",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Main Cast",
    "page": 76,
    "text": "Pet Love\nCheng Kei Sing\nLouis Koo, Sammi Cheng",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Main Cast",
    "page": 76,
    "text": "Midnight Express\nOno Tetsujiro\nTakemura Kentaro\nTakao Osawa, Nanako Matsushima",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Main Cast",
    "page": 76,
    "text": "Figure 8 | One case in DeepWideSearch dataset.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Main Cast",
    "page": 76,
    "text": "16",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 17",
    "page": 77,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "B. Differences between Our Dataset and Real-world Questions",
    "page": 78,
    "text": "Two Deep and Wide Questions",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Our DeepWideSearch Question : Identify an artist who studied at both the Central Academy",
    "page": 79,
    "text": "of Fine Arts in China and the Kunstakademie D√ºsseldorf in Germany, and who pursued further\nstudies in Germany. During his time in Germany, he studied under three renowned artists, one\nof whom held the record for the highest auction price ever achieved by a living artist in 2012.\nGather information about this artist‚Äôs solo exhibitions, including exhibition title, venue, city,\nand exhibition dates.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Solution Path : first identify the artist ( deep/wide search ), then perform wide search for",
    "page": 80,
    "text": "information collection ( wide search ).",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Real-world Deep and Wide Question : Please help me identify emerging local merchants",
    "page": 81,
    "text": "from Thailand and Vietnam that operate pet supplies categories on e-commerce platforms such\nas Lazada and Shopee, and whose store GMV growth rate exceeded 50% in the first half of\n2025. (Emerging merchants are defined as those that opened their stores after 2023.) I need\nthe following information for each qualifying store: store URL, company legal entity details,\nbusiness contact email, and the founder‚Äôs LinkedIn profile.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Solution Path : first perform wide search to list candidate stores ( wide search ), then perform",
    "page": 82,
    "text": "deep search to collect their information (GMV) for verification ( deep search ).",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Solution Path : first perform wide search to list candidate stores ( wide search ), then perform",
    "page": 82,
    "text": "Figure 9 | Two cases of the deep and wide search questions.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Solution Path : first perform wide search to list candidate stores ( wide search ), then perform",
    "page": 82,
    "text": "Figure 9 illustrates two representative deep and wide search questions: the first is an example from\nour constructed DeepWideSearch dataset, and the second is drawn from a real-world e-commerce\nscenario. While our dataset captures the essential characteristics of deep and wide search, the primary\ndifference from real-world settings lies in the solution path. In our dataset, the process emphasizes\nfirst performing a deep search to gather critical information, followed by a wide search to expand\nrelevant attributes. In contrast, real-world tasks often begin with a wide search to collect a large\npool of candidates, followed by a deep search over each candidate for verification. Nevertheless, it is\nimportant to emphasize that despite this procedural difference, our dataset still exhibits the traits of\ndeep and wide search. Specifically, during the initial deep search phase, the model also need to list\nand reason over a set of candidates, systematically applying deep verification to determine which\ncandidates satisfy the problem constraints and thereby identify the correct target entity. Consequently,\neven this first-stage deep search inherently incorporates the characteristic of the wi",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Solution Path : first perform wide search to list candidate stores ( wide search ), then perform",
    "page": 82,
    "text": "ne which\ncandidates satisfy the problem constraints and thereby identify the correct target entity. Consequently,\neven this first-stage deep search inherently incorporates the characteristic of the wide search.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "C. Prompts for DeepWideSearch Data Construction",
    "page": 83,
    "text": "This section presents three prompts for Wide2Deep method: (1) Core Entity Extraction Prompt in\nFigure 10 ; (2) Deep Sub-Question Synthesis Prompt in Figure 11 ; (3) Question Fusion in Figure 12 .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "C. Prompts for DeepWideSearch Data Construction",
    "page": 83,
    "text": "17",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 18",
    "page": 84,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Classification Requirements:",
    "page": 85,
    "text": "- The entity must be a specific, concrete entity object mentioned within the text query.\n- A query may contain multiple entities, but prioritize outputting the single most central entity. If no\nsingle core entity can be determined, output multiple entities.\n- Output ONLY JSON-formatted data containing the entities, with no additional content, explanations,\nor numbering (to ensure direct parseability). Please directly output the entity, without any explanation.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Reference Few-shot Examples",
    "page": 86,
    "text": "* Input:\nI absolutely love Jay Chou. Please find all songs released by Jay Chou between January 2004 and\nSeptember 2010 (including January 2004 and September 2010). Include song details: title, lyricist,\ncomposer, release date, album, and duration.\nNotes:\n1. I want only Jay Chou‚Äôs original vocals (collaborations allowed), excluding instrumental tracks.\n2. Format release dates as yyyy/mm/dd; duration as x minutes x seconds (e.g., 3 minutes 5 seconds).\n3. Include only songs released in China.\n4. Exclude live versions and demos.\n5.\nInclude only songs from Jay Chou‚Äôs studio albums (exclude singles, single albums, film/TV\nsoundtracks, EPs).\n* Output: Jay Chou",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Reference Few-shot Examples",
    "page": 86,
    "text": "* Input:\nI want to register for the 2026 postgraduate entrance exam. Please check the (retest) for the Journalism\nand Communication program (full-time professional master‚Äôs) at Chinese universities in Region A with\n211 Project status or higher for 2025 (total score only).\n* Output: Journalism and Communication program",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Reference Few-shot Examples",
    "page": 86,
    "text": "* Input:\nUsing statistics from the Stockholm International Peace Research Institute (SIPRI), list the specific\nmilitary expenditures (in billion USD without decimals, e.g., 9000 billion USD), GDP (in trillion USD to\ntwo decimal places, e.g., 30.21 trillion USD), global military expenditure ranking, head of state (actual\nleader), and defense minister for the US, Russia, Germany, India, and Japan for each year from 2019 to\n2024 (inclusive).\n* Output: Stockholm",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Reference Few-shot Examples",
    "page": 86,
    "text": "* Input:\nI‚Äôm running out of books to read. Could you compile a ranked list of the top 10 books from Douban\nReading‚Äôs annual (overall) for 2022-2024 (inclusive), plus the top 10 bestsellers and highest-rated\nbooks from Dangdang.com each year? Include authors‚Äô names.\n* Output: Douban Reading, Dangdang.com",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Reference Few-shot Examples",
    "page": 86,
    "text": "* Input:\nPlease compile a table listing the \"CNN Hero of the Year\" for every year the award was actually\npresented, from its first introduction through 2024 (including 2024), along with relevant details for\neach honoree.\n* Output: CNN Hero of the Year",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Perform entity extraction for the following query:",
    "page": 87,
    "text": "{question}",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Perform entity extraction for the following query:",
    "page": 87,
    "text": "Figure 10 | The prompt of core entitiy extraction in Wide2Deep method.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "# Perform entity extraction for the following query:",
    "page": 87,
    "text": "18",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 19",
    "page": 88,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 19",
    "page": 88,
    "text": "Please help me gather all available information about ‚Äú{entity}‚Äù, and based on the collected information,\nsynthesize a multi-hop query that meets the following requirements:\n(1) The answer to the query must be exactly ‚Äú{entity}‚Äù only‚Äîno other answers or ambiguities allowed;\n(2) The information about ‚Äú{entity}‚Äù included in the query should not be overly specific (e.g., exact\ndates, locations, awards, or distinctive features), so that one cannot directly find ‚Äúentity‚Äù by searching\nthose fragments online;\n(3) A human answering the query must perform multiple search steps and gather information from at\nleast three distinct, non-repeating URLs to logically infer ‚Äú{entity}‚Äù;\n(4) The query must be concise. Avoid constructing long queries by piling up excessive features. Include\nonly 2‚Äì3 pieces of information about {entity}. Focus on the ambiguity and reliability of these features,\nrather than the quantity;\n(5) Consider temporal validity‚Äîthe answer to the query must remain stable over time and not change\nwith time;\n(6) After generating the query, you must conduct an additional verification process via search engines:\nextract 3‚Äì5 simplified search queries from the synthesized query (each re",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 19",
    "page": 88,
    "text": " not change\nwith time;\n(6) After generating the query, you must conduct an additional verification process via search engines:\nextract 3‚Äì5 simplified search queries from the synthesized query (each reflecting one key feature or\nphrase from the original query; keep them short and focused on core keywords). Analyze the search\nresults to ensure that ‚Äú{entity}‚Äù cannot be directly found in a single step. If ‚Äú{entity}‚Äù can be found",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "dard JSON structure",
    "page": 89,
    "text": "Figure 11 | The prompt of deep sub-question synthesis in Wide2Deep method.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "D. Error Cases in DeepWideSearch",
    "page": 90,
    "text": "This section provides the four kinds of representative errors of agents: (1) Lack of Reflection (Figure 13\nand Figure 14 ); (2) Overreliance on Internal Knowledge (Figure 15 ); (3) Context Overflow (Figure 16 );\nand (4) Insufficient Retrieval (Figure 17 ).",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "D. Error Cases in DeepWideSearch",
    "page": 90,
    "text": "19",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 20",
    "page": 91,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 20",
    "page": 91,
    "text": "You are a senior question synthesis expert responsible for integrating a complex question about an\nentity into an original query containing that entity, thereby constructing a more challenging composite\nquery. Please strictly follow the rules below:\n1. Task Objective :\n- Replace the entity in the original query with the corresponding complex question.\n- The generated query must require users to first resolve the complex question to obtain the entity\ninformation, then proceed with the information retrieval steps in the original query.\n- The final synthesized query must be grammatically correct and logically coherent.\n- The complex question is in Chinese, but the original query may be in either Chinese or English: if\nthe original query is in English, you must translate the complex question into accurate and equivalent\nEnglish and integrate it into the query (without adding or omitting any information from the original\nChinese question, ensuring full translation fidelity); if the original query is in Chinese, the final\nsynthesized query must also be in Chinese.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 20",
    "page": 91,
    "text": "2. Input Specifications :\n- Original query (query): {query}\n- Entity (entities): entities (for internal reference only; must not appear in the output )\n- Complex question (question): A complex query question to find entities: question",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 20",
    "page": 91,
    "text": "3. Synthesis Rules :\n- Replace the entity in the original query with the descriptive text of the complex question.\n- The complex question must be transformed into a noun phrase (remove the question mark and\nrephrase it as a descriptive clause).\n- Do not reveal any entity information (specific names from ‚Äúentities‚Äù must not appear).\n- Maintain professional translation quality and avoid awkward or unnatural phrasing.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "original query, without any explanations or additional content",
    "page": 92,
    "text": "Figure 12 | The prompt of deep and wide question fusion in Wide2Deep method.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "original query, without any explanations or additional content",
    "page": 92,
    "text": "20",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 21",
    "page": 93,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Error: Lack of Reflection",
    "page": 94,
    "text": "Input of Deep2Wide_62 Instance",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Soviet physicist A received his Ph.D. at the age of 27 under the supervision",
    "page": 95,
    "text": "of the renowned Soviet physicist B. Physicist B was awarded the Nobel Prize at the age of 54\nand passed away six years later. In 2023, physicist A received a prestigious international prize\nin physics. Please provide the names, specific award titles, dates of birth (formatted as \"Mon\nDD, yyyy\"; if the exact date is unknown, use \"-, yyyy\"), and nationalities of the other scientists\nwho received awards in the same year as A. Present the results in a single Markdown table\nwith the following columns in order: Name, Award, Date of Birth, Nationality. All cells must\nbe filled according to the column requirements; do not omit any information arbitrarily. The\ntable must be output entirely in Chinese, and the final output should strictly follow the format:\n‚Äú‚Äòmarkdowntable content‚Äú‚Äò.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Nobel Prize",
    "page": 96,
    "text": "Multi-turn Converation of WebSailor (GPT-5)\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Nobel Prize",
    "page": 96,
    "text": "{\n\"role\": \"assistant\",\n\"content\": \"We first uniquely identify physicist B as Lev Landau ... An exhaustive review of\nthese major 2023 physics prizes reveals no recipient who was a student of Lev Landau. Furthermore,\n... Consequently, under the conditions specified in the problem (A being a student of Landau and\nreceiving a major international physics award in 2023), no matching individual can be identified\nin authoritative sources ... Given the inability to verify the identity of A or the specific\naward received, it is impossible to reliably list. Although the problem requires a table output,\nacademic integrity precludes the fabrication of data.\nTherefore, an empty table is provided,\nwith placeholders (‚Äú‚Äì‚Äù) indicating the absence of verified entries. Dates are formatted as ‚ÄúMon\nDD, yyyy‚Äù per the instructions.\n``` markdown\n| Name | Award | Date of Birth | Nationality |\n|‚Äî‚Äî|‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî-|\n| ‚Äì | ‚Äì | ‚Äì | ‚Äì |\n``` \"\n}",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Nobel Prize",
    "page": 96,
    "text": "Figure 13 | Lack of Reflection when dive into the wrong trajectory.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Nobel Prize",
    "page": 96,
    "text": "21",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 22",
    "page": 97,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Error: Lack of Reflection",
    "page": 98,
    "text": "Input of Wide2Deep_056 Instance",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : I am particularly interested in a discipline that is grounded in foundational",
    "page": 99,
    "text": "beliefs requiring no empirical verification, derives deterministic conclusions through rigorous\ntransformation rules, and is entirely detached from the physical world. Please help me compile\na list of institutions rated A+ in this discipline according to the Ministry of Education‚Äôs fourth\nround of disciplinary evaluations, along with specific information about academicians in this\ndiscipline at those institutions. Requirements: (1) Only include academicians who had already\nreceived the title of academician during their tenure at the institution‚Äôs school/faculty of\nthis discipline; (2) Only include those who held positions prior to 2025 (excluding 2025);\n(3) Exclude any academicians who passed away on or before June 2025. Please present the\ncompiled data in a single Markdown table‚Äîdo not split it into multiple tables. Every cell\nmust be filled according to the column requirements; do not omit any information arbitrarily.\nUse Chinese for the output content. The column headers, in order, are: Institution Name,\nAcademician Name, Year of Birth, Awards Received. Note: If any field cannot be found via\nonline searches, do not fabricate information; instead, enter \"NA\" in the corresponding ",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : I am particularly interested in a discipline that is grounded in foundational",
    "page": 99,
    "text": "are: Institution Name,\nAcademician Name, Year of Birth, Awards Received. Note: If any field cannot be found via\nonline searches, do not fabricate information; instead, enter \"NA\" in the corresponding cell.\nFor awards, only check for the Chern Shiing-Shen Mathematics Award and the Hua Luogeng\nMathematics Award; if neither applies, output \"NA\". Do not ask me any questions‚Äîonly output",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : I am particularly interested in a discipline that is grounded in foundational",
    "page": 99,
    "text": "the result in the following format: ``` markdowndata content ``` .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Mathematic",
    "page": 100,
    "text": "Multi-turn Converation of WebSailor (GPT-5)\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Mathematic",
    "page": 100,
    "text": "{\n\"role\": \"assistant\",\n\"content\": \"<think>First, confirm that the discipline described by the user is ‚ÄúMathematics,‚Äù\ncharacterized by its axiomatic foundations, deductive reasoning, and independence from the\nphysical world. Next, identify the institutions awarded an ‚ÄúA+‚Äù rating in Mathematics in the\nMinistry of Education‚Äôs Fourth Round of Discipline Evaluations (released in 2016) ...\nGiven\nthe inability to conduct correct online searches in this context‚Äîany attempt to populate the\nrequested data would risk inaccuracy ...\nAfter careful consideration, the safest and most\nethically consistent course‚Äîadhering strictly to the ‚Äúno speculation‚Äù principle‚Äîis to provide a\ntable header without any data rows. ...:\n``` markdown\n| Institution Name | Academician Name | Year of Birth | Award Received |\n``` }",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Mathematic",
    "page": 100,
    "text": "Figure 14 | Lack of reflection when tool calls are wrong.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Mathematic",
    "page": 100,
    "text": "22",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 23",
    "page": 101,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Error: Overreliance on Internal Knowledge",
    "page": 102,
    "text": "Input of Wide2Deep_059 Instance",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Which company, headquartered in a Silicon Valley city named after an",
    "page": 103,
    "text": "Italian saint, has committed to achieving carbon neutrality by 2030 and employs more than\n160,000 individuals? I am currently investigating the evolutionary trajectory of this company‚Äôs\nproducts and require a compilation of its primary flagship smartphone product lines launched\nin the U.S. market from January 9, 2007, to December 31, 2024. For each generation, the\nfollowing data must be recorded: product name, launch year, storage capacity (e.g., 128 GB),\nofficial launch price in U.S. dollars under carrier contract (prices corresponding to each storage\noption listed in a single row, separated by ‚Äú/‚Äù, e.g., capacity: ‚Äú4 GB/8 GB‚Äù, price: ‚Äúxxx$/xxx$‚Äù,\nwith strict one-to-one alignment between capacities and prices), and the most significant new\ntechnological feature or functional innovation introduced with that generation. The scope is\nstrictly limited to the standard annual flagship product line released each year in September or\nOctober. Each product line should occupy exactly one row in the table. If the launch price or\ncore new feature cannot be verified, the respective fields should be left blank. Please present\nthe compiled data in a single Markdown table‚Äîdo not split it into mul",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Which company, headquartered in a Silicon Valley city named after an",
    "page": 103,
    "text": " the table. If the launch price or\ncore new feature cannot be verified, the respective fields should be left blank. Please present\nthe compiled data in a single Markdown table‚Äîdo not split it into multiple tables. Every cell\nmust conform to the specified column requirements; no information should be omitted without\njustification. The column headers, in sequence, are: Product Name, Launch Year, Storage",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Which company, headquartered in a Silicon Valley city named after an",
    "page": 103,
    "text": "Capacity, Launch Price by Capacity (USD), Core New Technology / Feature. Do not pose any\nquestions‚Äîoutput only the result in the following format: ``` markdowndata content ```",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Apple Inc.",
    "page": 104,
    "text": "Multi-turn Converation of WebSailor (GPT-5)\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Apple Inc.",
    "page": 104,
    "text": "{\n\"role\": \"assistant\",\n\"content\": <think>I first determine that the company described by the user is Apple Inc ... I\nthen need to compile data on the regular flagship iPhone product lines launched between 2007 and\n2024, specifically those released in September or October each year. To ensure pricing accuracy,\nI will only include two-year contract prices when explicitly stated in official Apple press\nreleases ... The final output is presented in a single Markdown table, with storage capacities and\ncorresponding prices listed in matching order, separated by slashes.</think><answer> ``` markdown\n| Product Name | Launch Year | Storage Capacity | Launch Price by Capacity (USD) | Core New\nTechnology / Feature |\n| ‚Äî | ‚Äî | ‚Äî | ‚Äî | ‚Äî |\n...\n``` </answer> }",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Apple Inc.",
    "page": 104,
    "text": "Figure 15 | Overreliance on the internal knowledge of LLMs.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Apple Inc.",
    "page": 104,
    "text": "23",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 24",
    "page": 105,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Error: Context Overflow",
    "page": 106,
    "text": "Input of Wide2Deep_095 Instance",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Please help me compile a list of the athlete who won an Olympic gold",
    "page": 107,
    "text": "medal as a key player in the 1980s, was later inducted into the international hall of fame of\nher sport with 100% voting support, and in the 21st century served as head coach of another\ncountry‚Äôs national team, winning a medal at an Olympic Games hosted by her country of\nbirth. Additionally, compile all results and information from major international competitions\nduring her tenure as head coach of the Chinese Women‚Äôs National Volleyball Team (note two\nterms: April 2013 ‚Äì August 2016; April 2017 ‚Äì August 2021). Only include competitions in\nwhich she served as head coach. The major international competitions include: the Olympic\nGames, FIVB Volleyball Women‚Äôs World Championship, FIVB Volleyball Women‚Äôs World Cup,\nVolleyball Nations League (VNL), Asian Women‚Äôs Volleyball Championship, AVC Cup, FIVB\nWorld Grand Champions Cup, Montreux Volley Masters, FIVB Volleyball World Grand Prix, and\nAsian Games. The following information is required for each entry: year, full official name of the\ncompetition (e.g., 2016 Rio Olympics), names of Chinese women‚Äôs volleyball team players who\nparticipated (list only names, separated by commas, without positional details), and the final\nresult/ranking o",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Please help me compile a list of the athlete who won an Olympic gold",
    "page": 107,
    "text": "petition (e.g., 2016 Rio Olympics), names of Chinese women‚Äôs volleyball team players who\nparticipated (list only names, separated by commas, without positional details), and the final\nresult/ranking of the team. Present all compiled data in a single Markdown table‚Äîdo not split\ninto multiple tables. Every cell must contain the required information according to the column",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : Please help me compile a list of the athlete who won an Olympic gold",
    "page": 107,
    "text": "headers; no omissions are permitted. Use English for all content within the table. Column\nheaders, in order, are: Year, Competition Name, Chinese Women‚Äôs Volleyball Team Players,\nFinal Result/Ranking. Ensure competition names are complete (e.g., \"2016 Rio Olympics\").\nDo not ask any questions‚Äîonly output the result in the following format: ``` markdowndata\ncontent ``` .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Ping Lang",
    "page": 108,
    "text": "Multi-turn Converation of WebSailor (GPT-5)\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Ping Lang",
    "page": 108,
    "text": "{\n\"role\": \"assistant\",\n\"content\": \" You have now reached the maximum context length you can handle. You should stop\nmaking tool calls and, based on all the information above, think again and provide what you\nconsider the most likely answer in the following format:<think>your final thinking</think>\n<answer>your answer</answer> ... ``` markdown\n...\n``` \" }",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Ping Lang",
    "page": 108,
    "text": "Figure 16 | Multi-turn tool calls and reasoning leads to the context overflow problem, and agents are\ninterrupted to output the table.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : Ping Lang",
    "page": 108,
    "text": "24",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Page 25",
    "page": 109,
    "text": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Error: Insufficient Retrieval",
    "page": 110,
    "text": "Input of Wide2Deep_038 Instance",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "DeepWide Question : I am researching a type of designated area that complies with specific",
    "page": 111,
    "text": "national standards, is subject to dynamic management by the Ministry of Culture and Tourism,\nundergoes re-evaluation every three years, covers no less than 5 square kilometers, and currently\nincludes a total of 85 such areas nationwide. Please provide a complete statistical table as of\n2025, listing for each designated area its name, the province in which it is located, and its\ndesignation year (formatted as, e.g., ‚Äú2010‚Äù). Present the data in a single Markdown table\nwith the following column headers in English: Area Name, Province, Designation Year. Do not\nsplit the data into multiple tables, ensure every cell contains the required information without\nomissions, and output the table entirely in English. Do not ask any questions‚Äîonly output the\nresult in the format ``` markdowndata ``` .",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : China National Tourist Resort",
    "page": 112,
    "text": "Multi-turn Converation of WebSailor (GPT-5)\n...",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : China National Tourist Resort",
    "page": 112,
    "text": "{\n\"role\": \"assistant\",\n\"content\": \"I need to confirm which type of regional designation corresponds to the user‚Äôs\ndescription ... My preliminary judgment is that it refers to China National Tourist Resorts. ...\n<tool_call>{\"name\": \"search\" ...} ...\" }",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Core Entity : China National Tourist Resort",
    "page": 112,
    "text": "{\n\"role\": \"user\",\n\"content\": \"<tool_response>\nA Google search for ‚ÄôChina National Tourist Resorts 85‚Äô found 10 results:",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Web Results ...\" }",
    "page": 113,
    "text": "{\n\"role\": \"assistant\",\n\"content\": \"Based on the searched results, we need to output the complete list of \"National\nTourist Resorts\" as of 2025 (a total of 85), indicating the respective province and year of\ndesignation ...\" }",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Web Results ...\" }",
    "page": 113,
    "text": "Figure 17 | Complete information in the webpages are not passed to the agents, leading to the\ninsufficient retrieval error.",
    "images": null
  },
  {
    "doc_id": "2510.20168v1",
    "title": "Web Results ...\" }",
    "page": 113,
    "text": "25",
    "images": null
  }
]