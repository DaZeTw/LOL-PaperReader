{
  "question": "main contribute?",
  "hits": [
    {
      "index": 0,
      "score": 0.76315904,
      "text": "1.2 Contribution Our main contributions are threefold. First, we design a methodol- ogy to systematically collect the supply chain information of LLMs. In this paper, we mainly study the most popular AI platform, i.e., Hugging Face, but the same strategy applies to other platforms. In particular, we use the APIs from the AI platform to collect the metadata about the hosted models and datasets.",
      "metadata": {
        "chunk_id": "30255534c81b5efd6c421216dc5b7ba8a3726233",
        "doc_id": "2507.14240v3",
        "page": 2,
        "images": [],
        "tables": []
      }
    },
    {
      "index": 1,
      "score": 0.72844803,
      "text": "1.1 Motivation",
      "metadata": {
        "chunk_id": "d184fe2b78877baad139faaa595b0af60dd640ba",
        "doc_id": "2507.14240v3",
        "page": 2,
        "images": [],
        "tables": []
      }
    },
    {
      "index": 2,
      "score": 0.7272089,
      "text": "Acknowledgment This work was supported in part by National Science Foundation grants 2331301, 2508118, 2516003, 2419843. The views, opinions, and/or findings expressed in this material are those of the authors and should not be interpreted as representing the official views of the National Science Foundation, or the U.S. Government.",
      "metadata": {
        "chunk_id": "566ba6d729825c009cf20d832793cc9fa1aa1606",
        "doc_id": "2507.14240v3",
        "page": 7,
        "images": [],
        "tables": []
      }
    },
    {
      "index": 3,
      "score": 0.7219087,
      "text": "Finding #3: Base models like Llama-3.1-8B dominate the LLM supply chain, spawning thousands of derivatives, while task-specific models such as command-r-1-layer exhibit deep dependencies with other task-specific variants but avoid adapters or merges. Finding #4: Models and datasets exhibit strong bidirectional inter- dependence, with datasets like Mistral-v0.1 spawning hundreds of models, while models such as DeBERTa-ST-AllLayers-v3.1 leverage different datasets to enhance adaptability, highlighting the critical roles of dataset-model interactions in advancing AI.",
      "metadata": {
        "chunk_id": "a042bf605c03f461d68bc7cf977e49901a53c984",
        "doc_id": "2507.14240v3",
        "page": 6,
        "images": [],
        "tables": []
      }
    },
    {
      "index": 4,
      "score": 0.7211223,
      "text": "3.4 Accommodating Dynamic Update This highlights a broader issue in the AI community, where a sig- nificant number of models and datasets lack consistent and structured documentation on the supply chain. This reflects the need of more transparent disclosure. Degree distribution shows how node degrees (the number of edges connected to a node) are distributed across the graph. Figure 3 illustrates the indegree and outdegree distribution of the graph. We show not only the total distribution but also the distribution of six types of nodes, including base models, fine-tuned models, adapter models, quantization models, merged models, and datasets. We observe that the degree distribution in our supply chain graph is heavy-tailed . In particular, the indegree distribution shows a large spread across different categories. The outdegree distribu- tion follows a similar pattern but may differ in specific cases (e.g., adapters seem to have a more restricted degree distribution). This heavy-tailed behavior suggests that most nodes have low degrees, while a few central nodes (hubs) dominate the graph. In particular, the dataset macrocosm-os/images has the highest indegree 550, and dataset Mistral-v0.1 from “mistral AI” has the highest outdegree 1,093. Specifically, the base models act as high-degree hub nodes as they are heavily used by other task-specific models.",
      "metadata": {
        "chunk_id": "0b0414a27b26e94fef9b97dccf1afe3d652f7152",
        "doc_id": "2507.14240v3",
        "page": 4,
        "images": [],
        "tables": []
      }
    }
  ]
}