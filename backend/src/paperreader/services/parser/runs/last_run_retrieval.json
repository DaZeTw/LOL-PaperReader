{
  "question": "What methodology was used?",
  "hits": [
    {
      "index": 21,
      "score": 0.28904789276195725,
      "text": "Inspired by features proposed by prior work for identifying impor-\ntant citations in scientific papers [ 16 , 20 , 23 ], we selected a similar\nfeature set to use in our selection method. Unlike prior work, we\ncalculated a score of each citance using a linear combination of\nfeatures and hand-tuned parameters instead of training a binary\nclassifier. While a learned classifier might achieve higher precision,\nthe need for a large training dataset was a barrier. Unfortunately,\nexisting datasets used by prior work were not applicable for us due\nto differences in objectives. For example, the dataset used by Valen-\nzuela et al. [ 23 ] did not consider result comparison as an important\ncitation, but our formative study had shown that researchers found\nthose highly useful. When it came to choice of features, however,\nwe relied heavily on previous machine learning models (e.g., SciB-\nert [2], SPECTER [6], and Cohan’s citation intent classifier [5]).\nIn the end, our selection approach used the following features\n(we annotate whether the feature is a positive or negative signal):",
      "metadata": {
        "doc_id": "CiteRead",
        "title": "Selection Method",
        "page": 4,
        "images": []
      }
    },
    {
      "index": 66,
      "score": 0.28522476039724315,
      "text": "Q1: How do alternative approaches compare against the approach\nused in this paper? Select one or more statements that were made,\neither in this paper or in a citing paper.",
      "metadata": {
        "doc_id": "CiteRead",
        "title": "Service on Social Media”",
        "page": 13,
        "images": []
      }
    },
    {
      "index": 43,
      "score": 0.27296730935077557,
      "text": "Unfortunately, typical methods for answering these questions\ntoday are extremely cumbersome, involving manually scanning\nthrough a long list of potentially hundreds of papers, finding the\ncitation contexts, and then interpreting them in light of the parts\nof the reference paper they discuss. Even our baseline condition\nin the user study is a step up from typical methods as we used\nour selection method to reduce the list to a more relevant and\nmanageable set. For instance, when asked how long it would take\nto answer the questions posed in our user study using existing tools,\none participant said:",
      "metadata": {
        "doc_id": "CiteRead",
        "title": "DISCUSSION",
        "page": 10,
        "images": []
      }
    },
    {
      "index": 67,
      "score": 0.27092959081353946,
      "text": "• GRU models outperform the current work’s (baseline) LSTM\nmodel with respect to BLEU score\n• Convolutional Neural Network yields similar result to the\ncurrent work’s LSTM model for BLEU score\n• Bidirectional Long Short-term Memory encoder with\nattention-based architecture gets better results compared\nto plain LSTM encoder-decoder used in this paper\n• Human responses still outperform generated responses on\nappropriateness in this paper, but the responses generated\nby a tone-aware chatbot are perceived as appropriate as the\nresponses by human agents\n• There was no statistically significant difference between\nthis paper’s approach and human agents on empathy for\nemotional requests\n• Chatbots based on GRU models have shown better evaluation\nresults than human’s responses on attentiveness.\nQ2: In contrast to the dataset used in this paper to train chatbots,\nwhat have other papers tried to use as datasets instead? Select one\nor more answers.",
      "metadata": {
        "doc_id": "CiteRead",
        "title": "Service on Social Media”",
        "page": 13,
        "images": []
      }
    },
    {
      "index": 50,
      "score": 0.2677431800248606,
      "text": "Our evaluation was limited in several ways. First, our controlled\nlab experiment evaluated the benefits of CiteRead for a specific\nslice of scientists (HCI researchers) on a small set of papers. While\nour results are positive, more studies are needed to determine the\ngenerality of our findings. More extensive evaluation on a large\nrange of papers is also required to determine the general effective-\nness of our automated selection and localization techniques. Our\nend-to-end system evaluation provided support for its effectiveness\nin terms of strong quantitative and qualitative results corroborating\nthe value of these features, but evaluations of each individual tech-\nnique on many papers would provide additional evidence. We also\nnote the need for longitudinal studies to better understand how\ntools like CiteRead are used outside of laboratory settings. In this\nwork, we used existing extracted data from Semantic Scholar for\nselection and localization, which were pre-processed before study\nsessions. However, in the real world, it is impossible to exhaustively\npre-process all publications, and readers are likely to encounter\ndocuments that have not been processed. Moreover, this proces",
      "metadata": {
        "doc_id": "CiteRead",
        "title": "LIMITATIONS AND SOCIETAL IMPACTS",
        "page": 11,
        "images": []
      }
    }
  ]
}