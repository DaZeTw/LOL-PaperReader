{
  "question": "What animal is shown in the image and how does it relate to the Transformer model?",
  "hits": [
    {
      "index": 10,
      "score": 0.42024862825367537,
      "text": "Figure 1: The Transformer - model architecture.\n\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "3 Model Architecture",
        "page": 6,
        "images": []
      }
    },
    {
      "index": 85,
      "score": 0.4154287082793445,
      "text": "Figure 1: The Transformer - model architecture.\n\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "3 Model Architecture",
        "page": 6,
        "images": [
          {
            "data": "paperreader\\services\\parser\\output\\output\\1706.03762v7-referenced_artifacts\\image_000000_536d6dc5957170c29984f94ad0ddf7c2faaaf2cd88b962d1b385f13acf5ba66f.png",
            "caption": "The Transformer - model architecture.",
            "figure_id": "Figure 1"
          }
        ]
      }
    },
    {
      "index": 119,
      "score": 0.33163791700638323,
      "text": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 . 0 BLEU, establishing a new state-of-the-art BLEU score of 28 . 4 . The configuration of this model is listed in the bottom line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 . 0 , outperforming all of the previously published single models, at less than 1 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0 . 1 , instead of 0 .",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "6.1 Machine Translation",
        "page": 23,
        "images": []
      }
    },
    {
      "index": 44,
      "score": 0.33163791700638323,
      "text": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 . 0 BLEU, establishing a new state-of-the-art BLEU score of 28 . 4 . The configuration of this model is listed in the bottom line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 . 0 , outperforming all of the previously published single models, at less than 1 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0 . 1 , instead of 0 .",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "6.1 Machine Translation",
        "page": 23,
        "images": []
      }
    },
    {
      "index": 80,
      "score": 0.30537248562876407,
      "text": "The fundamental constraint of sequential computation, however, remains.\n\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "1 Introduction",
        "page": 4,
        "images": []
      }
    }
  ]
}