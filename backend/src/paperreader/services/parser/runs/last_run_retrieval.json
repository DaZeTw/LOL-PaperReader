{
  "question": "different of this image and method of this paper",
  "hits": [
    {
      "index": 148,
      "score": 0.35257226734727665,
      "text": "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads.",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "Attention Visualizations Input-Input Layer5",
        "page": 28,
        "images": [
          {
            "data": "paperreader\\services\\parser\\output\\output\\1706.03762v7-referenced_artifacts\\image_000003_5017bbb7a096da3d7466aa7d1f8724d5efca49213d9ca6f627bef63ded605814.png",
            "caption": "An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.",
            "figure_id": "Figure 3"
          }
        ]
      }
    },
    {
      "index": 85,
      "score": 0.3223811457299303,
      "text": "Figure 1: The Transformer - model architecture.\n\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "3 Model Architecture",
        "page": 6,
        "images": [
          {
            "data": "paperreader\\services\\parser\\output\\output\\1706.03762v7-referenced_artifacts\\image_000000_536d6dc5957170c29984f94ad0ddf7c2faaaf2cd88b962d1b385f13acf5ba66f.png",
            "caption": "The Transformer - model architecture.",
            "figure_id": "Figure 1"
          }
        ]
      }
    },
    {
      "index": 90,
      "score": 0.2813380972009719,
      "text": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "Scaled Dot-Product Attention",
        "page": 9,
        "images": [
          {
            "data": "paperreader\\services\\parser\\output\\output\\1706.03762v7-referenced_artifacts\\image_000002_35b5fe76ef3c23cddd7788e62be4d03daa9e37cdae00a416b15cbfce314a544d.png",
            "caption": "(left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.",
            "figure_id": "Figure 2"
          }
        ]
      }
    },
    {
      "index": 149,
      "score": 0.2604490428218995,
      "text": "Best viewed in color.\n\nInput-Input Layer5\n\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n\nInput-Input Layer5\n\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "Attention Visualizations Input-Input Layer5",
        "page": 28,
        "images": [
          {
            "data": "paperreader\\services\\parser\\output\\output\\1706.03762v7-referenced_artifacts\\image_000004_bdd23a5bd56a22c91f196ea3e78377d8e7eee842e1ba64020910524801c7db64.png",
            "caption": "Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.",
            "figure_id": "Figure 4"
          },
          {
            "data": "paperreader\\services\\parser\\output\\output\\1706.03762v7-referenced_artifacts\\image_000005_327640766cc26f2688c87a83103ecb39cc75036c36d13b044a9c4ad8e45a88f3.png",
            "caption": "Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.",
            "figure_id": "Figure 5"
          }
        ]
      }
    },
    {
      "index": 10,
      "score": 0.25372767726747547,
      "text": "Figure 1: The Transformer - model architecture.\n\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "3 Model Architecture",
        "page": 6,
        "images": []
      }
    }
  ]
}