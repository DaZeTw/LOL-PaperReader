{
  "question": "xin chào",
  "hits": [
    {
      "index": 1,
      "score": 0.2311462739290549,
      "text": "Ashish Vaswani ∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer ∗\nGoogle Brain\nnoam@google.com\nNiki Parmar ∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit ∗\nGoogle Research\nusz@google.com\nLlion Jones ∗\nGoogle Research\nllion@google.com\nAidan N. Gomez ∗†\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser ∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin ∗‡\nillia.polosukhin@gmail.com\nAbstract",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "Attention Is All You Need",
        "page": 3,
        "images": []
      }
    },
    {
      "index": 40,
      "score": 0.22390497169703916,
      "text": "[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434–443. ACL, August 2013.\n12",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "Page 12",
        "page": 15,
        "images": []
      }
    },
    {
      "index": 0,
      "score": 0.22285432350795573,
      "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "Page 1",
        "page": 2,
        "images": []
      }
    },
    {
      "index": 34,
      "score": 0.2208454550621085,
      "text": "The code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n10",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "Page 10",
        "page": 13,
        "images": []
      }
    },
    {
      "index": 41,
      "score": 0.21841136941797545,
      "text": "Attention Visualizations\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "Page 13",
        "page": 16,
        "images": []
      }
    }
  ]
}