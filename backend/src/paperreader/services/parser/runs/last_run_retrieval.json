{
  "question": "different of this image and method of this paper",
  "hits": [
    {
      "index": 148,
      "score": 0.3360412654900037,
      "text": "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads.",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "Attention Visualizations Input-Input Layer5",
        "page": 28,
        "images": [
          {
            "data": "paperreader\\services\\parser\\output\\output\\1706.03762v7-referenced_artifacts\\image_000003_5017bbb7a096da3d7466aa7d1f8724d5efca49213d9ca6f627bef63ded605814.png",
            "caption": "An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.",
            "figure_id": "Figure 3"
          }
        ]
      }
    },
    {
      "index": 19,
      "score": 0.25055394732957614,
      "text": "To counteract this effect, we scale the dot products by 1 √ d k .",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "3.2.1 Scaled Dot-Product Attention",
        "page": 10,
        "images": []
      }
    },
    {
      "index": 94,
      "score": 0.25055394732957614,
      "text": "To counteract this effect, we scale the dot products by 1 √ d k .",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "3.2.1 Scaled Dot-Product Attention",
        "page": 10,
        "images": []
      }
    },
    {
      "index": 134,
      "score": 0.2479373108114934,
      "text": "(2006) [29]          | WSJ only, discriminative |        90.4 |\n| Zhu et al. (2013) [40]             | WSJ only, discriminative |        90.4 |\n| Dyer et al. (2016) [8]             | WSJ only, discriminative |        91.7 |\n| Transformer (4 layers)             | WSJ only, discriminative |        91.3 |\n| Zhu et al. (2013) [40]             | semi-supervised          |        91.3 |\n| Huang &Harper (2009) [14]          | semi-supervised          |        91.3 |\n| McClosky et al. (2006) [26]        | semi-supervised          |        92.1 |\n| Vinyals &Kaiser el al. (2014) [37] | semi-supervised          |        92.1 |\n| Transformer (4 layers)             | semi-supervised          |        92.7 |\n| Luong et al. (2015) [23]           | multi-task               |        93   |\n| Dyer et al. (2016) [8]             | generative               |        93.3 |\n\nincreased the maximum output length to input length + 300 . We used a beam size of 21 and α = 0 . 3 for both WSJ only and the semi-supervised setting.\n\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the ex",
      "metadata": {
        "doc_id": "1706.03762v7-referenced",
        "title": "6.3 English Constituency Parsing",
        "page": 25,
        "images": []
      }
    },
    {
      "index": 59,
      "score": 0.2479373108114934,
      "text": "(2006) [29]          | WSJ only, discriminative |        90.4 |\n| Zhu et al. (2013) [40]             | WSJ only, discriminative |        90.4 |\n| Dyer et al. (2016) [8]             | WSJ only, discriminative |        91.7 |\n| Transformer (4 layers)             | WSJ only, discriminative |        91.3 |\n| Zhu et al. (2013) [40]             | semi-supervised          |        91.3 |\n| Huang &Harper (2009) [14]          | semi-supervised          |        91.3 |\n| McClosky et al. (2006) [26]        | semi-supervised          |        92.1 |\n| Vinyals &Kaiser el al. (2014) [37] | semi-supervised          |        92.1 |\n| Transformer (4 layers)             | semi-supervised          |        92.7 |\n| Luong et al. (2015) [23]           | multi-task               |        93   |\n| Dyer et al. (2016) [8]             | generative               |        93.3 |\n\nincreased the maximum output length to input length + 300 . We used a beam size of 21 and α = 0 . 3 for both WSJ only and the semi-supervised setting.\n\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the ex",
      "metadata": {
        "doc_id": "1706.03762v7-embedded",
        "title": "6.3 English Constituency Parsing",
        "page": 25,
        "images": []
      }
    }
  ]
}