{
  "question": "dual anchors là gì",
  "hits": [
    {
      "index": 5,
      "score": 0.4804331988896884,
      "text": "Functional Dual Anchor",
      "metadata": {
        "doc_id": "2510.21223v1-embedded",
        "title": "^",
        "page": 5,
        "images": []
      }
    },
    {
      "index": 17,
      "score": 0.33607587324086974,
      "text": "We aim to construct a set of inputs whose induced gradients on the pretrained model align with\nthe task vector. These gradients can be refined by comparing representation discrepancies between\nthe downstream checkpoints { φ ( θ i ) } m\ni =1 and the pretrained model φ ( θ 0 ) on the constructed inputs.\nFormally, assuming the model φ operates in a d -dimensional input space, we consider a set of n\ninput points { x ij } n\nj =1 ⊂ R d for the downstream checkpoint φ ( θ i ) . We refer to these points as\nanchors, as they link φ ( θ 0 ) and φ ( θ i ) . When these anchors ideally satisfy the following objective,\nthey constitute a set of Functional Dual Anchors (FDAs) for φ ( θ 0 ) and φ ( θ i ) ( i.e. , τ i ):\nmin\nx i 1 ,..., x in cos dist\n\u0012\n∇ θ\nn\nX\nj =1\nDist\n\u0000\nφ ( θ , x ij ) , φ ( θ i , x ij )\n\u0001\nθ = θ 0\n, τ i\n\u0013\n,\n(1)\nwhere cos dist( A , B ) = 1 − vec( A )vec( B )",
      "metadata": {
        "doc_id": "2510.21223v1-embedded",
        "title": "Page 3",
        "page": 8,
        "images": []
      }
    },
    {
      "index": 9,
      "score": 0.3153605427708534,
      "text": "In contrast to existing approaches, we focus on modeling the input-representation space to mitigate\ntask-specific knowledge conflicts. Rather than directly manipulating parameter offsets, we propose\ngenerating synthetic inputs, termed functional dual anchors (FDAs), that can effectively simulate the\nrole of task vectors. An illustration of this idea is provided in Figure 1 . Conceptually, this is akin\nto projecting task-specific knowledge into the input-representation space by constructing inputs that\nreproduce the downstream model’s functional shift relative to the pretrained model. Specifically,\nfor each downstream checkpoint, we construct a set of inputs whose induced gradients on the pre-\ntrained parameters align with the corresponding task vector. In this way, FDAs effectively act as\nthe dual of task vectors. While task vectors encode task-specific knowledge in the parameter space,\nFDAs capture the analogous knowledge in the input space through their induced gradients. This\n1",
      "metadata": {
        "doc_id": "2510.21223v1-embedded",
        "title": "θ B = FT( X B , θ 0 )",
        "page": 6,
        "images": []
      }
    },
    {
      "index": 2,
      "score": 0.3091235435023579,
      "text": "A BSTRACT\nModel merging is an efficient post-training strategy for integrating knowledge\nfrom multiple finetuned checkpoints of a shared foundation model. Existing meth-\nods operate in the parameter space, combining task vectors to mitigate conflicts,\nbut remain constrained by parameter inconsistencies. We propose Functional Dual\nAnchors (FDAs), a framework that instead models the input-representation space.\nFDAs are synthetic inputs whose induced gradients align with task vectors, captur-\ning task-specific functional shifts relative to the pretrained model. This perspec-\ntive bridges joint multi-task training and post-hoc merging, offering both robust-\nness and flexibility. We further introduce a principled initialization scheme and\nshow that FDAs are complementary to parameter-space model merging. Compre-\nhensive experiments demonstrate the effectiveness of FDAs in model merging.\n1\nI NTRODUCTION",
      "metadata": {
        "doc_id": "2510.21223v1-embedded",
        "title": "SphereLab.ai/fda",
        "page": 3,
        "images": []
      }
    },
    {
      "index": 26,
      "score": 0.29548691434253405,
      "text": "Technical Report\nAlgorithm 1: Model Merging with Functional Dual Anchors\nInput: Model architecture φ , pretrained parameters θ 0 , downstream parameters { θ i } m\ni =1\nOutput: Merged parameter ˆ θ , FDAs { x ( l )\nij } n\nj =1 , 1 ≤ i ≤ m, 1 ≤ l ≤ L\nfor l = 1 to L do\n/* --- Stage I: FDA Construction ---\n* /\nfor i = 1 to m do\nInitialization & Optimization: Initialize { x ( l )\nij } n\nj =1 using linear weight sampling or scalable\nGaussian sampling as starting points and then solve the following objective with gradient\ndescent:\n{ x ( l )\nij } n\nj =1 = arg min\nx ( l )\ni 1 ,..., x ( l )\nin\ncos dist\n\u0012\n∇ θ ( l )\nn\nX\nj =1\nDist\n\u0000 φ ( l ) ( θ ( l ) , x ( l )\nij ) , φ ( θ ( l )\ni , x ( l )\nij )\n\u0001 θ ( l ) = θ ( l )\n0\n, τ ( l )\ni\n\u0013\n.\nStore the optimized anchors { x ( l )\nij } n\nj =1 .\nend\n/* --- Stage II: Parameter Optimization using FDAs ---\n* /\nAggregate anchors across tasks { x ( l )\nij } .\nAcquire the merged parameter by solving:\nˆ θ ( l ) = arg min\nθ ( l )\nm\nX\ni =1\nn\nX\nj =1\nDist\n\u0010\nφ ( l ) ( θ ( l ) , x ( l )\nij ) , φ ( l ) ( θ ( l )\ni , x ( l )\nij )\n\u0011\n,\nfrom θ ( l )\n0 .\nend\nreturn ˆ θ , { x ( l )\nij } n\nj =1 for 1 ≤ i ≤ m, 1 ≤ l ≤ L .",
      "metadata": {
        "doc_id": "2510.21223v1-embedded",
        "title": "Page 5",
        "page": 10,
        "images": []
      }
    }
  ]
}