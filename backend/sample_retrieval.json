{
  "question": "What is the core idea of self-attention?",
  "hits": [
    {
      "index": 0,
      "score": 0.78,
      "text": "The self-attention mechanism computes a representation of a sequence by relating different positions, enabling each token to attend to all others via learned query-key interactions.",
      "metadata": {"doc_id": "fallback-1706.03762v7", "title": "Document", "page": 2}
    },
    {
      "index": 1,
      "score": 0.72,
      "text": "Scaled dot-product attention forms the basis of the Transformer, removing recurrence and allowing parallel computation across positions.",
      "metadata": {"doc_id": "fallback-1706.03762v7", "title": "Document", "page": 3}
    }
  ]
}
