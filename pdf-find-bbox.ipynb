{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b4b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df060707",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_INPUT = \"chunks (1).json\"  # your input JSON containing MANY chunks\n",
    "PDF_FOLDER = \"\"  # folder where PDFs are located\n",
    "JSON_OUTPUT = \"bboxes_output.json\"  # output file with bbox results\n",
    "PDF_PATH = \"2312.00752v2.pdf\"\n",
    "OUTPUT_JSON = \"page1_words.json\"\n",
    "# -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf15a7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pretty JSON â†’ chunks.json\n"
     ]
    }
   ],
   "source": [
    "OUTPUT = \"chunks.json\"\n",
    "\n",
    "with open(JSON_INPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(OUTPUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved pretty JSON â†’ {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3127ee1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved â†’ page1_words.json\n",
      "[OK] Saved â†’ page1_blocks.json\n",
      "[OK] Saved â†’ page1_dict.json\n",
      "[OK] Saved â†’ page1_raw.json\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_WORDS = \"page1_words.json\"\n",
    "OUTPUT_BLOCKS = \"page1_blocks.json\"\n",
    "OUTPUT_DICT = \"page1_dict.json\"\n",
    "OUTPUT_RAW = \"page1_raw.json\"\n",
    "OUTPUT_SPANS = \"page1_spans.json\"\n",
    "\n",
    "doc = fitz.open(PDF_PATH)\n",
    "page = doc[5]  # first page\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. WORDS\n",
    "# -------------------------------------------------------------------\n",
    "words = page.get_text(\"words\")\n",
    "\n",
    "words_formatted = []\n",
    "for w in words:\n",
    "    words_formatted.append(\n",
    "        {\n",
    "            \"x0\": w[0],\n",
    "            \"y0\": w[1],\n",
    "            \"x1\": w[2],\n",
    "            \"y1\": w[3],\n",
    "            \"text\": w[4],\n",
    "            \"block\": w[5],\n",
    "            \"line\": w[6],\n",
    "            \"word_index\": w[7],\n",
    "        }\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_WORDS, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(words_formatted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"[OK] Saved â†’ {OUTPUT_WORDS}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. BLOCKS (block-level layout)\n",
    "# -------------------------------------------------------------------\n",
    "blocks = page.get_text(\"blocks\")  # list of blocks\n",
    "# blocks structure: (x0, y0, x1, y1, \"block text\", block_type, block_no)\n",
    "\n",
    "blocks_formatted = []\n",
    "for b in blocks:\n",
    "    blocks_formatted.append(\n",
    "        {\n",
    "            \"x0\": b[0],\n",
    "            \"y0\": b[1],\n",
    "            \"x1\": b[2],\n",
    "            \"y1\": b[3],\n",
    "            \"text\": b[4],\n",
    "            \"type\": b[5],  # 0 = text, 1 = image, etc.\n",
    "            \"block_no\": b[6],\n",
    "        }\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_BLOCKS, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(blocks_formatted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"[OK] Saved â†’ {OUTPUT_BLOCKS}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. FULL PAGE DICT (block â†’ lines â†’ spans â†’ glyphs)\n",
    "# -------------------------------------------------------------------\n",
    "page_dict = page.get_text(\"dict\")\n",
    "\n",
    "with open(OUTPUT_DICT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(page_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"[OK] Saved â†’ {OUTPUT_DICT}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. RAW PyMuPDF JSON (EXACT raw with no processing)\n",
    "# -------------------------------------------------------------------\n",
    "raw = page.get_text(\"rawdict\")  # preserves numeric formatting\n",
    "\n",
    "with open(OUTPUT_RAW, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(raw, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"[OK] Saved â†’ {OUTPUT_RAW}\")\n",
    "\n",
    "\n",
    "def get_spans_from_page(page):\n",
    "    spans_formatted = []\n",
    "\n",
    "    page_dict = page.get_text(\"dict\")\n",
    "\n",
    "    global_span_index = 0  # <--- ADD THIS\n",
    "\n",
    "    for b_idx, block in enumerate(page_dict.get(\"blocks\", [])):\n",
    "        if block[\"type\"] != 0:\n",
    "            continue  # skip non-text blocks\n",
    "\n",
    "        for l_idx, line in enumerate(block.get(\"lines\", [])):\n",
    "            line_bbox = line[\"bbox\"]\n",
    "\n",
    "            for s_idx, span in enumerate(line.get(\"spans\", [])):\n",
    "                spans_formatted.append(\n",
    "                    {\n",
    "                        \"global_span_index\": global_span_index,  # <--- ADD THIS\n",
    "                        \"block_index\": b_idx,\n",
    "                        \"line_index\": l_idx,\n",
    "                        \"span_index\": s_idx,\n",
    "                        \"text\": span[\"text\"],\n",
    "                        \"span_bbox\": span[\"bbox\"],\n",
    "                        \"line_bbox\": line_bbox,\n",
    "                        \"size\": span.get(\"size\"),\n",
    "                        \"font\": span.get(\"font\"),\n",
    "                        \"color\": span.get(\"color\"),\n",
    "                        \"flags\": span.get(\"flags\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                global_span_index += 1\n",
    "\n",
    "    with open(OUTPUT_SPANS, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(spans_formatted, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[OK] Saved â†’ {OUTPUT_SPANS}\")\n",
    "\n",
    "\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f55c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spans(page):\n",
    "    dict_data = page.get_text(\"dict\")\n",
    "    spans = []\n",
    "\n",
    "    for block in dict_data.get(\"blocks\", []):\n",
    "        if block[\"type\"] != 0:\n",
    "            continue  # only text blocks\n",
    "\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                spans.append(\n",
    "                    {\n",
    "                        \"text\": span[\"text\"],\n",
    "                        \"bbox\": span[\"bbox\"],\n",
    "                        \"line_bbox\": line[\"bbox\"],\n",
    "                        \"font\": span[\"font\"],\n",
    "                        \"size\": span[\"size\"],\n",
    "                    }\n",
    "                )\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0edfb0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Convert Greek / Math Unicode â†’ ASCII semantic names\n",
    "#    e.g. Î” â†’ delta, ğ›¥ â†’ delta, ğ›¼ â†’ alpha, â„ â†’ h, ğ‘ â†’ c\n",
    "# -----------------------------------------------------------\n",
    "def fold_math_and_greek(text: str) -> str:\n",
    "    out = []\n",
    "\n",
    "    for ch in text:\n",
    "        try:\n",
    "            name = unicodedata.name(ch)\n",
    "        except ValueError:\n",
    "            # unprintable or private unicode\n",
    "            continue\n",
    "\n",
    "        # A) Greek letters (all variants)\n",
    "        if \"GREEK\" in name:\n",
    "            # Example names:\n",
    "            # \"GREEK CAPITAL LETTER DELTA\"\n",
    "            # \"GREEK SMALL LETTER ALPHA\"\n",
    "            #\n",
    "            # We extract the final word:\n",
    "            base = name.split(\"LETTER\")[-1].strip().lower()\n",
    "            out.append(base)\n",
    "            continue\n",
    "\n",
    "        # B) Mathematical alphanumeric symbols (ğ›¥, ğ‘¥, ğ’¶, ğ”¸, ğ•’)\n",
    "        if \"MATHEMATICAL\" in name:\n",
    "            # Example names:\n",
    "            # \"MATHEMATICAL BOLD CAPITAL DELTA\"\n",
    "            # \"MATHEMATICAL ITALIC SMALL X\"\n",
    "            parts = name.split()\n",
    "            # last part is usually the base letter (\"DELTA\", \"X\", etc.)\n",
    "            base = parts[-1].lower()\n",
    "            out.append(base)\n",
    "            continue\n",
    "\n",
    "        # C) All other characters: keep raw\n",
    "        out.append(ch)\n",
    "\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. Main Normalization (robust)\n",
    "# -----------------------------------------------------------\n",
    "def normalize(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Step 1: Fix broken Unicode encodings\n",
    "    text = ftfy.fix_text(text)\n",
    "\n",
    "    # Step 2: Fold Greek & Math Unicode to ASCII words\n",
    "    text = fold_math_and_greek(text)\n",
    "\n",
    "    # Step 3: Convert fancy unicode â†’ closest ASCII\n",
    "    text = unidecode(text)\n",
    "\n",
    "    # Step 4: Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 5: Remove stray symbols except basic punctuation\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,;:()\\[\\]{}+\\-=/]\", \" \", text)\n",
    "\n",
    "    # Step 6: Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def fuzzy(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "def match_percentage(combined, chunk):\n",
    "    \"\"\"\n",
    "    Percentage of chunk that appears in combined.\n",
    "    No penalty for unmatched prefix/suffix.\n",
    "    \"\"\"\n",
    "    if not chunk:\n",
    "        return 0.0\n",
    "    if not combined:\n",
    "        return 0.0\n",
    "\n",
    "    matcher = SequenceMatcher(None, chunk, combined)\n",
    "\n",
    "    matched = 0\n",
    "    for block in matcher.get_matching_blocks():\n",
    "        matched += block.size  # number of matching chars\n",
    "\n",
    "    pct = (matched / len(chunk)) * 100\n",
    "    return round(pct, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf78a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed(chunk_text, lengths=[10, 5, 2]):\n",
    "    words = normalize(chunk_text).split()\n",
    "    return [\"\".join(words[:l]) for l in lengths if len(words) >= l]\n",
    "\n",
    "\n",
    "def find_candidate_spans(spans, seeds, seed_threshold=0.40):\n",
    "    matched_indices = set()\n",
    "    for seed in seeds:\n",
    "        for idx, sp in enumerate(spans):\n",
    "            score = fuzzy(normalize(sp[\"text\"]), seed)\n",
    "            if score >= seed_threshold:\n",
    "                matched_indices.add(idx)\n",
    "    print(f\"Found {len(matched_indices)} unique candidate spans across seeds.\")\n",
    "    return list(matched_indices)\n",
    "\n",
    "\n",
    "def expand_best_span_match(spans, chunk_text, candidate_indices, window=8):\n",
    "    normalized_chunk = normalize(chunk_text)\n",
    "\n",
    "    best_score = 0\n",
    "    best_range = None\n",
    "\n",
    "    best_combined = \"\"\n",
    "    for start in candidate_indices:\n",
    "\n",
    "        combined = \"\"\n",
    "\n",
    "        for j in range(start, min(start + window, len(spans))):\n",
    "\n",
    "            combined += \"\" + spans[j][\"text\"]\n",
    "            score = match_percentage(normalize(combined), normalized_chunk)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "\n",
    "                best_range = (start, j)\n",
    "                best_combined = combined\n",
    "\n",
    "    print(f\"Best match score: {best_score:.4f} for spans {best_range}\")\n",
    "    print(f\"Best matched text: '{best_combined}'\")\n",
    "    return best_range, best_score\n",
    "\n",
    "\n",
    "def find_best_span_match(spans, chunk_text, threshold=0.80):\n",
    "    seed = get_seed(chunk_text)\n",
    "    candidates = find_candidate_spans(spans, seed)\n",
    "\n",
    "    if not candidates:\n",
    "        return None, 0\n",
    "\n",
    "    best_range, best_score = expand_best_span_match(\n",
    "        spans, chunk_text, candidates, window=100  # look ahead only 10 spans\n",
    "    )\n",
    "\n",
    "    if best_score >= threshold:\n",
    "        return best_range, best_score\n",
    "\n",
    "    return None, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fcdb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_spans_into_line_bboxes(spans, span_range):\n",
    "    (start, end) = span_range\n",
    "    selected = spans[start : end + 1]\n",
    "\n",
    "    line_map = {}\n",
    "\n",
    "    for sp in selected:\n",
    "        lx0, ly0, lx1, ly1 = sp[\"line_bbox\"]\n",
    "        if (ly0, ly1) not in line_map:\n",
    "            line_map[(ly0, ly1)] = [lx0, ly0, lx1, ly1]\n",
    "        else:\n",
    "            line_map[(ly0, ly1)][0] = min(line_map[(ly0, ly1)][0], lx0)\n",
    "            line_map[(ly0, ly1)][2] = max(line_map[(ly0, ly1)][2], lx1)\n",
    "\n",
    "    # convert dict to list\n",
    "    bboxes = []\n",
    "    for k, v in line_map.items():\n",
    "        bboxes.append({\"x0\": v[0], \"y0\": v[1], \"x1\": v[2], \"y1\": v[3]})\n",
    "\n",
    "    # keep in vertical reading order\n",
    "    bboxes.sort(key=lambda b: b[\"y0\"])\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def normalize_bboxes(bboxes, page_width, page_height):\n",
    "    normalized = []\n",
    "    for box in bboxes:\n",
    "        norm_box = {\n",
    "            \"x0\": box[\"x0\"] / page_width,\n",
    "            \"y0\": box[\"y0\"] / page_height,\n",
    "            \"x1\": box[\"x1\"] / page_width,\n",
    "            \"y1\": box[\"y1\"] / page_height,\n",
    "        }\n",
    "        normalized.append(norm_box)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7fc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_chunk_by_span(page, chunk_text, threshold=0.80):\n",
    "    spans = extract_spans(page)\n",
    "    span_range, score = find_best_span_match(spans, chunk_text, threshold)\n",
    "\n",
    "    if not span_range:\n",
    "        return {\"found\": False, \"score\": score, \"bboxes\": []}\n",
    "\n",
    "    bboxes = merge_spans_into_line_bboxes(spans, span_range)\n",
    "\n",
    "    # Get page size from PyMuPDF\n",
    "    page_width, page_height = page.rect.width, page.rect.height\n",
    "\n",
    "    normalized_bboxes = normalize_bboxes(bboxes, page_width, page_height)\n",
    "\n",
    "    return {\"found\": True, \"score\": score, \"bboxes\": normalized_bboxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b0e12ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved â†’ page1_spans.json\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 99.4300 for spans (28, 51)\n",
      "Best matched text: 'To understand this principle, we focus on two running examples of synthetic tasks (Figure 2).â€¢ The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying theposition of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens(colored) and filter out the irrelevant ones (white).â€¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learningabilities of LLMs (Olsson et al. 2022). It requires'\n",
      "{'found': True, 'score': 99.43, 'bboxes': [{'x0': 0.11715849084791795, 'y0': 0.35183850683347145, 'x1': 0.737118590111826, 'y1': 0.3644175673976089}, {'x0': 0.11764705882352941, 'y0': 0.374356549195569, 'x1': 0.9176506092345792, 'y1': 0.39011051678898356}, {'x0': 0.13149999481400634, 'y0': 0.3894512677433515, 'x1': 0.9176404367085376, 'y1': 0.4052962871512981}, {'x0': 0.1310114393047258, 'y0': 0.4046705612028488, 'x1': 0.45981961917253883, 'y1': 0.42035948628126973}, {'x0': 0.11764705882352941, 'y0': 0.42718864209724194, 'x1': 0.9176506092345792, 'y1': 0.4429426096906566}, {'x0': 0.13149999481400634, 'y0': 0.44250854338058315, 'x1': 0.9176472183925654, 'y1': 0.458071429320056}]}\n"
     ]
    }
   ],
   "source": [
    "doc = fitz.open(\"2312.00752v2.pdf\")\n",
    "page = doc[5 - 1]\n",
    "get_spans_from_page(page)\n",
    "chunk = \"To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). â€¢ The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). â€¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022).\"\n",
    "\n",
    "result = match_chunk_by_span(page, chunk, threshold=0.75)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4349f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: 2312.00752v2.pdf\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.2700 for spans (0, 14)\n",
      "Best matched text: 'Mamba: Linear-Time Sequence Modeling with Selective State SpacesAlbert Guâˆ—1 and Tri Daoâˆ—21Machine Learning Department, Carnegie Mellon University2Department of Computer Science, Princeton Universityagu@cs.cmu.edu, tri@tridao.me'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 99.1100 for spans (5, 33)\n",
      "Best matched text: 'and Tri Daoâˆ—21Machine Learning Department, Carnegie Mellon University2Department of Computer Science, Princeton Universityagu@cs.cmu.edu, tri@tridao.meAbstractFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on theTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,gated convolution and recurrent models, and structured state space models (SSMs) have been developed to addressTransformersâ€™ computational inefficiency on long sequences, but they have not performed as well as attention on importantmodalities such as language. We identify that a key weakness of such models is their inability to perform content-basedreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addressestheir weakness with discrete modalities, allowing the model to selectively propagate or forget information along thesequence length dimension depending on the current token. Second, even though this change prevents the use of efficientconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into asimplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fastinference (5Ã— higher throughput than Transformers) and linear scaling in sequence length, and its performance improveson real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 99.0500 for spans (0, 36)\n",
      "Best matched text: 'Mamba: Linear-Time Sequence Modeling with Selective State SpacesAlbert Guâˆ—1 and Tri Daoâˆ—21Machine Learning Department, Carnegie Mellon University2Department of Computer Science, Princeton Universityagu@cs.cmu.edu, tri@tridao.meAbstractFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on theTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,gated convolution and recurrent models, and structured state space models (SSMs) have been developed to addressTransformersâ€™ computational inefficiency on long sequences, but they have not performed as well as attention on importantmodalities such as language. We identify that a key weakness of such models is their inability to perform content-basedreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addressestheir weakness with discrete modalities, allowing the model to selectively propagate or forget information along thesequence length dimension depending on the current token. Second, even though this change prevents the use of efficientconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into asimplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fastinference (5Ã— higher throughput than Transformers) and linear scaling in sequence length, and its performance improveson real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-artperformance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B modeloutperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstreamevaluation.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 96.4400 for spans (36, 52)\n",
      "Best matched text: 'evaluation.1IntroductionFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emergedas an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating onarbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, andgenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 99.0700 for spans (53, 88)\n",
      "Best matched text: ' 2023; Sutskever,Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs arepredominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attentionlayer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information denselywithin a context window, allowing it to model complex data. However, this property brings fundamental drawbacks:an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length.An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay,Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of thesevariants have been shown to be empirically effective at scale across domains.Recently, structured state space sequence models (SSMs) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) haveemerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination ofrecurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state spacemodels (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, withlinear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-rangedependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Rangeâˆ—Alphabetical by first name.1arXiv:2312.00752v2  [cs.LG]  31 May 2024'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 99.4500 for spans (0, 20)\n",
      "Best matched text: 'Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu,and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have beensuccessful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al.'\n",
      "Found 16 unique candidate spans across seeds.\n",
      "Best match score: 99.0300 for spans (1, 79)\n",
      "Best matched text: ' 2021). Many flavors of SSMs (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu,and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have beensuccessful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022;Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data suchas text.We propose a new class of selective state space models, that improves on prior work on several axes to achieve themodeling power of Transformers while scaling linearly in sequence length.Selection Mechanism.First, we identify a key limitation of prior models: the ability to efficiently select data in aninput-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetictasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSMparameters based on the input. This allows the model to filter out irrelevant information and remember relevant informationindefinitely.Hardware-aware Algorithm.This simple change poses a technical challenge for the computation of the model; infact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome thiswith a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does notmaterialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. Theresulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared topseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3Ã— faster on A100 GPUs).Architecture.We simplify prior deep sequence model architectures by combining the design of prior SSM architectures(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenousarchitecture design (Mamba) incorporating selective state spaces.Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make themsuitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strongperformance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memoryscales linearly in sequence length during training, and unrolling the model autoregressively during inference requires onlyconstant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiencytogether yield performance improvements on real data up to sequence length 1M.We empirically validate Mambaâ€™s potential as a general sequence FM backbone, in both pretraining quality and domain-specific task performance, on several types of modalities and settings:â€¢ Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key tolarge language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long (>1M tokens).â€¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformerson modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducingFID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longercontext up to million-length sequences.'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 99.2000 for spans (67, 96)\n",
      "Best matched text: 'large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long (>1M tokens).â€¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformerson modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducingFID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longercontext up to million-length sequences.â€¢ Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance,both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mambaexceeds the performance of a large range of baselines, including very strong modern Transformer training recipes basedon LLaMa (Touvron et al. 2023). Our Mamba language model has 5Ã— generation throughput compared to Transformersof similar size, and Mamba-3Bâ€™s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on commonsense reasoning compared to Pythia-3B and even exceeding Pythia-7B).Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 91.7900 for spans (48, 105)\n",
      "Best matched text: 'Architecture.We simplify prior deep sequence model architectures by combining the design of prior SSM architectures(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenousarchitecture design (Mamba) incorporating selective state spaces.Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make themsuitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strongperformance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memoryscales linearly in sequence length during training, and unrolling the model autoregressively during inference requires onlyconstant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiencytogether yield performance improvements on real data up to sequence length 1M.We empirically validate Mambaâ€™s potential as a general sequence FM backbone, in both pretraining quality and domain-specific task performance, on several types of modalities and settings:â€¢ Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key tolarge language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long (>1M tokens).â€¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformerson modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducingFID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longercontext up to million-length sequences.â€¢ Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance,both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mambaexceeds the performance of a large range of baselines, including very strong modern Transformer training recipes basedon LLaMa (Touvron et al. 2023). Our Mamba language model has 5Ã— generation throughput compared to Transformersof similar size, and Mamba-3Bâ€™s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on commonsense reasoning compared to Pythia-3B and even exceeding Pythia-7B).Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.2State Space ModelsStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly relatedto RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a2'\n",
      "Found 1 unique candidate spans across seeds.\n",
      "Best match score: 43.3500 for spans (58, 87)\n",
      "Best matched text: '1-dimensional function or sequence ğ‘¥(ğ‘¡) âˆˆR â†¦â†’ğ‘¦(ğ‘¡) âˆˆR through an implicit latent state â„(ğ‘¡) âˆˆRğ‘.Concretely, S4 models are defined with four parameters (Î”, ğ‘¨, ğ‘©, ğ‘ª)'\n",
      "Found 41 unique candidate spans across seeds.\n",
      "Best match score: 97.6500 for spans (149, 240)\n",
      "Best matched text: ' )(3a)ğ‘¦= ğ‘¥âˆ—ğ‘²(3b)Discretization.The first stage transforms the â€œcontinuous parametersâ€ (Î”, ğ‘¨, ğ‘©) to â€œdiscrete parametersâ€ (ğ‘¨, ğ‘©) throughfixed formulas ğ‘¨= ğ‘“ğ´(Î”, ğ‘¨) and ğ‘©= ğ‘“ğµ(Î”, ğ‘¨, ğ‘©), where the pair (ğ‘“ğ´, ğ‘“ğµ) is called a discretization rule. Various rules canbe used such as the zero-order hold (ZOH) defined in equation (4).ğ‘¨= exp(Î”ğ‘¨)ğ‘©= (Î”ğ‘¨)âˆ’1(exp(Î”ğ‘¨) âˆ’ğ‘°) Â· Î”ğ‘©(4)Discretization has deep connections to continuous-time systems which can endow them with additional properties suchas resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized(Gu, Johnson, Timalsina, et al. 2023; Orvieto et al.'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 95.3200 for spans (236, 326)\n",
      "Best matched text: ' 2022) and automatically ensuring that the model is properly normalized(Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023). It also has connections to gating mechanisms of RNNs (Gu,Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from a mechanical pointof view discretization can simply be viewed as the first step of the computation graph in the forward pass of an SSM.Alternate flavors of SSMs can bypass the discretization step and parameterize (ğ‘¨, ğ‘©) directly instead (Zhang et al. 2023),which may be easier to reason about.Computation.After the parameters have been transformed from (Î”, ğ‘¨, ğ‘©, ğ‘ª) â†¦â†’(ğ‘¨, ğ‘©, ğ‘ª), the model can be computedin two ways, either as a linear recurrence (2) or a global convolution (3).Commonly, the model uses the convolutional mode (3) for efficient parallelizable training (where the whole input sequenceis seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (where the inputs areseen one timestep at a time).Linear Time Invariance (LTI).An important property of equations (1) to (3) is that the modelâ€™s dynamics are constantthrough time. In other words (Î”, ğ‘¨, ğ‘©, ğ‘ª), and consequently (ğ‘¨, ğ‘©) as well, are fixed for all time-steps. This property is3'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.9600 for spans (0, 52)\n",
      "Best matched text: 'called linear time invariance (LTI), which is deeply connected to recurrence and convolutions. Informally, we think ofLTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for theseclasses of models.Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental efficiency constraints,discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modelingcertain types of data, and our technical contributions involve removing the LTI constraint while overcoming the efficiencybottlenecks.Structure and Dimensions.Finally, we note that structured SSMs are so named because computing them efficientlyalso requires imposing structure on the ğ‘¨matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022;Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.In this case, the ğ‘¨âˆˆRğ‘Ã—ğ‘, ğ‘©âˆˆRğ‘Ã—1, ğ‘ªâˆˆR1Ã—ğ‘matrices can all be represented by'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.0700 for spans (31, 96)\n",
      "Best matched text: 'In this case, the ğ‘¨âˆˆRğ‘Ã—ğ‘, ğ‘©âˆˆRğ‘Ã—1, ğ‘ªâˆˆR1Ã—ğ‘matrices can all be represented by ğ‘numbers. To operate over an inputsequence ğ‘¥of batch size ğµand length ğ¿with ğ·channels, the SSM is applied independently to each channel. Note that inthis case, the total hidden state has dimension ğ·ğ‘per input, and computing it over the sequence length requires ğ‘‚(ğµğ¿ğ·ğ‘)time and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3.General State Space Models.We note that the term state space model has a very broad meaning which simply representsthe notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in differentdisciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causalmodeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)),hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimesconvolutional) models at large (deep learning).Throughout this entire paper we use the term â€œSSMâ€ to refer exclusively to the class of structured SSMs or S4 models (Gu,Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al.'\n",
      "Found 30 unique candidate spans across seeds.\n",
      "Best match score: 98.9400 for spans (24, 107)\n",
      "Best matched text: ' 2022;Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.In this case, the ğ‘¨âˆˆRğ‘Ã—ğ‘, ğ‘©âˆˆRğ‘Ã—1, ğ‘ªâˆˆR1Ã—ğ‘matrices can all be represented by ğ‘numbers. To operate over an inputsequence ğ‘¥of batch size ğµand length ğ¿with ğ·channels, the SSM is applied independently to each channel. Note that inthis case, the total hidden state has dimension ğ·ğ‘per input, and computing it over the sequence length requires ğ‘‚(ğµğ¿ğ·ğ‘)time and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3.General State Space Models.We note that the term state space model has a very broad meaning which simply representsthe notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in differentdisciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causalmodeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)),hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimesconvolutional) models at large (deep learning).Throughout this entire paper we use the term â€œSSMâ€ to refer exclusively to the class of structured SSMs or S4 models (Gu,Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington,and Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of suchmodels, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvietoet al.'\n",
      "Found 26 unique candidate spans across seeds.\n",
      "Best match score: 98.1200 for spans (80, 161)\n",
      "Best matched text: ' 2020)), dynamic causalmodeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)),hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimesconvolutional) models at large (deep learning).Throughout this entire paper we use the term â€œSSMâ€ to refer exclusively to the class of structured SSMs or S4 models (Gu,Goel, and RÃ© 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington,and Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of suchmodels, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvietoet al. 2023; Poli et al. 2023), and clarify nuances when necessary.SSM Architectures.SSMs are standalone sequence transformations that can be incorporated into end-to-end neuralnetwork architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linearconvolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as ourprimary baselines.â€¢ Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can beviewed as a degenerate linear SSM.â€¢ H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSMsandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as ashift-SSM, before the main SSM layer.â€¢ Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized globalconvolution (Romero et al. 2021).â€¢ RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternativeparallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.â€¢ RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approxi-mation, the attention-free Transformer (S. Zhai et al. 2021). Its main â€œWKVâ€ mechanism involves LTI recurrences andcan be viewed as the ratio of two SSMs.Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlightin particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), whichwe view as the most closely related methods to our core selective SSM.4'\n",
      "Found 13 unique candidate spans across seeds.\n",
      "Best match score: 98.4800 for spans (1, 8)\n",
      "Best matched text: 'Selective State Space ModelsWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporatethis mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presentinga technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 99.3300 for spans (8, 16)\n",
      "Best matched text: 'a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploitsthe memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention oreven MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5).'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 99.0200 for spans (14, 27)\n",
      "Best matched text: '). Finally, we discuss some additional properties of selection mechanisms (Section 3.5).3.1Motivation: Selection as a Means of CompressionWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we canview the tradeoffs of popular sequence models from this point of view. For example, attention is both effective andinefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inferencerequires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference andquadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state,implying constant-time inference and linear-time training. However, their effectiveness is limited by how well this statehas compressed the context.'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 99.4300 for spans (28, 51)\n",
      "Best matched text: 'To understand this principle, we focus on two running examples of synthetic tasks (Figure 2).â€¢ The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying theposition of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens(colored) and filter out the irrelevant ones (white).â€¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learningabilities of LLMs (Olsson et al. 2022). It requires'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 99.1400 for spans (38, 84)\n",
      "Best matched text: ' content-aware reasoning to be able to memorize the relevant tokens(colored) and filter out the irrelevant ones (white).â€¢ The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learningabilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output inthe appropriate context (black).These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the (ğ‘¨, ğ‘©)transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passedalong the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions cansolve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficultywith the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing betweeninputs-to-outputs is varying and cannot be modeled by static convolution kernels.In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compresstheir state: efficient models must have a small state, while effective models must have a state that contains all necessaryinformation from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity:or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanismcontrols how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion).'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 98.8200 for spans (85, 93)\n",
      "Best matched text: '3.2Improving SSMs with SelectionOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions alongthe sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be input-dependent.Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 98.9500 for spans (91, 171)\n",
      "Best matched text: ' and 2 illustrates the main selection mechanism that we use. The main difference is simply making severalparameters Î”, ğ‘©, ğ‘ªfunctions of the input, along with the associated changes to tensor shapes throughout. In particular, wehighlight that these parameters now have a length dimension ğ¿, meaning that the model has changed from time-invariantto time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3)with implications for its efficiency, discussed next.We specifically choose ğ‘ ğµ(ğ‘¥) = Linearğ‘(ğ‘¥), ğ‘ ğ¶(ğ‘¥) = Linearğ‘(ğ‘¥), ğ‘ Î”(ğ‘¥) = Broadcastğ·(Linear1(ğ‘¥)), and ğœÎ” = softplus,where Linearğ‘‘is a parameterized projection to dimension ğ‘‘. The choice of ğ‘ Î” and ğœÎ” is due to a connection to RNN gatingmechanisms explained in Section 3.5.5'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (23, 24)\n",
      "Best matched text: 'Algorithm 1 SSM (S4)'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 85.8600 for spans (25, 124)\n",
      "Best matched text: 'Input: ğ‘¥: (B, L, D)Output: ğ‘¦: (B, L, D)1: ğ‘¨: (D, N) â†ParameterâŠ²Represents structured ğ‘Ã— ğ‘matrix2: ğ‘©: (D, N) â†Parameter3: ğ‘ª: (D, N) â†Parameter4: Î” : (D) â†ğœÎ”(Parameter)5: ğ‘¨, ğ‘©: (D, N) â†discretize(Î”, ğ‘¨, ğ‘©)6: ğ‘¦â†SSM(ğ‘¨, ğ‘©, ğ‘ª)(ğ‘¥)âŠ²Time-invariant: recurrence or convolution7: return ğ‘¦'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (125, 126)\n",
      "Best matched text: 'Algorithm 2 SSM + Selection (S6)'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 66.0300 for spans (127, 226)\n",
      "Best matched text: 'Input: ğ‘¥: (B, L, D)Output: ğ‘¦: (B, L, D)1: ğ‘¨: (D, N) â†ParameterâŠ²Represents structured ğ‘Ã— ğ‘matrix2: ğ‘©: (B, L, N) â†ğ‘ ğµ(ğ‘¥)3: ğ‘ª: (B, L, N) â†ğ‘ ğ¶(ğ‘¥)4: Î” : (B, L, D) â†ğœÎ”(Parameter+ğ‘ Î”(ğ‘¥))5: ğ‘¨, ğ‘©: (B, L, D'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 90.6000 for spans (260, 273)\n",
      "Best matched text: '3.3Efficient Implementation of Selective SSMsHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau,Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient onmodern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporatespecial cases of selection, such as letting Î” vary over time in recurrent SSMs (Gu, Dao, et al.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.1400 for spans (274, 277)\n",
      "Best matched text: ' 2020). However, as previouslymentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivativesused LTI (non-selective) models, most commonly in the form of global convolutions.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 95.0400 for spans (253, 303)\n",
      "Best matched text: 'Time-varying: recurrence (scan) only7: return ğ‘¦3.3Efficient Implementation of Selective SSMsHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau,Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient onmodern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporatespecial cases of selection, such as letting Î” vary over time in recurrent SSMs (Gu, Dao, et al. 2020). However, as previouslymentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivativesused LTI (non-selective) models, most commonly in the form of global convolutions.3.3.1Motivation of Prior ModelsWe first revisit this motivation and overview our approach to overcome limitations of prior methods.â€¢ At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed inSection 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximizehidden state dimension without paying speed and memory costs.â€¢ Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expandingthe former (2) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 99.1500 for spans (294, 352)\n",
      "Best matched text: ' is derived from expandingthe former (2) (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing andmaterializing the latent state â„with shape (B, L, D, N), which is much larger (by a factor of ğ‘, the SSM state dimension)than the input ğ‘¥and output ğ‘¦of shape (B, L, D). Thus the more efficient convolution mode was introduced which couldbypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D).â€¢ Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension bya factor of ğ‘(â‰ˆ10 âˆ’100), much larger than traditional RNNs, without efficiency penalties.6'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 99.1000 for spans (0, 35)\n",
      "Best matched text: '3.3.2Overview of Selective Scan: Hardware-Aware State ExpansionThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need torevisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, andrecomputation. We make two main observations:â€¢ The naive recurrent computation uses ğ‘‚(ğµğ¿ğ·ğ‘) FLOPs while the convolutional computation uses ğ‘‚(ğµğ¿ğ·log(ğ¿)) FLOPs,and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension ğ‘, the recurrentmode can actually use fewer FLOPs.â€¢ The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just likethe convolutional mode, we can attempt to not actually materialize the full state â„.The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state â„only in more efficientlevels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memorybandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 99.0300 for spans (36, 96)\n",
      "Best matched text: ' 2021; Williams, Waterman, and Patterson 2009). This includes ourscan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup comparedto a standard implementation.Concretely, instead of preparing the scan input (ğ‘¨, ğ‘©) of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we loadthe SSM parameters (Î”, ğ‘¨, ğ‘©, ğ‘ª) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM,and then write the final outputs of size (B, L, D) back to HBM.To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficientparallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023).Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully applythe classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored butrecomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scanlayer has the same memory requirements as an optimized transformer implementation with FlashAttention.Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustratedin Figure 1.'\n",
      "Found 13 unique candidate spans across seeds.\n",
      "Best match score: 97.0700 for spans (96, 152)\n",
      "Best matched text: '.3.4A Simplified SSM ArchitectureAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated intoneural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which aregenerally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. Wesimplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This isinspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention.This architecture involves expanding the model dimension ğ·by a controllable expansion factor ğ¸. For each block, mostof the parameters (3ğ¸ğ·2) are in the linear projections (2ğ¸ğ·2 for input projections, ğ¸ğ·2 for output projection) whilethe inner SSM contributes less.The number of SSM parameters (projections for Î”, ğ‘©, ğ‘ª, and the matrix ğ‘¨) are muchsmaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to formthe Mamba architecture. We always fix to ğ¸= 2 in our experiments and use two stacks of the block to match the 12ğ·2parameters of a Transformerâ€™s interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activationfunction (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLPbecomes the popular â€œSwiGLUâ€ variant (Chowdhery et al. 2023; Dauphin et al.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.2200 for spans (148, 164)\n",
      "Best matched text: ' 2017), motivated so that the Gated MLPbecomes the popular â€œSwiGLUâ€ variant (Chowdhery et al. 2023; Dauphin et al. 2017; Shazeer 2020; Touvron et al. 2023).Finally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)),motivated by RetNetâ€™s usage of a normalization layer in a similar location (Y. Sun et al. 2023).'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 85.3800 for spans (122, 177)\n",
      "Best matched text: 'for input projections, ğ¸ğ·2 for output projection) whilethe inner SSM contributes less.The number of SSM parameters (projections for Î”, ğ‘©, ğ‘ª, and the matrix ğ‘¨) are muchsmaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to formthe Mamba architecture. We always fix to ğ¸= 2 in our experiments and use two stacks of the block to match the 12ğ·2parameters of a Transformerâ€™s interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activationfunction (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLPbecomes the popular â€œSwiGLUâ€ variant (Chowdhery et al. 2023; Dauphin et al. 2017; Shazeer 2020; Touvron et al. 2023).Finally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)),motivated by RetNetâ€™s usage of a normalization layer in a similar location (Y. Sun et al. 2023).3.5Properties of Selection MechanismsThe selection mechanism is a broader concept that can be applied in different ways, such as to more traditional RNNs orCNNs, to different parameters (e.g. ğ‘¨in Algorithm 2), or using different transformations ğ‘ (ğ‘¥).'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 27.8400 for spans (80, 119)\n",
      "Best matched text: ' ğœÎ” = softplus, then the selective SSM recurrence (Algorithm 2)takes the formğ‘”ğ‘¡= ğœ(Linear(ğ‘¥ğ‘¡))â„ğ‘¡= (1 âˆ’ğ‘”ğ‘¡)â„ğ‘¡âˆ’1 + ğ‘”ğ‘¡ğ‘¥ğ‘¡.(5)As mentioned in Section 3.2'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 30.7700 for spans (80, 118)\n",
      "Best matched text: ' ğœÎ” = softplus, then the selective SSM recurrence (Algorithm 2)takes the formğ‘”ğ‘¡= ğœ(Linear(ğ‘¥ğ‘¡))â„ğ‘¡= (1 âˆ’ğ‘”ğ‘¡)â„ğ‘¡âˆ’1 + ğ‘”ğ‘¡ğ‘¥ğ‘¡.(5)As mentioned in Section'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 96.1100 for spans (16, 35)\n",
      "Best matched text: 'SSMX!!ConvSSMâ¨‚Figure 3: (Architecture.) Our simplified block design combines the H3 block, which is the basis of most SSM architectures, withthe ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba blockhomogenously. Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared tothe MLP block, Mamba adds an SSM to the main branch. For ğœwe use the SiLU / Swish activation (Hendrycks and Gimpel 2016;Ramachandran, Zoph, and Quoc V Le 2017).'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 95.7400 for spans (36, 125)\n",
      "Best matched text: '3.5.1Connection to Gating MechanismsWe highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selectionmechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time systemsis well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is an improvement ofGu, Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and input-dependent gates (proof inAppendix C). More broadly, Î” in SSMs can be seen to play a generalized role of the RNN gating mechanism. In line withprior work, we adopt the view that discretization of SSMs is the principled foundation of heuristic gating mechanisms.Theorem 1. When ğ‘= 1, ğ‘¨= âˆ’1, ğ‘©= 1,ğ‘ Î” = Linear(ğ‘¥), and ğœÎ” = softplus, then the selective SSM recurrence (Algorithm 2)takes the formğ‘”ğ‘¡= ğœ(Linear(ğ‘¥ğ‘¡))â„ğ‘¡= (1 âˆ’ğ‘”ğ‘¡)â„ğ‘¡âˆ’1 + ğ‘”ğ‘¡ğ‘¥ğ‘¡.(5)As mentioned in Section 3.2, our specific choices of ğ‘ Î”,ğœÎ” is from this connection. In particular, note that if a given input'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 99.1500 for spans (93, 133)\n",
      "Best matched text: 'Linear(ğ‘¥ğ‘¡))â„ğ‘¡= (1 âˆ’ğ‘”ğ‘¡)â„ğ‘¡âˆ’1 + ğ‘”ğ‘¡ğ‘¥ğ‘¡.(5)As mentioned in Section 3.2, our specific choices of ğ‘ Î”,ğœÎ” is from this connection. In particular, note that if a given input ğ‘¥ğ‘¡should be completely ignored (as necessary in the synthetic tasks), all ğ·channels should ignore it, and so we project theinput down to 1 dimension before repeating/broadcasting with Î”.'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 95.5900 for spans (134, 161)\n",
      "Best matched text: '3.5.2Interpretation of Selection MechanismsWe elaborate on three particular mechanistic effects of selection.Variable Spacing.Selectivity allows filtering out irrelevant noise tokens that may occur between inputs of interest.This is exemplified by the Selective Copying task, but occurs ubiquitously in common data modalities, particularly fordiscrete data â€“ for example the presence of language fillers such as â€œumâ€. This property arises because the model canmechanistically filter out any particular input ğ‘¥ğ‘¡, for example in the gated RNN case (Theorem 1) when ğ‘”ğ‘¡â†’0.Filtering Context.It has been empirically observed that many sequence models do not improve with longer context (F.Shi et al. 2023), despite the principle that more context should lead to strictly better performance. An explanation isthat many sequence models cannot effectively ignore irrelevant context when necessary; an intuitive example are globalconvolutions (and general LTI models). On the other hand, selective models can simply reset their state at any timeto remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g.Section 4.3.2).'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 99.0400 for spans (0, 17)\n",
      "Best matched text: 'Boundary Resetting.In settings where multiple independent sequences are stitched together, Transformers can keepthem separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences.Selective SSMs can also reset their state at boundaries (e.g. Î”ğ‘¡â†’âˆ, or Theorem 1 when ğ‘”ğ‘¡â†’1). These settings mayoccur artificially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries inreinforcement learning (Lu et al. 2023)).'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 90.0200 for spans (18, 117)\n",
      "Best matched text: 'Additionally, we elaborate on effects of each selective parameter.Interpretation of Î”.In general, Î” controls the balance between how much to focus or ignore the current input ğ‘¥ğ‘¡. Itgeneralizes RNN gates (e.g. ğ‘”ğ‘¡in Theorem 1): mechanically, a large Î” resets the state â„and focuses on the current input ğ‘¥,while a small Î” persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous systemdiscretized by a timestep Î”, and in this context the intuition is that large Î” â†’âˆrepresents the system focusing on thecurrent input for longer (thus â€œselectingâ€ it and forgetting its current state) while a small Î” â†’0 represents a transientinput that is ignored.Interpretation of ğ‘¨.We remark that while the ğ‘¨parameter could also be selective, it ultimately affects the modelonly through its interaction with Î” via ğ‘¨= exp(Î”ğ‘¨) (the discretization (4)). Thus selectivity in Î” is enough to ensureselectivity in (ğ‘¨, ğ‘©), and is the main source of improvement. We hypothesize that making ğ‘¨selective in addition to (orinstead of) Î” would have similar performance, and leave it out for simplicity.Interpretation of ğ‘©and ğ‘ª.As discussed in Section 3.1, the most important property of selectivity is filtering outirrelevant information so that a sequence modelâ€™s context can be compressed into an efficient state. In an SSM, modifyingğ‘©and ğ‘ªto be selective allows finer-grained control over whether to let an input ğ‘¥ğ‘¡into the state â„ğ‘¡, or the state into theoutput ğ‘¦'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 94.6800 for spans (122, 135)\n",
      "Best matched text: 'Additional Model DetailsReal vs. Complex.Most prior SSMs use complex numbers in their state â„, which is necessary for strong performanceon many tasks in perceptual modalities (Gu, Goel, and RÃ© 2022). However, it has been empirically observed that completelyreal-valued SSMs seem to work fine, and possibly even better, in some settings (Ma et al. 2023). We use real values asthe default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoff is related to thecontinuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous modalities (e.g. audio,video) but not discrete (e.g. text, DNA).'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 93.6600 for spans (136, 232)\n",
      "Best matched text: 'Initialization.Most prior SSMs also suggest special initializations, particularly in the complex-valued case, which canhelp in several settings such as low-data regimes. Our default initialization for the complex case is S4D-Lin and for the realcase is S4D-Real (Gu, Gupta, et al. 2022), which is based on the HIPPO theory (Gu, Dao, et al. 2020). These define the ğ‘›-thelement of ğ‘¨as âˆ’1/2 + ğ‘›ğ‘–and âˆ’(ğ‘›+ 1) respectively. However, we expect many initializations to work fine, particularly inthe large-data and real-valued SSM regimes; some ablations are considered in Section 4.6.Parameterization of Î”.We defined the selective adjustment to Î” as ğ‘ Î”(ğ‘¥) = Broadcastğ·(Linear1(ğ‘¥)), which wasmotivated by the mechanics of Î” (Section 3.5). We observe that it can be generalized from dimension 1 to a largerdimension R. We set this to be a small fraction of D, which uses a negligible number of parameters compared to the mainLinear projections in the block. We additionally note that the broadcasting operation can instead be viewed as anotherLinear projection, initialized to a specific pattern of 1â€™s and 0â€™s; if this projection is trainable, this leads to the alternativeğ‘ Î”(ğ‘¥) = Linearğ·(Linearğ‘…(ğ‘¥)), which can be viewed as a low-rank projection.In our experiments, the Î” parameter (which can be viewed as a bias term) is initialized to ğœâˆ’1Î” (Uniform([0.001, 0.1])),'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 98.9300 for spans (236, 245)\n",
      "Best matched text: 'Remark 3.1. For brevity in our experimental results, we sometimes abbreviate selective SSMs as S6 models, because they areS4 models with a selection mechanism and computed with a scan.9'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 89.5200 for spans (1, 23)\n",
      "Best matched text: 'Empirical EvaluationIn Section 4.1 we test Mambaâ€™s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate on threedomains, each evaluated on autoregressive pretraining as well as downstream tasks.â€¢ Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.â€¢ Section 4.3: DNA sequence pretraining, and fine-tuning on a long-sequence classification task.â€¢ Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.Finally, Section 4.5 shows Mambaâ€™s computational efficiency at both training and inference time, and Section 4.6 ablatesvarious components of the architecture and selective SSMs.4.1'\n",
      "Found 11 unique candidate spans across seeds.\n",
      "Best match score: 99.5700 for spans (2, 31)\n",
      "Best matched text: 'In Section 4.1 we test Mambaâ€™s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate on threedomains, each evaluated on autoregressive pretraining as well as downstream tasks.â€¢ Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.â€¢ Section 4.3: DNA sequence pretraining, and fine-tuning on a long-sequence classification task.â€¢ Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.Finally, Section 4.5 shows Mambaâ€™s computational efficiency at both training and inference time, and Section 4.6 ablatesvarious components of the architecture and selective SSMs.4.1Synthetic TasksFull experiment details for these tasks including task details and training protocol are in Appendix E.1.4.1.1Selective CopyingThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to testthe memorization abilities of recurrent models. As discussed in Section'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (3, 30)\n",
      "Best matched text: ' 4.1 we test Mambaâ€™s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate on threedomains, each evaluated on autoregressive pretraining as well as downstream tasks.â€¢ Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.â€¢ Section 4.3: DNA sequence pretraining, and fine-tuning on a long-sequence classification task.â€¢ Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.Finally, Section 4.5 shows Mambaâ€™s computational efficiency at both training and inference time, and Section 4.6 ablatesvarious components of the architecture and selective SSMs.4.1Synthetic TasksFull experiment details for these tasks including task details and training protocol are in Appendix E.1.4.1.1Selective CopyingThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 99.1400 for spans (8, 47)\n",
      "Best matched text: 'â€¢ Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.â€¢ Section 4.3: DNA sequence pretraining, and fine-tuning on a long-sequence classification task.â€¢ Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.Finally, Section 4.5 shows Mambaâ€™s computational efficiency at both training and inference time, and Section 4.6 ablatesvarious components of the architecture and selective SSMs.4.1Synthetic TasksFull experiment details for these tasks including task details and training protocol are in Appendix E.1.4.1.1Selective CopyingThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to testthe memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and globalconvolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for example, byconstructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated in earlier work onglobal convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut by randomizing the spacingbetween tokens. Note that this task has been introduced before as the Denoising task (Jing et al. 2019).Note that many previous works argue that adding architecture gating (multiplicative interactions) can endow models withâ€œdata-dependenceâ€ and solve related tasks (Dao, Fu, Saab, et al. 2023; Poli et al.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.0200 for spans (48, 58)\n",
      "Best matched text: ' 2023). However, we find this explanationinsufficient intuitively because such gating does not interact along the sequence axis, and cannot affect the spacing betweentokens. In particular architecture gating is not an instance of a selection mechanism (Appendix A).Table 1 confirms that gated architectures such as H3 and Mamba only partially improve performance, while the selec-tion mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more powerfularchitectures.'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 97.5100 for spans (60, 105)\n",
      "Best matched text: 'Induction HeadsInduction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021) that issurprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative recall and copy:for example, if the model has seen a bigram such as â€œHarry Potterâ€ in the sequence, then the next time â€œHarryâ€ appears inthe same sequence, the model should be able to predict â€œPotterâ€ by copying from history.Dataset.We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which iscomparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigategeneralization and extrapolation abilities by evaluating on a range of sequence lengths from 26 = 64 up to 220 = 1048576 attest time.Models.Following established work on induction heads, we use 2 layer models, which allows attention to mechanisticallysolve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads, with various positionalencodings) and SSM variants. We use a model dimension ğ·of 64 for Mamba and 128 for the other models.Results.Table 2 shows that Mambaâ€”or more precisely, its selective SSM layerâ€”has the ability to solve the task perfectlybecause of its ability to selectively remember the relevant token while ignoring everything else in between. It generalizesperfectly to million-length sequences, or 4000Ã— longer than it saw during training, while no other method goesbeyond 2Ã—.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 86.3600 for spans (0, 3)\n",
      "Best matched text: 'ModelArch.LayerAcc.'\n",
      "Found 12 unique candidate spans across seeds.\n",
      "Best match score: 80.5600 for spans (4, 11)\n",
      "Best matched text: 'S4No gateS418.3-No gateS697.0'\n",
      "Found 17 unique candidate spans across seeds.\n",
      "Best match score: 76.0900 for spans (12, 23)\n",
      "Best matched text: 'H3H3S457.0HyenaH3Hyena30.1-H3S699.7'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 77.7800 for spans (25, 35)\n",
      "Best matched text: 'MambaS456.4-MambaHyena28.4MambaMambaS699.8'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 78.4600 for spans (67, 154)\n",
      "Best matched text: 'Train LengthTable 2: (Induction Heads.) Models are trained on sequence length 28 =256, and tested on increasing sequence lengths of 26 = 64 up to 220 =1048576. Full numbers in Table 11.10191020FLOPs (log scale)1016 Ã— 1002 Ã— 101Perplexity (log scale)Scaling Laws on The Pile (Sequence Length 2048)HyenaRWKVTransformerRetNetH3++Transformer++Mamba10191020FLOPs (log scale)1016 Ã— 1002 Ã— 101Perplexity (log scale)Scaling Laws on The Pile (Sequence Length 8192)HyenaRWKVTransformerRetNetH3++Transformer++MambaFigure 4: (Scaling Laws.) Models of size â‰ˆ125ğ‘€to â‰ˆ1.3ğµparameters, trained on the Pile. Mamba scales better than all otherattention-free models and is the first to match the performance of a very strong â€œTransformer++â€ recipe that has now become standard,particularly as the sequence length grows.Out of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightlybetter than the others; also note that all attention models were only tested up to sequence length 214 = 16384 due tomemory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the findings in Poli et al. ('\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 92.7100 for spans (157, 163)\n",
      "Best matched text: '4.2Language ModelingWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on bothpretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to mirror GPT3specifications. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (162, 168)\n",
      "Best matched text: ' 2020), and follow the training recipe described in Brownet al. (2020). All training details are in Appendix E.2.'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 96.0600 for spans (169, 176)\n",
      "Best matched text: '4.2.1Scaling LawsFor baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the strongestTransformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa architectures (e.g.rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher learning rates). We alsocompare against other recent subquadratic architectures (Figure 4). All model details are in Appendix'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 98.9000 for spans (176, 200)\n",
      "Best matched text: '). All model details are in Appendix E.2.Figure 4 shows scaling laws under the standard Chinchilla (Hoffmann et al. 2022) protocol, on models from â‰ˆ125ğ‘€to â‰ˆ1.3ğµparameters. Mamba is the first attention-free model to match the performance of a very strong Transformerrecipe (Transformer++) that has now become standard, particularly as the sequence length grows. (We notethat full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models thatcan also be interpreted as SSMs, because of a lack of efficient implementations leading to out-of-memory or unrealisticcomputation requirements.)11'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.7700 for spans (0, 5)\n",
      "Best matched text: '4.2.2Downstream EvaluationsTable 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compareagainst the most well-known open source models at these sizes, most importantly Pythia (Biderman et al.'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 39.2700 for spans (6, 95)\n",
      "Best matched text: ' 2023) andRWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as ourmodels. (Note that Mamba and Pythia are trained with context length 2048, while RWKV was trained with context length1024.)Table 3: (Zero-shot Evaluations.) Best results for each size in bold. We compare against open source LMs with various tokenizers,trained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and tokenizer(GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches baselines at twicethe model size.ModelToken.PileLAMBADALAMBADAHellaSwagPIQAArc-EArc-CWinoGrandeAverageppl â†“ppl â†“acc â†‘acc â†‘acc â†‘acc â†‘acc â†‘acc â†‘acc â†‘Hybrid H3-130MGPT2â€”89.4825.7731.764.244.424.250.640.1Pythia-160MNeoX29.6438.1033.030.261.443.224.151.940.6Mamba-130MNeoX10.5616.0744.335.364.548.024.351.944.7Hybrid H3-360MGPT2â€”12.5848.041.568.151.424.754.148.0Pythia-410MNeoX9.9510.84'\n",
      "Found 62 unique candidate spans across seeds.\n",
      "Best match score: 95.3600 for spans (277, 329)\n",
      "Best matched text: '64.163.0OPT-6.7BOPTâ€“4.2567.767.276.365.634.965.562.9Pythia-6.9BNeoX6.514.4567.164.075.267.335.561.361.7RWKV-7.4BNeoX6.314.3867.265.576.167.837.561.062.54.3DNA ModelingMotivated by the success of large language models, there has been recent exploration into using the foundation modelparadigm for genomics. DNA has been likened to language in that it consists of sequences of discrete tokens with a finitevocabulary. It is also known for requiring long-range dependencies to model (Avsec et al. 2021). We investigate Mamba asa FM backbone for pretraining and fine-tuning in the same setting as recent works on long-sequence models for DNA(Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling laws across model size and sequencelength (Figure 5), and a difficult downstream synthetic classification task requiring long context (Figure 6).For pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training andmodel details (see also Appendix'\n",
      "Found 9 unique candidate spans across seeds.\n",
      "Best match score: 98.9700 for spans (317, 336)\n",
      "Best matched text: ' 2021). We investigate Mamba asa FM backbone for pretraining and fine-tuning in the same setting as recent works on long-sequence models for DNA(Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling laws across model size and sequencelength (Figure 5), and a difficult downstream synthetic classification task requiring long context (Figure 6).For pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training andmodel details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA (Nguyen, Poli, et al. 2023),which uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5 billion tokens (DNA basepairs) in the training split.12'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.2600 for spans (37, 56)\n",
      "Best matched text: '.) Pretraining on the HG38 (human genome) dataset. (Left) Fixing short context length 210 = 1024 andincreasing size from â‰ˆ200ğ¾to â‰ˆ40ğ‘€parameters, Mamba scales better than baselines. (Right) Fixing model size and increasing sequencelengths while keeping tokens/batch and total training tokens fixed. Unlike baselines, the selection mechanism of Mamba facilitatesbetter performance with increasing context length.'\n",
      "Found 9 unique candidate spans across seeds.\n",
      "Best match score: 97.5200 for spans (58, 98)\n",
      "Best matched text: 'Scaling: Model SizeIn this experiment, we investigate the scaling properties of genomics foundation models with various model backbones(Figure 5 Left).Training.To advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we expectresults to favor Mamba even more at longer sequence lengths. We fix a global batch size of 1024, for a total of 220 â‰ˆ1ğ‘€tokens per batch. Models were trained for 10ğ¾gradient steps for a total of 10ğµtokens.Results.Figure 5 (Left) shows that Mambaâ€™s pretraining perplexity improves smoothly with model size, and that Mambascales better than both HyenaDNA and Transformer++. For example, at the largest model size of â‰ˆ40ğ‘€parameters, thecurve shows that Mamba can match the Transformer++ and HyenaDNA models with roughly 3Ã— to 4Ã— fewerparameters'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 98.8200 for spans (79, 99)\n",
      "Best matched text: 'Results.Figure 5 (Left) shows that Mambaâ€™s pretraining perplexity improves smoothly with model size, and that Mambascales better than both HyenaDNA and Transformer++. For example, at the largest model size of â‰ˆ40ğ‘€parameters, thecurve shows that Mamba can match the Transformer++ and HyenaDNA models with roughly 3Ã— to 4Ã— fewerparameters.'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 98.8800 for spans (66, 138)\n",
      "Best matched text: ' 4.3.2, we expectresults to favor Mamba even more at longer sequence lengths. We fix a global batch size of 1024, for a total of 220 â‰ˆ1ğ‘€tokens per batch. Models were trained for 10ğ¾gradient steps for a total of 10ğµtokens.Results.Figure 5 (Left) shows that Mambaâ€™s pretraining perplexity improves smoothly with model size, and that Mambascales better than both HyenaDNA and Transformer++. For example, at the largest model size of â‰ˆ40ğ‘€parameters, thecurve shows that Mamba can match the Transformer++ and HyenaDNA models with roughly 3Ã— to 4Ã— fewerparameters.4.3.2Scaling: Context LengthIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length. We onlycompare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequencelengths. We pretrain models on sequence lengths 210 = 1024, 212 = 4096, 214 = 16384, 216 = 65536, 218 = 262144,220 = 1048576. We fix a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20ğ¾gradient steps for a total of â‰ˆ330ğµtokens. The longer sequence lengths used sequence length warmup similar to (Nguyen,Poli, et al. 2023).'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 99.1300 for spans (139, 153)\n",
      "Best matched text: 'Results.Figure 5 (Right) shows that Mamba is able to make use of longer context even up to extremely longsequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand, theHyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on properties of theselection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, avery long convolution kernel is aggregating all information across a long sequence which may be very noisy. Note thatwhile HyenaDNA claims to improve with longer context, their results do not control for computation time.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 92.1100 for spans (154, 184)\n",
      "Best matched text: '4.3.3Synthetic Species ClassificationWe evaluate models on a downstream task of classifying between 5 different species by randomly sampling a contiguoussegment of their DNA. This task is adapted from HyenaDNA, which used the species {human, lemur, mouse, pig, hippo}.We modify the task to be significantly more challenging by classifying between the five great apes species{human, chimpanzee, gorilla, orangutan, bonobo}, which are known to share 99% of their DNA.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 1 unique candidate spans across seeds.\n",
      "Best match score: 79.7500 for spans (23, 37)\n",
      "Best matched text: 'Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 210 = 1024 up to 220 = 1048576 usingpretrained models of the same context length. Numerical results inTable 13.'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 98.7100 for spans (22, 62)\n",
      "Best matched text: 'Figure 6: (Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 210 = 1024 up to 220 = 1048576 usingpretrained models of the same context length. Numerical results inTable 13.104105106Sequence Length1.3001.3251.3501.3751.4001.4251.4501.475Bits Per ByteScaling Laws - Sequence Length (YouTubeMix)S4+FFNMambaFigure 7: (Audio Pretraining.) Mamba improves performanceover prior state-of-the-art (Sashimi) in autoregressive audio model-ing, while improving up to minute-long context or million-lengthsequences (controlling for computation).'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 98.1000 for spans (63, 67)\n",
      "Best matched text: '4.4Audio Modeling and GenerationFor the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel et al.2022). This model comprises:'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.9000 for spans (67, 76)\n",
      "Best matched text: '). This model comprises:1. a U-Net backbone with two stages of pooling by a factor ğ‘that doubles the model dimension ğ·per stage,2. alternating S4 and MLP blocks in each stage.We consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4.'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 99.0000 for spans (77, 118)\n",
      "Best matched text: '4.4.1Long-Context Autoregressive PretrainingWe evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a standardpiano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of 16000 Hz. Pretrainingdetails largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates the effect of increasing trainingsequence lengths from 213 = 8192 to 220 â‰ˆ106, while keeping computation fixed. (There are some slight edge cases to theway the data is curated, which may lead to kinks in the scaling curves. For example, only minute-long clips were availableso the maximum sequence length is actually bounded by 60ğ‘ Â· 16000ğ»ğ‘§= 960000.)Both Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba isbetter throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a constantfactor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.We note one important detail: this is the only experiment in this paper in which we switched from the real parameterizationto complex (Section 3.6). We show additional ablations in Appendix'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (118, 120)\n",
      "Best matched text: '). We show additional ablations in Appendix E.4.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 98.8400 for spans (78, 129)\n",
      "Best matched text: 'Long-Context Autoregressive PretrainingWe evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a standardpiano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of 16000 Hz. Pretrainingdetails largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates the effect of increasing trainingsequence lengths from 213 = 8192 to 220 â‰ˆ106, while keeping computation fixed. (There are some slight edge cases to theway the data is curated, which may lead to kinks in the scaling curves. For example, only minute-long clips were availableso the maximum sequence length is actually bounded by 60ğ‘ Â· 16000ğ»ğ‘§= 960000.)Both Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba isbetter throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a constantfactor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.We note one important detail: this is the only experiment in this paper in which we switched from the real parameterizationto complex (Section 3.6). We show additional ablations in Appendix E.4.4.4.2Autoregressive Speech GenerationSC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of1-second clips sampled at 16000 Hz of the digits â€œzeroâ€ through â€œnineâ€ with highly variable characteristics. We largelyfollow the autoregressive training setup and generation protocol of Goel et al. ('\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 98.9200 for spans (66, 159)\n",
      "Best matched text: '2022). This model comprises:1. a U-Net backbone with two stages of pooling by a factor ğ‘that doubles the model dimension ğ·per stage,2. alternating S4 and MLP blocks in each stage.We consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4.4.4.1Long-Context Autoregressive PretrainingWe evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a standardpiano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of 16000 Hz. Pretrainingdetails largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates the effect of increasing trainingsequence lengths from 213 = 8192 to 220 â‰ˆ106, while keeping computation fixed. (There are some slight edge cases to theway the data is curated, which may lead to kinks in the scaling curves. For example, only minute-long clips were availableso the maximum sequence length is actually bounded by 60ğ‘ Â· 16000ğ»ğ‘§= 960000.)Both Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba isbetter throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a constantfactor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.We note one important detail: this is the only experiment in this paper in which we switched from the real parameterizationto complex (Section 3.6). We show additional ablations in Appendix E.4.4.4.2Autoregressive Speech GenerationSC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of1-second clips sampled at 16000 Hz of the digits â€œzeroâ€ through â€œnineâ€ with highly variable characteristics. We largelyfollow the autoregressive training setup and generation protocol of Goel et al. (2022).Table 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al. (2022):WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette 2019), DiffWave (Z.Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art (and much larger) GAN-and diffusion- based models. A larger model parameter-matched to the baselines further improves on fidelity metricsdramatically.Table 5 takes the small Mamba model and investigates combinations of different architectures for the outer stages andcenter stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba > S4+MLP >MHA+MLP in the center blocks.14'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 98.6200 for spans (0, 7)\n",
      "Best matched text: 'Table 4: (SC09) Automated metrics for unconditional generation ona challenging dataset of fixed-length speech clips. (Top to Bottom)Autoregressive baselines, non-autoregressive baselines, Mamba, anddataset metrics.'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 75.0900 for spans (8, 55)\n",
      "Best matched text: 'ModelParamsNLL â†“FID â†“IS â†‘mIS â†‘AM â†“SampleRNN35.0M2.0428.961.713.021.76WaveNet4.2M1.9255.082.275.801.47SaShiMi5.8M1.8731.995.1342.570.74WaveGAN19.1M-2.034.9036.100.80DiffWave24.1M-1.925.2651.210.68+ SaShiMi'\n",
      "Found 16 unique candidate spans across seeds.\n",
      "Best match score: 85.7400 for spans (62, 117)\n",
      "Best matched text: 'Mamba6.1M1.8520.946.2688.540.52Mamba24.3M1.8600.677.33144.90.36Train--0.008.56292.50.16Test--0.028.33257.60.19Table 5: (SC09 Model Ablations) Models with 6M parameters. InSaShiMiâ€™s U-Net backbone, there are 8 center blocks operating onsequence length 1000, sandwiched on each side by 8 outer blocks onsequence length 4000, sandwiched by 8 outer blocks on sequencelength 16000 (40 blocks total). The architecture of the 8 centerblocks are ablated independently of the rest. Note that Transformers(MHA+MLP) were not tested in the more important outer blocksbecause of efficiency constraints.OuterCenter'\n",
      "Found 9 unique candidate spans across seeds.\n",
      "Best match score: 63.7800 for spans (91, 179)\n",
      "Best matched text: 'Test--0.028.33257.60.19Table 5: (SC09 Model Ablations) Models with 6M parameters. InSaShiMiâ€™s U-Net backbone, there are 8 center blocks operating onsequence length 1000, sandwiched on each side by 8 outer blocks onsequence length 4000, sandwiched by 8 outer blocks on sequencelength 16000 (40 blocks total). The architecture of the 8 centerblocks are ablated independently of the rest. Note that Transformers(MHA+MLP) were not tested in the more important outer blocksbecause of efficiency constraints.OuterCenterNLL â†“FID â†“IS â†‘mIS â†‘AM â†“S4+MLPMHA+MLP1.8591.455.0647.030.70S4+MLPS4+MLP1.8671.435.4253.540.65S4+MLPMamba1.8591.425.7156.510.64MambaMHA+MLP1.8501.375.6358.230.62MambaS4+MLP1.8531.076.0573.340.55MambaMamba1.8520.946.2688.540.524.5Speed and Memory BenchmarksWe benchmark the speed of the SSM scan operation (state expansion ğ‘= 16), as well as the end-to-end inferencethroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of(FlashAttention-2 (Dao'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.9200 for spans (170, 193)\n",
      "Best matched text: '4.5Speed and Memory BenchmarksWe benchmark the speed of the SSM scan operation (state expansion ğ‘= 16), as well as the end-to-end inferencethroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40Ã— faster than a standard scan implementationin PyTorch. Mamba achieves 4-5Ã— higher inference throughput than a Transformer of similar size, since without theKV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inferencethroughput than a 5Ã— smaller Transformer-1.3B. Details in Appendix E.5, which additionally includes a benchmark ofmemory consumption.'\n",
      "Found 1 unique candidate spans across seeds.\n",
      "Best match score: 81.2000 for spans (269, 279)\n",
      "Best matched text: 'Efficiency Benchmarks.) (Left) Training: our efficient scan is 40Ã— faster than a standard implementation. (Right) Inference:as a recurrent model, Mamba can achieve 5Ã— higher throughput than Transformers.'\n",
      "Found 14 unique candidate spans across seeds.\n",
      "Best match score: 98.5800 for spans (280, 287)\n",
      "Best matched text: '4.6Model AblationsWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling withsize â‰ˆ350M models at Chinchilla token counts (same setting as Figure 4).'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 97.3300 for spans (288, 303)\n",
      "Best matched text: '4.6.1ArchitectureTable 6 investigates the effects of the architecture (block) and its inner SSM layer (Figure 3). We find thatâ€¢ Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.â€¢ Replacing the complex-valued S4 variant from previous work with a real-valued one does not affect performance much,suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware efficiency.â€¢ Replacing any of these with a selective SSM (S6) significantly improves performance, validating the motivation ofSection 3.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 11 unique candidate spans across seeds.\n",
      "Best match score: 99.4400 for spans (0, 4)\n",
      "Best matched text: 'Table 6: (Ablations: Architecture and SSM layer.) The Mamba block performs similarly to H3 while being simpler. In the inner layer,there is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. Morespecifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.'\n",
      "Found 18 unique candidate spans across seeds.\n",
      "Best match score: 67.9800 for spans (64, 160)\n",
      "Best matched text: ' Î” is the most impor-tant parameter (Theorem 1), but using multiple selective parameterstogether synergizes.Selective Î”Selective ğ‘©Selective ğ‘ªPerplexityâœ—âœ—âœ—10.93âœ—âœ“âœ—10.15âœ—âœ—âœ“9.98âœ“âœ—âœ—9.81âœ“âœ“âœ“8.71Table 8: (Ablations: Parameterization of ğ‘¨.) The morestandard initializations based on S4D-Lin (Gu, Gupta, et al.2022) perform worse than S4D-Real or a random initialization,when the SSM is selective.ğ‘¨ğ‘›InitializationFieldPerplexityğ‘¨ğ‘›= âˆ’12 + ğ‘›ğ‘–Complex9.16ğ‘¨ğ‘›= âˆ’1/2Real8.85ğ‘¨ğ‘›= âˆ’(ğ‘›+ 1)Real8.71ğ‘¨ğ‘›âˆ¼exp(N (0, 1))Real8.71â€¢ The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selectivelayer).We also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybridattention architecture) in Appendix E.2.2.4.6.2Selective SSMTable'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 99.0400 for spans (104, 185)\n",
      "Best matched text: 'when the SSM is selective.ğ‘¨ğ‘›InitializationFieldPerplexityğ‘¨ğ‘›= âˆ’12 + ğ‘›ğ‘–Complex9.16ğ‘¨ğ‘›= âˆ’1/2Real8.85ğ‘¨ğ‘›= âˆ’(ğ‘›+ 1)Real8.71ğ‘¨ğ‘›âˆ¼exp(N (0, 1))Real8.71â€¢ The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selectivelayer).We also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybridattention architecture) in Appendix E.2.2.4.6.2Selective SSMTable 7 ablates the selective SSM layer by considering different combinations of selective Î”, ğ‘©, and ğ‘ªparameters (Algo-rithm 2), showing that Î” is the most important parameter due to its connection to RNN gating (Theorem 1).Table 8 considers different initializations of the SSM, which have been shown to make a large difference in some datamodalities and settings (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022). On language modeling, we find that simplerreal-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued parameterizations (S4D-Lin,row 1) perform better. Random initializations also work well, consistent with findings from prior work (Mehta et al.'\n",
      "Found 12 unique candidate spans across seeds.\n",
      "Best match score: 98.8900 for spans (160, 210)\n",
      "Best matched text: 'Table 7 ablates the selective SSM layer by considering different combinations of selective Î”, ğ‘©, and ğ‘ªparameters (Algo-rithm 2), showing that Î” is the most important parameter due to its connection to RNN gating (Theorem 1).Table 8 considers different initializations of the SSM, which have been shown to make a large difference in some datamodalities and settings (Gu, Goel, and RÃ© 2022; Gu, Gupta, et al. 2022). On language modeling, we find that simplerreal-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued parameterizations (S4D-Lin,row 1) perform better. Random initializations also work well, consistent with findings from prior work (Mehta et al.2023).Table 9 and Table 10 consider varying the dimension of the Î” and (ğ‘©, ğ‘ª) projections respectively. Changing them fromstatic to selective provides the most benefit, while increasing the dimensions further generally improves performancemodestly with a small increase in parameter count.Of particular note is the dramatic improvement of the selective SSM when the state size ğ‘is increased, with over a 1.0perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1and 3.3.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 95.6500 for spans (212, 214)\n",
      "Best matched text: 'DiscussionWe discuss related work, limitations, and some future directions.Related Work.'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 98.7000 for spans (208, 221)\n",
      "Best matched text: 'and 3.3.5DiscussionWe discuss related work, limitations, and some future directions.Related Work.Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has anextended related work of SSMs and other related models.16'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 97.8300 for spans (0, 15)\n",
      "Best matched text: 'Table 9: (Ablations: Expressivity of Î”.)The selection mechanism of Î” constructs itwith a projection of the input. Projecting iteven to dim. 1 provides a large increase inperformance; increasing it further providesfurther improvements at the cost of a mod-est increase in parameters. State size fixedto ğ‘= 16.'\n",
      "Found 1 unique candidate spans across seeds.\n",
      "Best match score: 49.4700 for spans (16, 115)\n",
      "Best matched text: 'Size of Î” proj.Params (M)Perplexity-358.99.121359.18.972359.38.974359.78.918360.58.8316362.18.8432365.28.8064371.58.71Table 10: (Ablations: SSM state dimension.) (Top) Constant ğ‘©and ğ‘ª(Bottom) Selectiveğ‘©and ğ‘ª. Increasing the SSM state dimension ğ‘, which can be viewed as an expansionfactor on the dimension of the recurrent state, can significantly improve performance fora negligible cost in parameters/FLOPs, but only when ğ‘©and ğ‘ªare also selective. Size ofÎ” projection fixed to 64.State dimension ğ‘Params (M)Perplexity1367.19.882367.49.864368.09.828369.19.8216371.59.811367.19.732367.49.404368.09.098369.18.8416371.58.71No Free Lunch: Continuous-Discrete Spectrum.Structured SSMs were originally defined as discretizations ofcontinuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptualsignals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.2400 for spans (126, 134)\n",
      "Best matched text: ' 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sunet al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba stillcompares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges andadjustments to the model that are not discussed in this paper.'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 95.8000 for spans (136, 140)\n",
      "Best matched text: 'ConclusionWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependentreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mambaachieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strongTransformer models. We are excited about the broad applications of selective state space models to build foundation models'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 99.3600 for spans (140, 142)\n",
      "Best matched text: 'Transformer models. We are excited about the broad applications of selective state space models to build foundation modelsfor different domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Ourresults suggest that Mamba is a strong candidate to be a general sequence model backbone.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 99.0000 for spans (143, 144)\n",
      "Best matched text: 'AcknowledgmentsWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.4300 for spans (145, 150)\n",
      "Best matched text: 'References[1]Martin Arjovsky, Amar Shah, and Yoshua Bengio. â€œUnitary Evolution Recurrent Neural Networksâ€. In: The Interna-tional Conference on Machine Learning (ICML). 2016, pp. 1120â€“1128.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 15 unique candidate spans across seeds.\n",
      "Best match score: 99.0100 for spans (0, 5)\n",
      "Best matched text: '[2]Å½iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. â€œEffective Gene Expression Prediction fromSequence by Integrating Long-range Interactionsâ€. In: Nature Methods 18.10 (2021), pp. 1196â€“1203.'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 97.9000 for spans (5, 30)\n",
      "Best matched text: ' 18.10 (2021), pp. 1196â€“1203.[3]Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. â€œUsing Fast Weights to Attendto the Recent Pastâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016).[4]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. â€œLayer Normalizationâ€. In: arXiv preprint arXiv:1607.06450(2016).[5]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. â€œNeural Machine Translation by Jointly Learning toAlign and Translateâ€. In: The International Conference on Learning Representations (ICLR). 2015.[6]David Balduzzi and Muhammad Ghifary. â€œStrongly-typed Recurrent Neural Networksâ€. In: International Conferenceon Machine Learning. PMLR. 2016, pp. 1292â€“1300.[7]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan,Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. â€œPythia: A Suite for AnalyzingLarge Language Models across Training and Scalingâ€. In: The International Conference on Machine Learning (ICML).'\n",
      "Found 20 unique candidate spans across seeds.\n",
      "Best match score: 98.6500 for spans (5, 36)\n",
      "Best matched text: ' 18.10 (2021), pp. 1196â€“1203.[3]Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. â€œUsing Fast Weights to Attendto the Recent Pastâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016).[4]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. â€œLayer Normalizationâ€. In: arXiv preprint arXiv:1607.06450(2016).[5]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. â€œNeural Machine Translation by Jointly Learning toAlign and Translateâ€. In: The International Conference on Learning Representations (ICLR). 2015.[6]David Balduzzi and Muhammad Ghifary. â€œStrongly-typed Recurrent Neural Networksâ€. In: International Conferenceon Machine Learning. PMLR. 2016, pp. 1292â€“1300.[7]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan,Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. â€œPythia: A Suite for AnalyzingLarge Language Models across Training and Scalingâ€. In: The International Conference on Machine Learning (ICML).PMLR. 2023, pp. 2397â€“2430.[8]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. â€œPIQA: Reasoning about Physical Commonsense inNatural Languageâ€. In: Proceedings of the AAAI conference on Artificial Intelligence. Vol. 34. 2020.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 19 unique candidate spans across seeds.\n",
      "Best match score: 98.1700 for spans (14, 87)\n",
      "Best matched text: '(2016).[5]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. â€œNeural Machine Translation by Jointly Learning toAlign and Translateâ€. In: The International Conference on Learning Representations (ICLR). 2015.[6]David Balduzzi and Muhammad Ghifary. â€œStrongly-typed Recurrent Neural Networksâ€. In: International Conferenceon Machine Learning. PMLR. 2016, pp. 1292â€“1300.[7]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan,Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. â€œPythia: A Suite for AnalyzingLarge Language Models across Training and Scalingâ€. In: The International Conference on Machine Learning (ICML).PMLR. 2023, pp. 2397â€“2430.[8]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. â€œPIQA: Reasoning about Physical Commonsense inNatural Languageâ€. In: Proceedings of the AAAI conference on Artificial Intelligence. Vol. 34. 2020.[9]Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,Kyle McDonell, Jason Phang, et al. â€œGpt-NeoX-20B: An Open-source Autoregressive Language Modelâ€. In: arXivpreprint arXiv:2204.06745 (2022).[10]Guy E Blelloch. â€œPrefix Sums and Their Applicationsâ€. In: (1990).[11]James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. â€œQuasi-recurrent Neural Networksâ€. In:arXiv preprint arXiv:1611.01576 (2016).[12]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,Pranav Shyam, Girish Sastry, Amanda Askell, et al. â€œLanguage Models are Few-shot Learnersâ€. In: Advances inNeural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877â€“1901.[13]Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. â€œScaling Transformer to 1M tokens and Beyond with RMTâ€.In: arXiv preprint arXiv:2304.11062 (2023).[14]Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. â€œGenerating Long Sequences with Sparse Transformersâ€.In: arXiv preprint arXiv:1904.10509 (2019).[15]Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, PeterHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. â€œRethinking Attention with Performersâ€. In: TheInternational Conference on Learning Representations (ICLR). 2021.[16]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. â€œPaLM: Scaling Language Modeling with Pathwaysâ€.In: Journal of Machine Learning Research 24.240 (2023), pp. 1â€“113. url: http://jmlr.org/papers/v24/22-1144.html.[17]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. â€œEmpirical Evaluation of Gated RecurrentNeural Networks on Sequence Modelingâ€. In: arXiv preprint arXiv:1412.3555 (2014).[18]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.â€œThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challengeâ€. In:'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 98.4900 for spans (65, 122)\n",
      "Best matched text: '[15]Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, PeterHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. â€œRethinking Attention with Performersâ€. In: TheInternational Conference on Learning Representations (ICLR). 2021.[16]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. â€œPaLM: Scaling Language Modeling with Pathwaysâ€.In: Journal of Machine Learning Research 24.240 (2023), pp. 1â€“113. url: http://jmlr.org/papers/v24/22-1144.html.[17]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. â€œEmpirical Evaluation of Gated RecurrentNeural Networks on Sequence Modelingâ€. In: arXiv preprint arXiv:1412.3555 (2014).[18]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.â€œThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challengeâ€. In: arXiv preprint arXiv:1803.05457(2018).[19]Tri Dao. â€œFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioningâ€. In: The InternationalConference on Learning Representations (ICLR). 2024.[20]Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â€œFlashAttention: Fast and Memory-EfficientExact Attention with IO-Awarenessâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2022.[21]Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher RÃ©. â€œHungry Hungry Hippos:Towards Language Modeling with State Space Modelsâ€. In: The International Conference on Learning Representations(ICLR). 2023.[22]Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. â€œLanguage Modeling with Gated ConvolutionalNetworksâ€. In: The International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933â€“941.[23]DeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017.[24]Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. â€œLongNet:Scaling Transformers to 1,000,000,000 Tokensâ€. In: arXiv preprint arXiv:2307.02486 (2023).18'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 98.5900 for spans (0, 29)\n",
      "Best matched text: '[25]Chris Donahue, Julian McAuley, and Miller Puckette. â€œAdversarial Audio Synthesisâ€. In: The International Conferenceon Learning Representations (ICLR). 2019.[26]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. â€œAn Image is Worth 16x16 Words:Transformers for Image Recognition at Scaleâ€. In: The International Conference on Learning Representations (ICLR).2020.[27]Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, YuntaoBai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,Sam McCandlish, and Chris Olah. â€œA Mathematical Framework for Transformer Circuitsâ€. In: Transformer CircuitsThread (2021). https://transformer-circuits.pub/2021/framework/index.html.[28]Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. â€œBlock-StateTransformerâ€. In: arXiv preprint arXiv:2306.09539 (2023).[29]Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,Yangyang Shi, Ozlem Kalinli, Mike Seltzer, and Mark J. F. Gales. â€œMulti-Head State Space Model for SpeechRecognitionâ€. In: Proc. INTERSPEECH 2023'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 98.3700 for spans (24, 50)\n",
      "Best matched text: ' (2023).[29]Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,Yangyang Shi, Ozlem Kalinli, Mike Seltzer, and Mark J. F. Gales. â€œMulti-Head State Space Model for SpeechRecognitionâ€. In: Proc. INTERSPEECH 2023. 2023, pp. 241â€“245. doi: 10.21437/Interspeech.2023-1036.[30]Karl J Friston, Lee Harrison, and Will Penny. â€œDynamic Causal Modellingâ€. In: Neuroimage 19.4 (2003), pp. 1273â€“1302.[31]Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and ChristopherRÃ©. â€œSimple Hardware-efficient Long Convolutions for Sequence Modelingâ€. In: The International Conference onMachine Learning (ICML) (2023).[32]Ken-ichi Funahashi and Yuichi Nakamura. â€œApproximation of Dynamical Systems by Continuous Time RecurrentNeural Networksâ€. In: Neural Networks 6.6 (1993), pp. 801â€“806.[33]Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.6800 for spans (48, 60)\n",
      "Best matched text: ' 6.6 (1993), pp. 801â€“806.[33]Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. â€œThe Pile: An 800GB Dataset of Diverse Text forLanguage Modelingâ€. In: arXiv preprint arXiv:2101.00027 (2020).[34]Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, JeffreyHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, KevinWang, and Andy Zou. A Framework for Few-shot Language Model Evaluation. Version v0.0.1. Sept. 2021. doi:'\n",
      "Found 19 unique candidate spans across seeds.\n",
      "Best match score: 98.1300 for spans (54, 106)\n",
      "Best matched text: ' (2020).[34]Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, JeffreyHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, KevinWang, and Andy Zou. A Framework for Few-shot Language Model Evaluation. Version v0.0.1. Sept. 2021. doi:10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628.[35]Karan Goel, Albert Gu, Chris Donahue, and Christopher RÃ©. â€œItâ€™s Raw! Audio Generation with State-Space Modelsâ€.In: The International Conference on Machine Learning (ICML). 2022.[36]Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher RÃ©. â€œHIPPO: Recurrent Memory with OptimalPolynomial Projectionsâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2020.[37]Albert Gu, Karan Goel, and Christopher RÃ©. â€œEfficiently Modeling Long Sequences with Structured State Spacesâ€.In: The International Conference on Learning Representations (ICLR). 2022.[38]Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. â€œImproving the Gating Mechanismof Recurrent Neural Networksâ€. In: The International Conference on Machine Learning (ICML). 2020.[39]Albert Gu, Ankit Gupta, Karan Goel, and Christopher RÃ©. â€œOn the Parameterization and Initialization of DiagonalState Space Modelsâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2022.[40]Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher RÃ©. â€œCombining Recurrent,Convolutional, and Continuous-time Models with the Linear State Space Layerâ€. In: Advances in Neural InformationProcessing Systems (NeurIPS). 2021.[41]Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher RÃ©. â€œHow to Train Your HIPPO: State SpaceModels with Generalized Basis Projectionsâ€. In: The International Conference on Learning Representations (ICLR).2023.[42]Ankit Gupta, Albert Gu, and Jonathan Berant. â€œDiagonal State Spaces are as Effective as Structured State Spacesâ€.In: Advances in Neural Information Processing Systems 35 (2022), pp. 22982â€“22994.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.3000 for spans (106, 121)\n",
      "Best matched text: ' 35 (2022), pp. 22982â€“22994.[43]Ankit Gupta, Harsh Mehta, and Jonathan Berant. â€œSimplifying and Understanding State Space Models with DiagonalLinear RNNsâ€. In: arXiv preprint arXiv:2212.00768 (2022).[44]David Ha, Andrew Dai, and Quoc V. Le. â€œHyperNetworksâ€. In: The International Conference on Learning Representa-tions (ICLR). 2017.[45]Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. â€œDream to Control: Learning Behaviors byLatent Imaginationâ€. In: The International Conference on Learning Representations (ICLR). 2020.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 17 unique candidate spans across seeds.\n",
      "Best match score: 98.4200 for spans (0, 12)\n",
      "Best matched text: '[46]Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. â€œLiquidStructural State-Space Modelsâ€. In: The International Conference on Learning Representations (ICLR). 2023.[47]Mikael Henaff, Arthur Szlam, and Yann LeCun. â€œRecurrent Orthogonal Networks and Long-Memory Tasksâ€. In:The International Conference on Machine Learning (ICML). 2016.[48]Dan Hendrycks and Kevin Gimpel. â€œGaussian Error Linear Units (GELUs)â€. In: arXiv preprint arXiv:1606.08415(2016).'\n",
      "Found 11 unique candidate spans across seeds.\n",
      "Best match score: 98.3200 for spans (0, 34)\n",
      "Best matched text: '[46]Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. â€œLiquidStructural State-Space Modelsâ€. In: The International Conference on Learning Representations (ICLR). 2023.[47]Mikael Henaff, Arthur Szlam, and Yann LeCun. â€œRecurrent Orthogonal Networks and Long-Memory Tasksâ€. In:The International Conference on Machine Learning (ICML). 2016.[48]Dan Hendrycks and Kevin Gimpel. â€œGaussian Error Linear Units (GELUs)â€. In: arXiv preprint arXiv:1606.08415(2016).[49]Sepp Hochreiter. â€œUntersuchungen zu dynamischen neuronalen Netzenâ€. In: Diploma, Technische UniversitÃ¤tMÃ¼nchen 91.1 (1991), p. 31.[50]Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, JÃ¼rgen Schmidhuber, et al. Gradient Flow in Recurrent Nets: TheDifficulty of Learning Long-term Dependencies. 2001.[51]Sepp Hochreiter and JÃ¼rgen Schmidhuber. â€œLong Short-Term Memoryâ€. In: Neural Computation 9.8 (1997), pp. 1735â€“1780.[52]Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diegode Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. â€œAn Empirical Analysis of Compute-Optimal Large Language Model Trainingâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 35 (2022),pp. 30016â€“30030.'\n",
      "Found 13 unique candidate spans across seeds.\n",
      "Best match score: 98.4800 for spans (35, 63)\n",
      "Best matched text: '[53]Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. â€œTransformer Quality in Linear Timeâ€. In: The InternationalConference on Machine Learning (ICML). PMLR. 2022, pp. 9099â€“9117.[54]Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. â€œDeepLearning for Time Series Classification: A Reviewâ€. In: Data Mining and Knowledge Discovery 33.4 (2019), pp. 917â€“963.[55]Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. â€œData Movement is All You Need: ACase Study on Optimizing Transformersâ€. In: Proceedings of Machine Learning and Systems 3 (2021), pp. 711â€“732.[56]Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. â€œGatedOrthogonal Recurrent Units: On Learning to Forgetâ€. In: Neural Computation 31.4 (2019), pp. 765â€“783.[57]Rudolph Emil Kalman. â€œA New Approach to Linear Filtering and Prediction Problemsâ€. In: (1960).[58]Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â€œTransformers are RNNs: FastAutoregressive Transformers with Linear Attentionâ€. In: International Conference on Machine Learning. PMLR. 2020,pp. 5156â€“5165.'\n",
      "Found 17 unique candidate spans across seeds.\n",
      "Best match score: 98.5600 for spans (64, 88)\n",
      "Best matched text: '[59]Shiva Kaul. â€œLinear Dynamical Systems as a Core Computational Primitiveâ€. In: Advances in Neural InformationProcessing Systems 33 (2020), pp. 16808â€“16820.[60]Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. â€œDiffWave: A Versatile Diffusion Modelfor Audio Synthesisâ€. In: International Conference on Learning Representations. 2021.[61]Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. â€œTime-Parameterized Convolutional NeuralNetworks for Irregularly Sampled Time Seriesâ€. In: arXiv preprint arXiv:2308.03210 (2023).[62]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. â€œImageNet Classification with Deep Convolutional NeuralNetworksâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 25 (2012).[63]Tao Lei. â€œWhen Attention Meets Fast Recurrence: Training Language Models with Reduced Computeâ€. In: Proceedingsof the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 7633â€“7648.'\n",
      "Found 17 unique candidate spans across seeds.\n",
      "Best match score: 98.4300 for spans (69, 109)\n",
      "Best matched text: '[60]Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. â€œDiffWave: A Versatile Diffusion Modelfor Audio Synthesisâ€. In: International Conference on Learning Representations. 2021.[61]Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. â€œTime-Parameterized Convolutional NeuralNetworks for Irregularly Sampled Time Seriesâ€. In: arXiv preprint arXiv:2308.03210 (2023).[62]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. â€œImageNet Classification with Deep Convolutional NeuralNetworksâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 25 (2012).[63]Tao Lei. â€œWhen Attention Meets Fast Recurrence: Training Language Models with Reduced Computeâ€. In: Proceedingsof the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 7633â€“7648.[64]Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. â€œSimple Recurrent Units for Highly ParallelizableRecurrenceâ€. In: arXiv preprint arXiv:1709.02755 (2017).[65]Mario Lezcano-Casado and David MartÃ­nez-Rubio. â€œCheap Orthogonal Constraints in Neural Networks: A SimpleParametrization of the Orthogonal and Unitary Groupâ€. In: The International Conference on Machine Learning(ICML). 2019.[66]Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. â€œWhat Makes Convolutional Models Greaton Long Sequence Modeling?â€ In: The International Conference on Learning Representations (ICLR). 2023.[67]Vasileios Lioutas and Yuhong Guo. â€œTime-aware Large Kernel Convolutionsâ€. In: The International Conference onMachine Learning (ICML). PMLR. 2020, pp. 6172â€“6183.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 97.8500 for spans (27, 120)\n",
      "Best matched text: '1780.[52]Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diegode Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. â€œAn Empirical Analysis of Compute-Optimal Large Language Model Trainingâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 35 (2022),pp. 30016â€“30030.[53]Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. â€œTransformer Quality in Linear Timeâ€. In: The InternationalConference on Machine Learning (ICML). PMLR. 2022, pp. 9099â€“9117.[54]Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. â€œDeepLearning for Time Series Classification: A Reviewâ€. In: Data Mining and Knowledge Discovery 33.4 (2019), pp. 917â€“963.[55]Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. â€œData Movement is All You Need: ACase Study on Optimizing Transformersâ€. In: Proceedings of Machine Learning and Systems 3 (2021), pp. 711â€“732.[56]Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. â€œGatedOrthogonal Recurrent Units: On Learning to Forgetâ€. In: Neural Computation 31.4 (2019), pp. 765â€“783.[57]Rudolph Emil Kalman. â€œA New Approach to Linear Filtering and Prediction Problemsâ€. In: (1960).[58]Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. â€œTransformers are RNNs: FastAutoregressive Transformers with Linear Attentionâ€. In: International Conference on Machine Learning. PMLR. 2020,pp. 5156â€“5165.[59]Shiva Kaul. â€œLinear Dynamical Systems as a Core Computational Primitiveâ€. In: Advances in Neural InformationProcessing Systems 33 (2020), pp. 16808â€“16820.[60]Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. â€œDiffWave: A Versatile Diffusion Modelfor Audio Synthesisâ€. In: International Conference on Learning Representations. 2021.[61]Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. â€œTime-Parameterized Convolutional NeuralNetworks for Irregularly Sampled Time Seriesâ€. In: arXiv preprint arXiv:2308.03210 (2023).[62]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. â€œImageNet Classification with Deep Convolutional NeuralNetworksâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 25 (2012).[63]Tao Lei. â€œWhen Attention Meets Fast Recurrence: Training Language Models with Reduced Computeâ€. In: Proceedingsof the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 7633â€“7648.[64]Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. â€œSimple Recurrent Units for Highly ParallelizableRecurrenceâ€. In: arXiv preprint arXiv:1709.02755 (2017).[65]Mario Lezcano-Casado and David MartÃ­nez-Rubio. â€œCheap Orthogonal Constraints in Neural Networks: A SimpleParametrization of the Orthogonal and Unitary Groupâ€. In: The International Conference on Machine Learning(ICML). 2019.[66]Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. â€œWhat Makes Convolutional Models Greaton Long Sequence Modeling?â€ In: The International Conference on Learning Representations (ICLR). 2023.[67]Vasileios Lioutas and Yuhong Guo. â€œTime-aware Large Kernel Convolutionsâ€. In: The International Conference onMachine Learning (ICML). PMLR. 2020, pp. 6172â€“6183.[68]Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani.â€œStructured State Space Models for In-Context Reinforcement Learningâ€. In: Advances in Neural Information ProcessingSystems (NeurIPS). 2023.[69]Shahar Lutati, Itamar Zimerman, and Lior Wolf. â€œFocus Your Attention (with Adaptive IIR Filters)â€. In: arXiv preprintarXiv:2305.14952 (2023).'\n",
      "Found 13 unique candidate spans across seeds.\n",
      "Best match score: 98.5500 for spans (0, 39)\n",
      "Best matched text: '[70]Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and LukeZettlemoyer. â€œMega: Moving Average Equipped Gated Attentionâ€. In: The International Conference on LearningRepresentations (ICLR). 2023.[71]Eric Martin and Chris Cundy. â€œParallelizing Linear Recurrent Neural Nets Over Sequence Lengthâ€. In: The Interna-tional Conference on Learning Representations (ICLR). 2018.[72]Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, andYoshua Bengio. â€œSampleRNN: An Unconditional End-to-End Neural Audio Generation Modelâ€. In: The InternationalConference on Learning Representations (ICLR). 2017.[73]Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. â€œLong Range Language Modeling via GatedState Spacesâ€. In: The International Conference on Learning Representations (ICLR). 2023.[74]Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. â€œEfficient Orthogonal Parametrisationof Recurrent Neural Networks using Householder Reflectionsâ€. In: International Conference on Machine Learning.PMLR. 2017, pp. 2401â€“2409.[75]Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and ChristopherRÃ©. â€œS4ND: Modeling Images and Videos as Multidimensional Signals with State Spacesâ€. In: Advances in NeuralInformation Processing Systems (NeurIPS). 2022.[76]Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel,Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. â€œHyenaDNA: Long-range Genomic Sequence Modelingat Single Nucleotide Resolutionâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2023.'\n",
      "Found 24 unique candidate spans across seeds.\n",
      "Best match score: 98.6500 for spans (0, 71)\n",
      "Best matched text: '[70]Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and LukeZettlemoyer. â€œMega: Moving Average Equipped Gated Attentionâ€. In: The International Conference on LearningRepresentations (ICLR). 2023.[71]Eric Martin and Chris Cundy. â€œParallelizing Linear Recurrent Neural Nets Over Sequence Lengthâ€. In: The Interna-tional Conference on Learning Representations (ICLR). 2018.[72]Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, andYoshua Bengio. â€œSampleRNN: An Unconditional End-to-End Neural Audio Generation Modelâ€. In: The InternationalConference on Learning Representations (ICLR). 2017.[73]Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. â€œLong Range Language Modeling via GatedState Spacesâ€. In: The International Conference on Learning Representations (ICLR). 2023.[74]Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. â€œEfficient Orthogonal Parametrisationof Recurrent Neural Networks using Householder Reflectionsâ€. In: International Conference on Machine Learning.PMLR. 2017, pp. 2401â€“2409.[75]Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and ChristopherRÃ©. â€œS4ND: Modeling Images and Videos as Multidimensional Signals with State Spacesâ€. In: Advances in NeuralInformation Processing Systems (NeurIPS). 2022.[76]Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel,Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. â€œHyenaDNA: Long-range Genomic Sequence Modelingat Single Nucleotide Resolutionâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2023.[77]Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,Jared Kaplan, Sam McCandlish, and Chris Olah. â€œIn-context Learning and Induction Headsâ€. In: Transformer CircuitsThread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.[78]Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,Andrew Senior, and Koray Kavukcuoglu. â€œWaveNet: A Generative Model for Raw Audioâ€. In: arXiv preprintarXiv:1609.03499 (2016).[79]Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.â€œResurrecting Recurrent Neural Networks for Long Sequencesâ€. In: The International Conference on Machine Learning(ICML). 2023.[80]Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle,Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. â€œThe LAMBADA Dataset: Word Prediction Requiring a BroadDiscourse Contextâ€. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2016,pp. 1525â€“1534.[81]Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. â€œOn the Difficulty of Training Recurrent Neural Networksâ€.In: International Conference on Machine Learning. 2013, pp. 1310â€“1318.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.4500 for spans (71, 77)\n",
      "Best matched text: '. 2013, pp. 1310â€“1318.[82]Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung,Matteo Grella, Kranthi Kiran GV, et al. â€œRWKV: Reinventing RNNs for the Transformer Eraâ€. In: arXiv preprintarXiv:2305.13048 (2023).'\n",
      "Found 12 unique candidate spans across seeds.\n",
      "Best match score: 98.5200 for spans (67, 99)\n",
      "Best matched text: '[81]Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. â€œOn the Difficulty of Training Recurrent Neural Networksâ€.In: International Conference on Machine Learning. 2013, pp. 1310â€“1318.[82]Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung,Matteo Grella, Kranthi Kiran GV, et al. â€œRWKV: Reinventing RNNs for the Transformer Eraâ€. In: arXiv preprintarXiv:2305.13048 (2023).[83]Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. â€œRandom FeatureAttentionâ€. In: The International Conference on Learning Representations (ICLR). 2021.[84]Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,and Christopher RÃ©. â€œHyena Hierarchy: Towards Larger Convolutional Language Modelsâ€. In: The InternationalConference on Machine Learning (ICML). 2023.[85]Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, andYiran Zhong. â€œToeplitz Neural Network for Sequence Modelingâ€. In: The International Conference on LearningRepresentations (ICLR). 2023.[86]Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. â€œThe devil inlinear transformerâ€. In: arXiv preprint arXiv:2210.10340 (2022).'\n",
      "Found 22 unique candidate spans across seeds.\n",
      "Best match score: 98.2500 for spans (17, 111)\n",
      "Best matched text: '[73]Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. â€œLong Range Language Modeling via GatedState Spacesâ€. In: The International Conference on Learning Representations (ICLR). 2023.[74]Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. â€œEfficient Orthogonal Parametrisationof Recurrent Neural Networks using Householder Reflectionsâ€. In: International Conference on Machine Learning.PMLR. 2017, pp. 2401â€“2409.[75]Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and ChristopherRÃ©. â€œS4ND: Modeling Images and Videos as Multidimensional Signals with State Spacesâ€. In: Advances in NeuralInformation Processing Systems (NeurIPS). 2022.[76]Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel,Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. â€œHyenaDNA: Long-range Genomic Sequence Modelingat Single Nucleotide Resolutionâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2023.[77]Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, AmandaAskell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,Jared Kaplan, Sam McCandlish, and Chris Olah. â€œIn-context Learning and Induction Headsâ€. In: Transformer CircuitsThread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.[78]Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner,Andrew Senior, and Koray Kavukcuoglu. â€œWaveNet: A Generative Model for Raw Audioâ€. In: arXiv preprintarXiv:1609.03499 (2016).[79]Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.â€œResurrecting Recurrent Neural Networks for Long Sequencesâ€. In: The International Conference on Machine Learning(ICML). 2023.[80]Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle,Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez. â€œThe LAMBADA Dataset: Word Prediction Requiring a BroadDiscourse Contextâ€. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 2016,pp. 1525â€“1534.[81]Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. â€œOn the Difficulty of Training Recurrent Neural Networksâ€.In: International Conference on Machine Learning. 2013, pp. 1310â€“1318.[82]Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung,Matteo Grella, Kranthi Kiran GV, et al. â€œRWKV: Reinventing RNNs for the Transformer Eraâ€. In: arXiv preprintarXiv:2305.13048 (2023).[83]Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. â€œRandom FeatureAttentionâ€. In: The International Conference on Learning Representations (ICLR). 2021.[84]Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,and Christopher RÃ©. â€œHyena Hierarchy: Towards Larger Convolutional Language Modelsâ€. In: The InternationalConference on Machine Learning (ICML). 2023.[85]Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, andYiran Zhong. â€œToeplitz Neural Network for Sequence Modelingâ€. In: The International Conference on LearningRepresentations (ICLR). 2023.[86]Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. â€œThe devil inlinear transformerâ€. In: arXiv preprint arXiv:2210.10340 (2022).[87]Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and YiranZhong. â€œCosFormer: Rethinking Softmax in Attentionâ€. In: The International Conference on Learning Representations(ICLR). 2022.[88]Ali Rahimi and Benjamin Recht. â€œRandom Features for Large-Scale Kernel Machinesâ€. In: Advances in NeuralInformation Processing Systems (NeurIPS) 20 (2007).21'\n",
      "Found 14 unique candidate spans across seeds.\n",
      "Best match score: 94.8300 for spans (5, 104)\n",
      "Best matched text: '[90]David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. â€œCKConv: ContinuousKernel Convolution For Sequential Dataâ€. In: arXiv preprint arXiv:2102.02611 (2021).[91]Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. â€œWinogrande: An Adversarial WinogradSchema Challenge at Scaleâ€. In: Communications of the ACM 64.9 (2021), pp. 99â€“106.[92]George Saon, Ankit Gupta, and Xiaodong Cui. â€œDiagonal State Space Augmented Transformers for Speech Recogni-tionâ€. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE.2023, pp. 1â€“5.[93]Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. â€œLinear Transformers are Secretly Fast Weight Programmersâ€.In: The International Conference on Machine Learning (ICML). PMLR. 2021, pp. 9355â€“9366.[94]JÃ¼rgen Schmidhuber. â€œLearning to control fast-weight memories: An alternative to dynamic recurrent networksâ€.In: Neural Computation 4.1 (1992), pp. 131â€“139.[95]Noam Shazeer. â€œGLU Variants Improve Transformerâ€. In: arXiv preprint arXiv:2002.05202 (2020).[96]Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael SchÃ¤rli, and DennyZhou. â€œLarge Language Models can be Easily Distracted by Irrelevant Contextâ€. In: The International Conference onMachine Learning (ICML). PMLR. 2023, pp. 31210â€“31227.[97]Jiaxin Shi, Ke Alexander Wang, and Emily Fox. â€œSequence Modeling with Multiresolution Convolutional Memoryâ€.In: The International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31312â€“31327.[98]Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. â€œSimplified State Space Layers for SequenceModelingâ€. In: The International Conference on Learning Representations (ICLR). 2023.[99]Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. â€œRoformer: Enhanced Transformerwith Rotary Position Embeddingâ€. In: arXiv preprint arXiv:2104.09864 (2021).[100]Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. â€œRetentivenetwork: A successor to transformer for large language modelsâ€. In: arXiv preprint arXiv:2307.08621 (2023).[101]Ilya Sutskever, Oriol Vinyals, and Quoc V Le. â€œSequence to Sequence Learning with Neural Networksâ€. In: Advancesin Neural Information Processing Systems (NeurIPS) 27 (2014).[102]Corentin Tallec and Yann Ollivier. â€œCan Recurrent Neural Networks Warp Time?â€ In: The International Conferenceon Learning Representations (ICLR). 2018.[103]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, SebastianRuder, and Donald Metzler. â€œLong Range Arena: A Benchmark for Efficient Transformersâ€. In: InternationalConference on Learning Representations (ICLR). 2021.[104]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. â€œEfficient Transformers: A Surveyâ€. In: ACM ComputingSurveys 55.6 (2022), pp. 1â€“28.[105]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, BaptisteRoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. â€œLlama: Open and Efficient Foundation Language Modelsâ€.In: arXiv preprint arXiv:2302.13971 (2023).[106]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, andIllia Polosukhin. â€œAttention Is All You Needâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2017.[107]Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. â€œOn Orthogonality and Learning RecurrentNetworks with Long Term Dependenciesâ€. In: International Conference on Machine Learning. PMLR. 2017, pp. 3570â€“3578.[108]Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. â€œSelective StructuredState-Spaces for Long-form Video Understandingâ€. In: Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. 2023, pp. 6387â€“6397.'\n",
      "Found 13 unique candidate spans across seeds.\n",
      "Best match score: 97.7400 for spans (66, 109)\n",
      "Best matched text: '[102]Corentin Tallec and Yann Ollivier. â€œCan Recurrent Neural Networks Warp Time?â€ In: The International Conferenceon Learning Representations (ICLR). 2018.[103]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, SebastianRuder, and Donald Metzler. â€œLong Range Arena: A Benchmark for Efficient Transformersâ€. In: InternationalConference on Learning Representations (ICLR). 2021.[104]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. â€œEfficient Transformers: A Surveyâ€. In: ACM ComputingSurveys 55.6 (2022), pp. 1â€“28.[105]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, BaptisteRoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. â€œLlama: Open and Efficient Foundation Language Modelsâ€.In: arXiv preprint arXiv:2302.13971 (2023).[106]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, andIllia Polosukhin. â€œAttention Is All You Needâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2017.[107]Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. â€œOn Orthogonality and Learning RecurrentNetworks with Long Term Dependenciesâ€. In: International Conference on Machine Learning. PMLR. 2017, pp. 3570â€“3578.[108]Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. â€œSelective StructuredState-Spaces for Long-form Video Understandingâ€. In: Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. 2023, pp. 6387â€“6397.[109]Pete Warden. â€œSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognitionâ€. In: ArXiv abs/1804.03209(2018).'\n",
      "Found 16 unique candidate spans across seeds.\n",
      "Best match score: 98.9200 for spans (66, 114)\n",
      "Best matched text: '[102]Corentin Tallec and Yann Ollivier. â€œCan Recurrent Neural Networks Warp Time?â€ In: The International Conferenceon Learning Representations (ICLR). 2018.[103]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, SebastianRuder, and Donald Metzler. â€œLong Range Arena: A Benchmark for Efficient Transformersâ€. In: InternationalConference on Learning Representations (ICLR). 2021.[104]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. â€œEfficient Transformers: A Surveyâ€. In: ACM ComputingSurveys 55.6 (2022), pp. 1â€“28.[105]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, BaptisteRoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. â€œLlama: Open and Efficient Foundation Language Modelsâ€.In: arXiv preprint arXiv:2302.13971 (2023).[106]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, andIllia Polosukhin. â€œAttention Is All You Needâ€. In: Advances in Neural Information Processing Systems (NeurIPS). 2017.[107]Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. â€œOn Orthogonality and Learning RecurrentNetworks with Long Term Dependenciesâ€. In: International Conference on Machine Learning. PMLR. 2017, pp. 3570â€“3578.[108]Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. â€œSelective StructuredState-Spaces for Long-form Video Understandingâ€. In: Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. 2023, pp. 6387â€“6397.[109]Pete Warden. â€œSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognitionâ€. In: ArXiv abs/1804.03209(2018).[110]Samuel Williams, Andrew Waterman, and David Patterson. â€œRoofline: An Insightful Visual Performance Model forMulticore Architecturesâ€. In: Communications of the ACM 52.4 (2009), pp. 65â€“76.'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 98.6700 for spans (110, 119)\n",
      "Best matched text: '[110]Samuel Williams, Andrew Waterman, and David Patterson. â€œRoofline: An Insightful Visual Performance Model forMulticore Architecturesâ€. In: Communications of the ACM 52.4 (2009), pp. 65â€“76.[111]Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. â€œCondConv: Conditionally Parameterized Convolu-tions for Efficient Inferenceâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019).'\n",
      "Found 16 unique candidate spans across seeds.\n",
      "Best match score: 99.1300 for spans (99, 124)\n",
      "Best matched text: '[108]Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. â€œSelective StructuredState-Spaces for Long-form Video Understandingâ€. In: Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition. 2023, pp. 6387â€“6397.[109]Pete Warden. â€œSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognitionâ€. In: ArXiv abs/1804.03209(2018).[110]Samuel Williams, Andrew Waterman, and David Patterson. â€œRoofline: An Insightful Visual Performance Model forMulticore Architecturesâ€. In: Communications of the ACM 52.4 (2009), pp. 65â€“76.[111]Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. â€œCondConv: Conditionally Parameterized Convolu-tions for Efficient Inferenceâ€. In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019).[112]Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. â€œHellaSwag: Can a Machine Really FinishYour Sentence?â€ In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 98.8300 for spans (0, 13)\n",
      "Best matched text: '[113]Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind.â€œAn Attention Free Transformerâ€. In: arXiv preprint arXiv:2105.14103 (2021).[114]Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher RÃ©. â€œEffectively Modeling TimeSeries with Simple Discrete State Spacesâ€. In: The International Conference on Learning Representations (ICLR). 2023.[115]Lin Zheng, Chong Wang, and Lingpeng Kong. â€œLinear complexity randomized self-attention mechanismâ€. In:International Conference on Machine Learning. PMLR. 2022, pp. 27011â€“27041.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.2900 for spans (4, 19)\n",
      "Best matched text: ' (2021).[114]Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher RÃ©. â€œEffectively Modeling TimeSeries with Simple Discrete State Spacesâ€. In: The International Conference on Learning Representations (ICLR). 2023.[115]Lin Zheng, Chong Wang, and Lingpeng Kong. â€œLinear complexity randomized self-attention mechanismâ€. In:International Conference on Machine Learning. PMLR. 2022, pp. 27011â€“27041.[116]Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. â€œEfficient LongSequence Modeling via State Space Augmented Transformerâ€. In: arXiv preprint arXiv:2212.08136 (2022).23'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.4200 for spans (1, 53)\n",
      "Best matched text: 'Discussion: Selection MechanismOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. Itcan also be viewed as related to â€œfast weightsâ€ (J. Ba et al. 2016; Schmidhuber 1992), which connects classical RNNs withthe mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct conceptthat is worth clarifying.Gating.Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber1997) and GRU (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanismfor controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signalthrough time and causes inputs to interact along the sequence length dimension.However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction(often with an activation function). For example, elementwise multiplicative components of neural network architectures(that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehtaet al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of RNNgating versus the popular usage of multiplicative gating actually have a very different semantic meaning.Hypernetworks.Hypernetworks refer to neural networks whose parameters are themselves generated by smallerneural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whoserecurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber1992).Data-dependence.Similar to hypernetworks, data-dependence can refer to any notion where some parameters of themodel depend on the data (Poli et al. 2023).'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.1400 for spans (54, 99)\n",
      "Best matched text: 'Example: GLU Activation.To illustrate the issues with these concepts, consider a simple diagonal linear layer ğ‘¦= ğ‘«ğ‘¥,where ğ‘«is a diagonal weight parameter. Now suppose that ğ‘«is itself generated from a linear transformation of ğ‘¥,with an optional nonlinearity: ğ‘«= ğœ(ğ‘¾ğ‘¥). Since it is diagonal, the multiplication becomes an elementwise product:ğ‘¦= ğœ(ğ‘¾ğ‘¥) â—¦ğ‘¥.This is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicativeâ€œbranchâ€), hypernetworks (since the parameter ğ‘«is generated by another layer), and data-dependent (since ğ‘«dependson the data ğ‘¥). However, this in fact simply defines a GLU function, which is so simple that it is often considered just anactivation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer.'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 99.1000 for spans (100, 139)\n",
      "Best matched text: 'Selection.Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating,hypernetworks, or data-dependence, so can an enormous range of other constructionsâ€”essentially anything with amultiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) aswellâ€”and we find it uninformative to think of them as such.Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1)and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of Î” (Funahashiand Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term â€œgatingâ€ in favor of selection toclarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to selector ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gatedRNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas andGuo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.3900 for spans (141, 143)\n",
      "Best matched text: 'Related WorkWe overview several prior works related to our methods. We mention that some of the most closely related models includerecurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 99.1200 for spans (0, 18)\n",
      "Best matched text: 'B.1S4 Variants and DerivativesWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to ourmethod.â€¢ S4 (Gu, Goel, and RÃ© 2022; Gu, Johnson, Goel, et al. 2021) introduced the first structured SSM, describing diagonalstructure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to aconnection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020).â€¢ DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximat-ing the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al.'\n",
      "Found 24 unique candidate spans across seeds.\n",
      "Best match score: 99.2500 for spans (6, 54)\n",
      "Best matched text: ' 2022; Gu, Johnson, Goel, et al. 2021) introduced the first structured SSM, describing diagonalstructure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to aconnection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020).â€¢ DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximat-ing the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022).â€¢ S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is thefirst S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective statedimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) toMIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISOdimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome thecomputation issue, (iii) adding the selection mechanism.Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Theirmechanism can be viewed as a particular hard-coded instance of a selection mechanism, where ğ‘¨is manually set to 0,instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs genericallyto this setting and probe if the model has learned to automatically reset its state on episode boundaries.â€¢ Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation ofbeing an exponential moving average (EMA). They additionally make an interesting connection of the discretization stepof SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show thatreal-valued SSMs are empirically effective in certain settings or when combined with different architectural components.â€¢ Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From thisperspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionallyand close to LTI.â€¢ SGConv (Y. Li et al.'\n",
      "Found 24 unique candidate spans across seeds.\n",
      "Best match score: 99.0700 for spans (16, 77)\n",
      "Best matched text: ' 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximat-ing the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022).â€¢ S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is thefirst S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective statedimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) toMIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISOdimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome thecomputation issue, (iii) adding the selection mechanism.Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Theirmechanism can be viewed as a particular hard-coded instance of a selection mechanism, where ğ‘¨is manually set to 0,instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs genericallyto this setting and probe if the model has learned to automatically reset its state on episode boundaries.â€¢ Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation ofbeing an exponential moving average (EMA). They additionally make an interesting connection of the discretization stepof SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show thatreal-valued SSMs are empirically effective in certain settings or when combined with different architectural components.â€¢ Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From thisperspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionallyand close to LTI.â€¢ SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation ofS4 and create global or long convolution kernels with different parameterizations. However, these methods cannot dofast autoregressive inference directly.Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usuallystrictly LTI (linear time invariant).B.2SSM ArchitecturesWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporatingone of the previous SSMs as a black box layer.â€¢ GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gatedattention unit (GAU) of Hua et al. ('\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 96.5200 for spans (69, 89)\n",
      "Best matched text: 'B.2SSM ArchitecturesWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporatingone of the previous SSMs as a black box layer.â€¢ GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gatedattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Mostimportantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands themodel dimension in order to increase the state size, based on the motivation in Section 3.1.â€¢ Mega (Ma et al.'\n",
      "Found 14 unique candidate spans across seeds.\n",
      "Best match score: 99.1900 for spans (40, 109)\n",
      "Best matched text: ' 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation ofbeing an exponential moving average (EMA). They additionally make an interesting connection of the discretization stepof SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show thatreal-valued SSMs are empirically effective in certain settings or when combined with different architectural components.â€¢ Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From thisperspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionallyand close to LTI.â€¢ SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation ofS4 and create global or long convolution kernels with different parameterizations. However, these methods cannot dofast autoregressive inference directly.Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usuallystrictly LTI (linear time invariant).B.2SSM ArchitecturesWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporatingone of the previous SSMs as a black box layer.â€¢ GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gatedattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Mostimportantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands themodel dimension in order to increase the state size, based on the motivation in Section 3.1.â€¢ Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficientattention approximation.â€¢ H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It isthe first to generalize this formulation of linear attention to more general recurrences, which is also the basis of laterarchitectures.â€¢ Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on theinput. While sharing the â€œselectionâ€ name, we consider this an architectural modification that is closer to architecturalgating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective25'\n",
      "Found 1 unique candidate spans across seeds.\n",
      "Best match score: 99.2600 for spans (0, 22)\n",
      "Best matched text: 'Copying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones(indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0).â€¢ RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to aspecial case where the state dimension is ğ‘= 1. Although not framed as such, its recurrence can be viewed as a specialcase of a linear SSM.Its primary source of improvement is using a linear attention with large head dimension, which can be viewed as anothermethod to perform input-dependent state expansion. Using a larger head dimension in the context of linear attentionvariants was first done by H3, but not extensively used since this requires a proportional amount of extra computation.RetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attentioninstead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.â€¢ RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-freeTransformer (S. Zhai et al.'\n",
      "Found 9 unique candidate spans across seeds.\n",
      "Best match score: 98.9400 for spans (10, 46)\n",
      "Best matched text: 'case of a linear SSM.Its primary source of improvement is using a linear attention with large head dimension, which can be viewed as anothermethod to perform input-dependent state expansion. Using a larger head dimension in the context of linear attentionvariants was first done by H3, but not extensively used since this requires a proportional amount of extra computation.RetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attentioninstead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.â€¢ RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-freeTransformer (S. Zhai et al. 2021)), another variant of linear attention. Its main â€œWKVâ€ mechanism involves LTI recurrencesand can be seen as the ratio of two SSMs.We also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformerâ€™sMHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLPblocks.B.3Relationship to RNNsRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state.Several older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016),and simple recurrent unit (SRU) (Lei 2021'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 96.8200 for spans (34, 87)\n",
      "Best matched text: 'Relationship to RNNsRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state.Several older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016),and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities.Because of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs,and are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:â€¢ They do not use state expansion (ğ‘= 1) or selective ğ‘©, ğ‘ªparameters, both of which are important for performance(Section 4.6).â€¢ They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +discretization (Theorem 1). The connections to principled SSM theory provides better parameterizations andinitializations (Section 3.6).Additionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991;Hochreiter, Bengio, et al. 2001; Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The formercould be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the latterwas difficult without theory later developed for SSMs. For example, modern structured SSMs differ in more carefulparameterization of the recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson,Goel, et al. 2021; Gu, Johnson, Timalsina, et al.'\n",
      "Found 26 unique candidate spans across seeds.\n",
      "Best match score: 99.1300 for spans (20, 116)\n",
      "Best matched text: ' 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-freeTransformer (S. Zhai et al. 2021)), another variant of linear attention. Its main â€œWKVâ€ mechanism involves LTI recurrencesand can be seen as the ratio of two SSMs.We also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformerâ€™sMHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLPblocks.B.3Relationship to RNNsRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state.Several older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016),and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities.Because of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs,and are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:â€¢ They do not use state expansion (ğ‘= 1) or selective ğ‘©, ğ‘ªparameters, both of which are important for performance(Section 4.6).â€¢ They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +discretization (Theorem 1). The connections to principled SSM theory provides better parameterizations andinitializations (Section 3.6).Additionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991;Hochreiter, Bengio, et al. 2001; Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The formercould be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the latterwas difficult without theory later developed for SSMs. For example, modern structured SSMs differ in more carefulparameterization of the recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson,Goel, et al. 2021; Gu, Johnson, Timalsina, et al. 2023)), or direct analysis (Gupta, Mehta, and Berant 2022; Kaul 2020; Orvietoet al. 2023)).We also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Henaff, Szlam,and LeCun 2016; Lezcano-Casado and MartÃ­nez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017) which aremotivated by constraining the ğ‘¨transition matrix to be orthogonal or unitary, in order to control its eigenvalues andprevent the vanishing gradient problem. However, these had other limitations; we believe that these stem from the factthat orthogonal/unitary RNNs are also LTI. For example, they are almost always evaluated on the Copying task which theycan solve perfectly, but observed to struggle on the Selective Copying task (Jing et al. 2019).'\n",
      "Found 9 unique candidate spans across seeds.\n",
      "Best match score: 90.5000 for spans (66, 122)\n",
      "Best matched text: 'discretization (Theorem 1). The connections to principled SSM theory provides better parameterizations andinitializations (Section 3.6).Additionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991;Hochreiter, Bengio, et al. 2001; Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The formercould be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the latterwas difficult without theory later developed for SSMs. For example, modern structured SSMs differ in more carefulparameterization of the recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson,Goel, et al. 2021; Gu, Johnson, Timalsina, et al. 2023)), or direct analysis (Gupta, Mehta, and Berant 2022; Kaul 2020; Orvietoet al. 2023)).We also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Henaff, Szlam,and LeCun 2016; Lezcano-Casado and MartÃ­nez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017) which aremotivated by constraining the ğ‘¨transition matrix to be orthogonal or unitary, in order to control its eigenvalues andprevent the vanishing gradient problem. However, these had other limitations; we believe that these stem from the factthat orthogonal/unitary RNNs are also LTI. For example, they are almost always evaluated on the Copying task which theycan solve perfectly, but observed to struggle on the Selective Copying task (Jing et al. 2019).B.4Linear AttentionThe Linear Attention (LA) (Katharopoulos et al. 2020) framework is an important result popularizing kernel attention andshowing how it relates to recurrent autoregressive models. Many variants have proposed alternative kernels and other'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.0500 for spans (121, 134)\n",
      "Best matched text: ') framework is an important result popularizing kernel attention andshowing how it relates to recurrent autoregressive models. Many variants have proposed alternative kernels and othermodifications. Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmaxattention (i.e. the exp feature map) using the random Fourier feature approximation of Gaussian kernels (Rahimi andRecht 2007). Performer (Choromanski et al. 2021) finds an approximation to the exponential kernel involving only positive26'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 99.1700 for spans (0, 13)\n",
      "Best matched text: 'features, which also allows the softmax normalization term. TransNormer (Qin, Han, W. Sun, D. Li, et al. 2022) showedthat the LA denominator term can be unstable and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al.2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality.Linear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importancesampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformednumerator).Aside from kernel attention, many other variants of efficient attention exist; the survey Tay, Dehghani, Bahri, et al. ('\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 98.3100 for spans (4, 22)\n",
      "Best matched text: '2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality.Linear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importancesampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformednumerator).Aside from kernel attention, many other variants of efficient attention exist; the survey Tay, Dehghani, Bahri, et al. (2022)offers an extensive categorization of many of these.B.5Long Context ModelsLong context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences.However, these are often from a computational standpoint and have not been extensively validated. These include:â€¢ Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 99.3200 for spans (17, 39)\n",
      "Best matched text: 'B.5Long Context ModelsLong context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences.However, these are often from a computational standpoint and have not been extensively validated. These include:â€¢ Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformerbackbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main resultis similar to our Induction Heads extrapolation experiment (Table 2).â€¢ LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length < 100ğ¾for actual tasks.â€¢ Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al.'\n",
      "Found 13 unique candidate spans across seeds.\n",
      "Best match score: 99.1000 for spans (1, 55)\n",
      "Best matched text: ' 2022) showedthat the LA denominator term can be unstable and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al.2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality.Linear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importancesampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformednumerator).Aside from kernel attention, many other variants of efficient attention exist; the survey Tay, Dehghani, Bahri, et al. (2022)offers an extensive categorization of many of these.B.5Long Context ModelsLong context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences.However, these are often from a computational standpoint and have not been extensively validated. These include:â€¢ Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformerbackbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main resultis similar to our Induction Heads extrapolation experiment (Table 2).â€¢ LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length < 100ğ¾for actual tasks.â€¢ Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to 1M context. How-ever, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if qualityimprovements at 1M context are due to context length or due to more data and computation.â€¢ Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer tomodel audio waveforms of length 220 = 1048576, although did not discuss performance tradeoffs when controlling forcomputation and model size.In contrast, we believe this work presents one of the first approaches to meaningfully demonstrate increasing performancewith longer context.'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 93.8800 for spans (57, 60)\n",
      "Best matched text: 'Mechanics of Selective SSMsProof of Theorem 1.'\n",
      "Found 23 unique candidate spans across seeds.\n",
      "Best match score: 76.5700 for spans (61, 154)\n",
      "Best matched text: ' Consider a selective SSM (Algorithm 2) with ğ‘= 1, ğ‘¨= âˆ’1, ğ‘©= 1,ğ‘ Î” = Linear(ğ‘¥),ğœÎ” = softplus. Thecorresponding continuous-time SSM (1) isâ„(ğ‘¡) = âˆ’â„(ğ‘¡) + ğ‘¥(ğ‘¡)which is also called a leaky integrator.The discretization step size isÎ”ğ‘¡= ğœÎ”(Parameter + ğ‘ Î”(ğ‘¥ğ‘¡))= softplus(Parameter + Linear(ğ‘¥ğ‘¡))= softplus(Linear(ğ‘¥ğ‘¡))where we observe that the parameter can be viewed as a learnable bias and folded into the linear projection.Now applying the zero-order hold (ZOH) discretization formulas:ğ‘¨ğ‘¡= exp(Î”ğ‘¨) =11 +'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 96.9700 for spans (0, 31)\n",
      "Best matched text: 'Thus the final discrete recurrence (2a) isğ‘”ğ‘¡= ğœ(Linear(ğ‘¥ğ‘¡))â„ğ‘¡= (1 âˆ’ğ‘”ğ‘¡)â„ğ‘¡âˆ’1 + ğ‘”ğ‘¡ğ‘¥ğ‘¡as desired.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.8000 for spans (34, 66)\n",
      "Best matched text: 'Hardware-aware Algorithm For Selective SSMsWithout input-dependent selectivity, SSMs can be efficiently implemented as a convolution (Dao, Fu, Saab, et al. 2023; Gu,Goel, and RÃ© 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity, SSMs are no-longerequivalent to convolution, but we leverage the parallel associative scan. While SSM scans are theoretically efficient(ğ‘‚(ğµğ¿ğ·ğ‘) FLOPs, scaling linear in ğ¿), training foundation models with selective SSMs requires them to be efficient onmodern hardware (GPUs) as well. We describe how we use kernel fusion and recomputation to make SSM scan fast andmemory-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5,showing that it is up to 7Ã— times faster than attention at sequence length 32K, and is as memory-efficient as the bestattention implementation (FlashAttention).Speed.On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by memory-bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.9300 for spans (67, 146)\n",
      "Best matched text: ' 2021; Williams, Waterman, and Patterson 2009). This the case with ourscan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to significant speedup compared toa standard implementation.The standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input ğ‘¨, ğ‘©of size (ğµ, ğ¿, ğ·, ğ‘) in GPUHBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel associative scan implementation towrite the scan output of size (ğµ, ğ¿, ğ·, ğ‘) to GPU HBM, then multiply that scan output with ğ‘ªto produce an output of size(ğµ, ğ¿, ğ·). However, this requires the number of memory reads/writes on the order of ğ‘‚(ğµğ¿ğ·ğ‘). We can instead fuse thediscretization step, the scan, and the multiplication with ğ‘ªinto one kernel:1. We read in ğ‘‚(ğµğ¿ğ·+ ğ·ğ‘) bytes of memory (Î”, ğ‘¨, ğ‘©, ğ‘ª) from slow HBM to fast SRAM.2. We discretize to produce ğ‘¨, ğ‘©of size (ğµ, ğ¿, ğ·, ğ‘) in SRAM.3. We perform a parallel associative scan, yielding intermediate states of size (ğµ, ğ¿, ğ·, ğ‘) in SRAM.4. We multiply and sum with ğ‘ª, producing outputs of size (ğµ, ğ¿, ğ·) and write it to HBM.This way, we reduce IOs by a factor of ğ‘‚(ğ‘) (the state dimension), which in practice speeds up the operation by 20-40'\n",
      "Found 9 unique candidate spans across seeds.\n",
      "Best match score: 98.9300 for spans (134, 200)\n",
      "Best matched text: '4. We multiply and sum with ğ‘ª, producing outputs of size (ğµ, ğ¿, ğ·) and write it to HBM.This way, we reduce IOs by a factor of ğ‘‚(ğ‘) (the state dimension), which in practice speeds up the operation by 20-40times (Section 4.5).For sequence length ğ¿too long where we cannot fit the sequence in SRAM (which is much smaller than HBM), we split thesequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate scan states, we cancontinue the scan with the next chunk.Memory.We describe how we use the classical technique of recomputation to reduce the total amount of memoryrequired to train selective SSM layers.From the way we fuse the forward pass, we do not save the intermediate states of size (ğµ, ğ¿, ğ·, ğ‘) to avoid memory blowup.However, these intermediate states are necessary for the backward pass to compute gradients. We instead recompute thoseintermediate states in the backward pass. Since the inputs Î”, ğ‘¨, ğ‘©, ğ‘ªand output gradient read from HBM to SRAM areof size ğ‘‚(ğµğ¿ğ‘+ ğ·ğ‘), and the input gradients are also of size ğ‘‚(ğµğ¿ğ‘+ ğ·ğ‘), recomputation avoids the cost of readingğ‘‚(ğµğ¿ğ‘ğ·) elements from HBM. This means that recomputation of the SSM states in the backward pass speeds up thecomputation compared to storing them and reading them from HBM.Beyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize thememory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output projection).In particular, we do not save intermediate activations that take a lot of memory but are fast to recompute (e.g. output ofactivation function or short convolution). As a result, the selective SSM layer has the same memory requirement as an28'\n",
      "Found 10 unique candidate spans across seeds.\n",
      "Best match score: 41.1100 for spans (0, 94)\n",
      "Best matched text: 'Table 11: (Induction heads.) Models are trained on sequence length 28 = 256, and tested on various sequence lengths of 26 = 64 up to220 = 1048576. âœ“denotes perfect generalization accuracy, while âœ—denotes out of memory.ModelParamsTest Accuracy (%) at Seqence Length26272829210211212213214215216217218219220MHA-Abs137Kâœ“99.6100.058.626.618.89.810.97.8âœ—âœ—âœ—âœ—âœ—âœ—MHA-RoPE137Kâœ“âœ“100.083.631.318.48.69.05.5âœ—âœ—âœ—âœ—âœ—âœ—MHA-xPos137Kâœ“âœ“100.099.667.625.4'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 99.0700 for spans (158, 162)\n",
      "Best matched text: 'optimized Transformer implementation with FlashAttention. In particular, each attention layer (FlashAttention) storesaround 12 bytes of activations per token, an each MLP layer stores around 20 bytes of activations per token, for a total of32 bytes ((assuming mixed-precision training in FP16 or BF16)). Each selective SSM stores around 16 bytes of activationsper token. Hence two layers of selective SSMs have around the same activation memory as an attention layer and an MLPlayer.'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 95.5600 for spans (164, 164)\n",
      "Best matched text: 'Experimental Details and Additional Results'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.1800 for spans (165, 175)\n",
      "Best matched text: 'E.1Synthetic TasksSelective Copying.Our setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including thewhite â€œnoiseâ€ token from Figure 2) and requiring models to memorize 16 â€œdataâ€ tokens. We use 2 layer models with a modeldimension of ğ·= 64.'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 99.1300 for spans (176, 221)\n",
      "Best matched text: 'Models are trained for 400K steps at a constant learning rate of 0.0001 with a batch size of 64.Induction Heads.Training consists of randomly generating data every step, with a batch size of 8. We choose anâ€œepochâ€ size of 8192 steps, and track the accuracy on fixed validation sets (also randomly generated) of each target sequencelength. For the MHA-Abs and Mamba models, results are reported after the 25th epoch (8192 Ã— 25 = 204800 steps). For theMHA-RoPE and MHA-xPos models, results are reported after the 50th epoch (8192 Ã— 50 = 409600 steps). For the LTI H3and Hyena models, results are reported after the 10th epoch (81920 steps) because they had converged by then and failedto improve further.We use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2ğ‘’âˆ’4 and 1ğ‘’âˆ’3, andthe better results are reported for each model (2ğ‘’âˆ’4 for all models except Mamba). The attention and Hyena models didnot learn at LR 1ğ‘’âˆ’3. H3 learned at both LRs, but interestingly generalized better to shorter sequences at the smaller LR of2ğ‘’âˆ’4. Mamba learned at both LRs, but extrapolated better at the larger LR of 1ğ‘’âˆ’3.E.2Language ModelingE.2.1Scaling Law DetailsScaling law experiments generally followed the GPT3 recipe. All models were trained on the Pile with the GPT2 tok-enizer.'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 95.2400 for spans (216, 217)\n",
      "Best matched text: 'E.2Language Modeling'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 95.5700 for spans (216, 230)\n",
      "Best matched text: 'E.2Language ModelingE.2.1Scaling Law DetailsScaling law experiments generally followed the GPT3 recipe. All models were trained on the Pile with the GPT2 tok-enizer.Model Sizes.Table 12 specifies the model sizes we use for scaling laws. This is taken directly from the GPT3 specifi-cations (Brown et al. 2020), with very minor modifications. First, we changed the batch size of the 1.3B model from 1Mtokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch size. Second, we changed thenumber of training steps and total tokens to roughly match Chinchilla scaling laws (Hoffmann et al.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.8500 for spans (227, 233)\n",
      "Best matched text: ' 2020), with very minor modifications. First, we changed the batch size of the 1.3B model from 1Mtokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch size. Second, we changed thenumber of training steps and total tokens to roughly match Chinchilla scaling laws (Hoffmann et al. 2022), which specifythat training tokens should increase proportionally to model size.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 96.7200 for spans (233, 236)\n",
      "Best matched text: 'that training tokens should increase proportionally to model size.Training Recipes.All models used the AdamW optimizer with29'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 84.6000 for spans (0, 66)\n",
      "Best matched text: 'Table 12: (Scaling Law Model Sizes.) Our model sizes and hyperparameters for scaling experiments. (Model dimension and number ofheads applies only to Transformer models.)Paramsn_layersd_modeln_heads / d_headTraining stepsLearning RateBatch SizeTokens125M1276812 / 6448006e-40.5M tokens2.5B350M24102416 / 64135003e-40.5M tokens7B760M24153616 / 96290002.5e-40.5M tokens15B1.3B24204832 / 64500002e-40.5M tokens26Bâ€¢ gradient clip value 1.0â€¢ weight decay 0.1â€¢ no dropoutâ€¢ linear learning rate warmup with cosine decayBy default, the peak learning rate is the GPT3 specification.We give several models an â€œimproved recipeâ€, inspired by changes adopted by popular large language models such asPaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al.'\n",
      "Found 14 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (65, 94)\n",
      "Best matched text: ' 2023) and LLaMa (Touvron et al. 2023). These include:â€¢ linear learning rate warmup with cosine decay to 1ğ‘’âˆ’5, with a peak value of 5Ã— the GPT3 valueâ€¢ no linear bias termsâ€¢ RMSNorm instead of LayerNormâ€¢ AdamW hyperparameter ğ›½= (.9, .95) (the GPT3 value) instead of the PyTorch default of ğ›½= (.9, .999)'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 98.0000 for spans (95, 96)\n",
      "Best matched text: 'Architecture and Training Details.Our models are:'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 99.5100 for spans (96, 139)\n",
      "Best matched text: 'Our models are:â€¢ Transformer: The standard Transformer based on GPT3 (Table 12).â€¢ Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al. 2021) andSwiGLU MLP (Shazeer 2020), and the improved training recipe above.â€¢ Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) withstandard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondinglyincreased by 1.5Ã— to preserve parameter count.â€¢ H3++: The H3 architecture with a few modifications, including (i) using the same â€œthinâ€ Hyena dimensions above (ii) theimproved training recipe above (iii) a linear attention head dimension of 8.â€¢ RWKV: The default RWKV model from B. Peng et al. (2023), including its modified MLP block. We also used as much ofits specified training recipe as possible, such as increasing the learning rates by 2Ã— or 3Ã— on certain parameters.â€¢ RetNet: The default RetNet model from Y. Sun et al. (2023). We also gave it the improved training recipe above.'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (139, 139)\n",
      "Best matched text: '). We also gave it the improved training recipe above.'\n",
      "Found 9 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (139, 142)\n",
      "Best matched text: '). We also gave it the improved training recipe above.â€¢ Mamba: The standard Mamba architecture, with the improved training recipe.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 82.4600 for spans (143, 160)\n",
      "Best matched text: 'E.2.2Additional Scaling Law AblationsWe perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws inFigure 4 (Left).Mamba Architecture: Interleaving Blocks.We test the effect of different architectural blocks combined with theMamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extraconv â†’SSM path added. This leads to two natural ablations:â€¢ What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also beinterpreted as taking Mamba and removing half of the SSMs.'\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 0 unique candidate spans across seeds.\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 99.2600 for spans (37, 57)\n",
      "Best matched text: 'Figure 9: (Scaling laws: extra ablations.) (Left) Instead of (Right) Instead ofâ€¢ What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as takinga Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks.Figure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neitherchange matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models exceptTransformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the factthat many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao,Fu, Saab, et al. 2023; Fathi et al.'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 99.2400 for spans (57, 73)\n",
      "Best matched text: '; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022).H3 Architecture: Training Recipes.Next we ablate differences between the Hyena and H3++ models, our weakestand strongest models outside of Transformer++ and Mamba, particularly to isolate the effect of training recipes.â€¢ Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).'\n",
      "Found 11 unique candidate spans across seeds.\n",
      "Best match score: 99.3400 for spans (66, 100)\n",
      "Best matched text: 'H3 Architecture: Training Recipes.Next we ablate differences between the Hyena and H3++ models, our weakestand strongest models outside of Transformer++ and Mamba, particularly to isolate the effect of training recipes.â€¢ Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).â€¢ Hyena+: The same architecture but with the improved training recipe described above.â€¢ H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel.â€¢ H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside the SSMrecurrence but does not increase parameters.Our general convention is that â€œModel+â€ represents the base model with the improved training recipe, and â€œModel++â€ alsoallows for architectural changes.Figure 9 (Right) shows thatâ€¢ A large improvement is achieved by the improved training recipe, which was used for many of the models in the mainFigure 4 (RetNet, H3++, Transformer++, Mamba).â€¢ The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with findings throughout this paper.â€¢ The head dimension expansion improves performance, consistent with one of our main themes that expanded state'\n",
      "Found 1 unique candidate spans across seeds.\n",
      "Best match score: 99.5300 for spans (98, 103)\n",
      "Best matched text: 'â€¢ The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with findings throughout this paper.â€¢ The head dimension expansion improves performance, consistent with one of our main themes that expanded statedimension improves performance for SSMs (Section 3).'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 99.0900 for spans (104, 119)\n",
      "Best matched text: 'E.2.3Downstream Evaluation DetailsThis pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens and with the GPT-NeoXtokenizer (Black et al. 2022) instead of GPT2 tokenizer. For the 1.3B model, we use a batch size of 1M tokens to be consistentwith the GPT3 specifications. We report the perplexity on the Pile validation set, and for this metric only compare tomodels trained on the same dataset and with the same tokenizer, in particular Pythia and RWKV.For downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021), as done by mostwork in this area. We evaluate on the following tasks/datasets that measure common sense reasoning:â€¢ LAMBADA (Paperno et al. 2016)â€¢ HellaSwag (Zellers et al.'\n",
      "Found 12 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (25, 109)\n",
      "Best matched text: '191020FLOPs (log scale)101Perplexity (log scale)Scaling Laws on The Pile (Sequence Length 2048)HyenaHyena+H3+H3++Figure 9: (Scaling laws: extra ablations.) (Left) Instead of (Right) Instead ofâ€¢ What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as takinga Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks.Figure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neitherchange matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models exceptTransformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the factthat many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao,Fu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022).H3 Architecture: Training Recipes.Next we ablate differences between the Hyena and H3++ models, our weakestand strongest models outside of Transformer++ and Mamba, particularly to isolate the effect of training recipes.â€¢ Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).â€¢ Hyena+: The same architecture but with the improved training recipe described above.â€¢ H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel.â€¢ H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside the SSMrecurrence but does not increase parameters.Our general convention is that â€œModel+â€ represents the base model with the improved training recipe, and â€œModel++â€ alsoallows for architectural changes.Figure 9 (Right) shows thatâ€¢ A large improvement is achieved by the improved training recipe, which was used for many of the models in the mainFigure 4 (RetNet, H3++, Transformer++, Mamba).â€¢ The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with findings throughout this paper.â€¢ The head dimension expansion improves performance, consistent with one of our main themes that expanded statedimension improves performance for SSMs (Section 3).E.2.3Downstream Evaluation DetailsThis pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens and with the GPT-NeoXtokenizer (Black et al. 2022) instead of GPT2 tokenizer. For the 1.3B model, we use a batch size of 1M tokens to be consistent'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (0, 3)\n",
      "Best matched text: 'â€¢ PIQA (Bisk et al. 2020)â€¢ ARC-challenge (P. Clark et al.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 99.3300 for spans (4, 11)\n",
      "Best matched text: ' 2018)â€¢ ARC-easy: an easy subset of ARC-challengeâ€¢ WinoGrande (Sakaguchi et al. 2021)We report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length forHellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these task).'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 93.7500 for spans (12, 13)\n",
      "Best matched text: 'E.3DNA Modeling'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 97.7700 for spans (14, 88)\n",
      "Best matched text: 'E.3.1Pretraining DetailsWe describe the dataset and training procedure of the HG38 pretraining task in more detail.The dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split contains atotal of ğ‘†= 34021 segments of length 217 = 131072 that cover the genome, for a total of approximately 4.5 billion tokens(DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended ifnecessary (e.g. to get longer segments).We deviate from HyenaDNA when the training sequence length is not 217. HyenaDNA always takes a fixed sub-segment(e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length each epoch is fixedto 34021 samples and doesnâ€™t necessarily go through the whole genome. On the other hand, we use the entire trainingdata:â€¢ When the context length ğ¿is less than (or equal to) 217, we divide up each segment into non-overlapping sub-segmentsof length ğ¿, so that there are ğ‘†Ã— 217ğ¿total samples and ğ‘†Ã— 217 â‰ˆ4.5ğµtokens per epoch.â€¢ When the context length ğ¿is greater than 217, we turn each segment into two samples, one that begins with the prescribedsegment and one that ends with the prescribed segment. Thus each epoch has 2ğ‘†items and 2ğ‘†ğ¿tokens per epoch. Forexample, at sequence length 218 = 262144 there are 4Ã— as many tokens as the default, and at sequence length 220 thereare 16Ã— as many tokens.Other training details generally follow the same protocol as our language modeling experiments (Appendix E.2). For'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.8600 for spans (51, 109)\n",
      "Best matched text: 'total samples and ğ‘†Ã— 217 â‰ˆ4.5ğµtokens per epoch.â€¢ When the context length ğ¿is greater than 217, we turn each segment into two samples, one that begins with the prescribedsegment and one that ends with the prescribed segment. Thus each epoch has 2ğ‘†items and 2ğ‘†ğ¿tokens per epoch. Forexample, at sequence length 218 = 262144 there are 4Ã— as many tokens as the default, and at sequence length 220 thereare 16Ã— as many tokens.Other training details generally follow the same protocol as our language modeling experiments (Appendix E.2). Forexample, we use the AdamW with (ğ›½1, ğ›½2) = (0.9, 0.95), no dropout, weight decay 0.1. We use a cosine learning ratescheduler with linear warmup for 10% of total steps.'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 96.9700 for spans (110, 111)\n",
      "Best matched text: 'E.3.2Scaling: Model Size Details'\n",
      "Found 8 unique candidate spans across seeds.\n",
      "Best match score: 98.7300 for spans (112, 117)\n",
      "Best matched text: 'Models.The models we consider are:â€¢ Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su et al.2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al.'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 99.7200 for spans (117, 127)\n",
      "Best matched text: '). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al. 2017).â€¢ HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer withthe MHA block replaced by an H3 block using a global convolution parameterized by an MLP.â€¢ Mamba: the standard Mamba architecture.'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (128, 128)\n",
      "Best matched text: 'Model Sizes.'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 83.4700 for spans (111, 158)\n",
      "Best matched text: 'Scaling: Model Size DetailsModels.The models we consider are:â€¢ Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su et al.2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al. 2017).â€¢ HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer withthe MHA block replaced by an H3 block using a global convolution parameterized by an MLP.â€¢ Mamba: the standard Mamba architecture.Model Sizes.We use the following model sizes.Blocks456781012Model Dimension6496128192256384512Params (Approx.)250K700K1.4M3.5M7.0M19.3M40.7MNote that the number of blocks for Mamba is doubled, because one Transformer â€œlayerâ€ includes both the MHA and MLPblocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).32'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 99.0200 for spans (0, 26)\n",
      "Best matched text: 'Training.For each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1ğ‘’âˆ’3, 2ğ‘’âˆ’3, 4ğ‘’âˆ’3, 8ğ‘’âˆ’3}. The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal Mamba learningrate was 8e-3; note that Mamba performed better than baselines with matched learning rates (2e-3), but was more stableand improved even more at higher learning rates. (Furthermore, as this LR is on the upper range of the sweep, it is possiblethat our results are still suboptimal.)'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 99.2900 for spans (27, 31)\n",
      "Best matched text: 'Note that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for simplicity. Theoptimal LR should go down for larger models, but we didnâ€™t find a noticeable effect at the small model sizes (at most a fewmillion parameters) we considered.'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 98.9000 for spans (32, 63)\n",
      "Best matched text: 'E.3.3Scaling: Context Length DetailsWe use a total batch size of 224 â‰ˆ16ğ‘€tokens per training step, for every sequence length (e.g. at length 220 there are16 segments per batch and at length 210 there are 16384 segments per batch). This is a large batch size relative to themodel size by usual LM standards, but note that a batch size of 223 is the minimum possible on a machine with 8 GPUs andsequence length of 220, and that HyenaDNA used much larger batches of 228.The learning rate used was 0.008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same learning rateof 0.002 from the previous section for HyenaDNA, but found that it was unstable at the longest context length.Sequence Length Warmup.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 99.1200 for spans (64, 97)\n",
      "Best matched text: 'Following (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW) duringpretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from 210 = 1024.(Note that because of how data is curated, at the longest sequence lengths more steps and tokens are spent proportionally.In particular, each stage up to length 217 processes the same number of tokens, but 4Ã— as many tokens are processed atlength 218, 8Ã— as many at length 219, and 16Ã— as many at length 220.)Unlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successivelyhalved as the sequence lengths are doubled in each stage.Remark E.1. We also note that the schedule was not tuned, and we never experimented with turning off sequence lengthwarmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at similarlengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.9300 for spans (98, 117)\n",
      "Best matched text: 'E.3.4Species (Great Apes) ClassificationModels are causal and therefore only the last element (across the sequence length) of the modelâ€™s output is used for theclassification head. Note that we control for the total number of elements in the loss function per gradient step. Thepretraining objective includes all positions across the sequence length, so that batch_size Ã— sequence_length is heldconstant; in other words, the batch size decreases as the sequence length increases. However, for a classification task, sinceonly the last position enters the loss, the batch size itself is held constant. Note that this also means that fine-tuning modelswith longer sequence lengths is more computationally expensive.Training consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which are allindependently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then uniformlypicking a contiguous segment of DNA.Following (Nguyen, Poli, et al.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 99.1000 for spans (118, 207)\n",
      "Best matched text: ' 2023), models with a maximum context length greater than 214 = 16384 use sequence lengthwarmup with 1 epoch at length 214 = 16384, 1 epoch at length 215 = 32768, 1 epoch at length 216 = 65536, and so on up tothe maximum sequence length. For example, the model with 220 = 1048576 context undergoes 6 epochs of sequence lengthwarmup before 4 more epochs at its maximum sequence length.The learning rate for all Hyena models is 4e âˆ’5, while the learning rate for all Mamba models is 1e âˆ’4. These were foundby performing learning rate sweeps for each model among {1ğ‘’âˆ’5, 2ğ‘’âˆ’5, 4ğ‘’âˆ’5, 1ğ‘’âˆ’4, 2ğ‘’âˆ’4} for the smaller sequencelengths (210, 212, 214, 216), and these values were consistently found to be the best for each model. An abridged learning ratesweep was done at length 218, which agreed with these values, and a single run at length 220 was performed (as describedabove, the computational cost of these experiments is proportional to the sequence length). The learning rate followeda cosine decay schedule with warmup with 5 epochs of linear warmup to the maximum learning rate, and 5 epochs ofcosine decay down to 1ğ‘’âˆ’6. The unusually long learning rate warmup schedule was chosen because the sequence length33'\n",
      "Found 7 unique candidate spans across seeds.\n",
      "Best match score: 99.4300 for spans (0, 11)\n",
      "Best matched text: 'Table 13: (Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 210 = 1024 up to 220 = 1048576 usingpretrained models of the same context length. Random guessing is 20%.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 90.1100 for spans (11, 26)\n",
      "Best matched text: 'pretrained models of the same context length. Random guessing is 20%.ModelParamsAccuracy (%) at Seqence Length210212214216218220'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 84.3800 for spans (27, 42)\n",
      "Best matched text: 'HyenaDNA1.4M28.0428.4341.1742.2231.1054.87Mamba1.4M31.4727.5027.6640.7242.4171.67'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 83.2400 for spans (43, 139)\n",
      "Best matched text: 'Mamba7M30.0029.0131.4843.7356.6081.31Table 14: YouTubeMix length scaling sequence lengths and batch sizes.Seqence lengthBatch sizeTokens / batch468 Ã— 2048 = 9584641958464234 Ã— 2048 = 4792322958464117 Ã— 2048 = 239616495846459 Ã— 2048 = 120832896665630 Ã— 2048 = 614401698304015 Ã— 2048 = 30720329830408 Ã— 2048 = 163846410485764 Ã— 2048 = 81921281048576warmup was also long (e.g. comprising 6 out of 10 epochs for the model with context length 220); we did not experimentwith this choice.Results for the Species classification task are in Table 13.E.4Audio DetailsE.4.1YouTubeMix Audio PretrainingModel.We use a model with 3 blocks per stage (3 Ã— 5 = 15 total Mamba blocks), pooling factor ğ‘= 16, and outerdimension ğ·= 64, for about 3.5M parameters.Dataset.The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of 256.The dataset consists of clips of up to 1 minute long, or length 960000, which is subsampled and divided into segments of anydesired sequence length. Since the architecture involves two stages of pooling by a factor of 16, and we want the resultingsequence length to be a a multiple of 8 for hardware efficiency, the longest possible sequence is 468'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 100.0000 for spans (115, 117)\n",
      "Best matched text: 'Results for the Species classification task are in Table 13.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 94.1200 for spans (120, 131)\n",
      "Best matched text: 'E.4.1YouTubeMix Audio PretrainingModel.We use a model with 3 blocks per stage (3 Ã— 5 = 15 total Mamba blocks), pooling factor ğ‘= 16, and outerdimension'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 97.4900 for spans (120, 194)\n",
      "Best matched text: 'E.4.1YouTubeMix Audio PretrainingModel.We use a model with 3 blocks per stage (3 Ã— 5 = 15 total Mamba blocks), pooling factor ğ‘= 16, and outerdimension ğ·= 64, for about 3.5M parameters.Dataset.The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of 256.The dataset consists of clips of up to 1 minute long, or length 960000, which is subsampled and divided into segments of anydesired sequence length. Since the architecture involves two stages of pooling by a factor of 16, and we want the resultingsequence length to be a a multiple of 8 for hardware efficiency, the longest possible sequence is 468 Ã— 2048 = 958464. Therest of our sequence lengths are defined by successively halving this and rounding up to the nearest multiple of 2048.Table 14 lists the specifications used in Figure 7. Beyond the varying batch sizes, the number of valid segments in thetraining set varied between different sequence lengths (e.g. the number of training steps per epoch was not constant fordifferent points in the graph), which may have contributed to kinks in the scaling curves.Training.Models were trained for 200ğ¾training steps with a maximum learning rate of 0.002, 20ğ¾(10%) warmup steps,and weight decay 0.1 (similar to our general pretraining recipe across domains).Additional Ablations: SSM Parameterizations.We investigate SSM parameterizations on long-form audio waveformpretraining in the setting of Figure 7. The setting is modified slightly to use larger models (8 layers and ğ·= 64 for 6Mparams, the SaShiMi default), shorter sequences (211 = 2048 to 218 = 262144 instead of 213 to 220), lower LR (0.001 from0.002), and shorter training cycles (100K instead of 200K steps).Figure 10 shows that the change from S4 â†’S6 (i.e. the selection mechanism) is not always beneficial. On long-form'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.9200 for spans (194, 196)\n",
      "Best matched text: 'S6 (i.e. the selection mechanism) is not always beneficial. On long-formaudio waveforms, it in fact significantly hampers performance, which may be intuitive from the point of view that audio34'\n",
      "Found 1 unique candidate spans across seeds.\n",
      "Best match score: 87.0000 for spans (37, 46)\n",
      "Best matched text: 'Audio Pretraining (YouTubeMix) Ablations.) As a uniformly-sampled â€œcontinuousâ€ signal modality, audio waveforms actu-ally benefit from LTI models which have matching inductive bias. (Left) Homogenous models (all blocks have the same parameterization)(Right) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as figure on left.is uniformly sampled and very smooth, and therefore benefits from continuous linear time-invariant (LTI) methods.After ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. To'\n",
      "Found 2 unique candidate spans across seeds.\n",
      "Best match score: 99.1300 for spans (44, 53)\n",
      "Best matched text: ') Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as figure on left.is uniformly sampled and very smooth, and therefore benefits from continuous linear time-invariant (LTI) methods.After ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. Todisambiguate, we call this Mamba-S4 as opposed the default Mamba architecture Mamba-S6.However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. Theperformance differences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio signal shouldbe LTI, but once they are â€œtokenizedâ€ and compressed by the outer layers, the inner layers no longer need to be LTI. In thissetting however, the real-valued SSM still underperforms the complex-valued one.'\n",
      "Found 5 unique candidate spans across seeds.\n",
      "Best match score: 99.1100 for spans (54, 102)\n",
      "Best matched text: 'E.4.2SC09 Speech GenerationAutoregressive training largely followed the autoregressive language modeling protocol, such asâ€¢ Weight decay 0.1â€¢ Learning rate warmup for 10% of total stepsâ€¢ AdamW optimizer with ğ›½= (0.9, 0.95)â€¢ Gradient clip value 0.1We used a learning rate of 0.002 and 200000 training steps at a batch size of 16.The large Mamba model in Table 4 has 15 layers per stage with an outer dimension of ğ·= 96 and pooling factor 4. Wenote that this dataset is small (training went through 100 epochs) and for this large model, there was significant overfittingof the BPB or NLL. However, automated metrics of generated samples continually improving throughout training.The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of D = 64 andpooling factor 4. The S4+MLP block has roughly 2ğ·2 + 4ğ·2 parameters (expansion factor 2 in the MLP). The Transformer'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 90.0000 for spans (103, 119)\n",
      "Best matched text: 'block has 4ğ·2 + 2ğ·2 parameters (expansion factor 1 in the MLP). The Mamba block has the usual â‰ˆ6ğ·2 parameters. Allmodels have roughly 6M total parameters.'\n",
      "Found 6 unique candidate spans across seeds.\n",
      "Best match score: 98.0300 for spans (121, 136)\n",
      "Best matched text: 'Efficiency BenchmarkScan Operation.We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), againstconvolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operationsoutside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing theQKV projections in attention.As a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing theparameters ğ‘¨, ğ‘©, ğ‘ªin HBM.'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 98.6400 for spans (137, 139)\n",
      "Best matched text: 'Our scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the largeparameters in HBM.35'\n",
      "Found 4 unique candidate spans across seeds.\n",
      "Best match score: 96.3100 for spans (0, 68)\n",
      "Best matched text: 'Table 15: (Memory benchmark.) Mambaâ€™s memory footprint is comparable to the most optimized Transformer. Results for 125Mmodels.Batch sizeTransformer (w/ FlashAttention-2)Mamba14.6GB4.8GB25.2GB5.8GB46.9GB7.3GB811.5GB12.3GB1620.7GB23.1GB3234.5GB38.2GBFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and thefilters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexityis ğ‘‚(ğ¿log(ğ¿)) for sequence length ğ¿.For attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2024)), with causalmask. Note that FlashAttention-2 with causal mask is about 1.7Ã— faster than without causal mask, since approximatelyonly half of the attention entries are computed.We use batch size of 1 and increase the sequence length from 29 = 512, 210 â‰ˆ1ğ¾, 211 â‰ˆ2ğ¾, up to 219 â‰ˆ500ğ¾(some of thebaselines run out of memory before reaching 500K). We use a model dimension of'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.8500 for spans (68, 81)\n",
      "Best matched text: 'baselines run out of memory before reaching 500K). We use a model dimension of ğ·= 1024 and state dimension ğ‘= 16.We measure with BF16 inputs, which is the data type most commonly used for large scale training.End-to-end Inference.We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba 6.9Bmodel, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard Transformerimplementation in the Huggingface transformers library.'\n",
      "Found 3 unique candidate spans across seeds.\n",
      "Best match score: 98.9100 for spans (82, 102)\n",
      "Best matched text: 'We set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s) asbatch size Ã— 128/time taken. We repeat the measurements 3 times and take the average. Measurements are done on anA100 80GB PCIe GPU.Memory Benchmark.The memory usage simply scales proportionally to the size of the activation tensors, as withmost deep sequence models. We report measurements of the training memory requirements of 125M models on 1 A10080GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-efficient Transformerimplementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2). Table 15 shows thatMambaâ€™s memory requirement is comparable to a similar-sized Transformer with an extremely optimized implementation,and we expect further improvement in Mambaâ€™s memory footprint in the future.36'\n",
      "Done. Saved bbox results â†’ bboxes_output.json\n"
     ]
    }
   ],
   "source": [
    "with open(JSON_INPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "    items = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "# --------------------------\n",
    "# Group by document_key extracted from doc_id\n",
    "# --------------------------\n",
    "\n",
    "pdf_groups = {}\n",
    "for item in items:\n",
    "    doc_id = item[\"doc_id\"]  # e.g. \"2312.00752v2-merged-document\"\n",
    "    page = item[\"page\"]  # page number\n",
    "    text = item[\"text\"]\n",
    "\n",
    "    # Extract document_key before \"-merged-document\"\n",
    "    if \"-merged-document\" in doc_id:\n",
    "        document_key = doc_id.split(\"-merged-document\")[0]\n",
    "    else:\n",
    "        document_key = doc_id\n",
    "\n",
    "    entry = {\n",
    "        \"document_key\": document_key,\n",
    "        \"page_number\": page,\n",
    "        \"text\": text,\n",
    "    }\n",
    "\n",
    "    pdf_groups.setdefault(document_key, []).append(entry)\n",
    "\n",
    "# --------------------------\n",
    "# Process each PDF\n",
    "# --------------------------\n",
    "\n",
    "for doc_key, entries in pdf_groups.items():\n",
    "\n",
    "    pdf_path = Path(PDF_FOLDER) / f\"{doc_key}.pdf\"\n",
    "\n",
    "    if not pdf_path.exists():\n",
    "        print(f\"[WARN] PDF missing â†’ {pdf_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing PDF: {pdf_path}\")\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    for ch in entries:\n",
    "        page_num = ch[\"page_number\"] - 1  # convert to zero-based index\n",
    "        if page_num < 0 or page_num >= len(doc):\n",
    "            print(f\"[WARN] Invalid page {page_num+1} in PDF {doc_key}\")\n",
    "            continue\n",
    "\n",
    "        page = doc[page_num]\n",
    "\n",
    "        # --------------------------\n",
    "        # NEW MATCHING FUNCTION\n",
    "        # --------------------------\n",
    "        bboxes = match_chunk_by_span(page, ch[\"text\"], threshold=0.78)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"document_key\": doc_key,\n",
    "                \"page_number\": ch[\"page_number\"],\n",
    "                \"text\": ch[\"text\"],\n",
    "                \"bboxes\": bboxes,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "# --------------------------\n",
    "# Save output\n",
    "# --------------------------\n",
    "\n",
    "with open(JSON_OUTPUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Done. Saved bbox results â†’ {JSON_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eacfb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open(\"2312.00752v2.pdf\")\n",
    "page = doc[0]  # Get the first page\n",
    "\n",
    "search_text = \"Abstract Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ€™ computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks ( Mamba ). Mamba enjoys fast inference (5 Ã— higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences.\"\n",
    "text_instances = page.search_for(search_text)\n",
    "\n",
    "for inst in text_instances:\n",
    "    # 'inst' is a Rect object representing the bounding box\n",
    "    print(f\"Bounding Box: {inst}\")\n",
    "    print(f\"Coordinates: x0={inst.x0}, y0={inst.y0}, x1={inst.x1}, y1={inst.y1}\")\n",
    "\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb23d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperreader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
