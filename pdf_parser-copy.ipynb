{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d85ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing PDF: 1810.04805v2.pdf\n",
      "Total pages: 16\n",
      "\n",
      "--- Processing Page 1/16 ---\n",
      "[Page 1] Found 0 images, saved 0 files\n",
      "[Page 1] Found 0 tables, saved 0 files\n",
      "  Text blocks: 9\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 2/16 ---\n",
      "[Page 2] Found 0 images, saved 0 files\n",
      "[Page 2] Found 0 tables, saved 0 files\n",
      "  Text blocks: 11\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 3/16 ---\n",
      "[Page 3] Found 14 images, saved 14 files\n",
      "[Page 3] Found 2 tables, saved 2 files\n",
      "  Text blocks: 38\n",
      "  Images: 14\n",
      "  Tables: 2\n",
      "\n",
      "--- Processing Page 4/16 ---\n",
      "[Page 4] Found 0 images, saved 0 files\n",
      "[Page 4] Found 0 tables, saved 0 files\n",
      "  Text blocks: 8\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 5/16 ---\n",
      "[Page 5] Found 1 images, saved 1 files\n",
      "[Page 5] Found 0 tables, saved 0 files\n",
      "  Text blocks: 15\n",
      "  Images: 1\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 6/16 ---\n",
      "[Page 6] Found 0 images, saved 0 files\n",
      "[Page 6] Found 0 tables, saved 0 files\n",
      "  Text blocks: 10\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 7/16 ---\n",
      "[Page 7] Found 0 images, saved 0 files\n",
      "[Page 7] Found 0 tables, saved 0 files\n",
      "  Text blocks: 24\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 8/16 ---\n",
      "[Page 8] Found 0 images, saved 0 files\n",
      "[Page 8] Found 0 tables, saved 0 files\n",
      "  Text blocks: 10\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 9/16 ---\n",
      "[Page 9] Found 0 images, saved 0 files\n",
      "[Page 9] Found 0 tables, saved 0 files\n",
      "  Text blocks: 17\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 10/16 ---\n",
      "[Page 10] Found 0 images, saved 0 files\n",
      "[Page 10] Found 0 tables, saved 0 files\n",
      "  Text blocks: 25\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 11/16 ---\n",
      "[Page 11] Found 0 images, saved 0 files\n",
      "[Page 11] Found 0 tables, saved 0 files\n",
      "  Text blocks: 25\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 12/16 ---\n",
      "[Page 12] Found 0 images, saved 0 files\n",
      "[Page 12] Found 0 tables, saved 0 files\n",
      "  Text blocks: 29\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 13/16 ---\n",
      "[Page 13] Found 1 images, saved 1 files\n",
      "[Page 13] Found 0 tables, saved 0 files\n",
      "  Text blocks: 42\n",
      "  Images: 1\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 14/16 ---\n",
      "[Page 14] Found 0 images, saved 0 files\n",
      "[Page 14] Found 0 tables, saved 0 files\n",
      "  Text blocks: 17\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 15/16 ---\n",
      "[Page 15] Found 22 images, saved 22 files\n",
      "[Page 15] Found 2 tables, saved 2 files\n",
      "  Text blocks: 64\n",
      "  Images: 22\n",
      "  Tables: 2\n",
      "\n",
      "--- Processing Page 16/16 ---\n",
      "[Page 16] Found 1 images, saved 1 files\n",
      "[Page 16] Found 1 tables, saved 1 files\n",
      "  Text blocks: 24\n",
      "  Images: 1\n",
      "  Tables: 1\n",
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Processing time: 8.89s\n",
      "Total text blocks: 368\n",
      "Total images: 39\n",
      "Total tables: 5\n",
      "\n",
      "Files saved:\n",
      "  - Complete analysis: output_parser_elements_analysis\\1810.04805v2-elements-analysis.json\n",
      "  - Text elements: output_parser_elements_analysis\\1810.04805v2-text-elements.json\n",
      "  - Image elements: output_parser_elements_analysis\\1810.04805v2-image-elements.json\n",
      "  - Table elements: output_parser_elements_analysis\\1810.04805v2-table-elements.json\n",
      "  - Images saved to: output_parser_elements_analysis\\images\n",
      "  - Tables saved to: output_parser_elements_analysis\\tables\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_log = logging.getLogger(__name__)\n",
    "\n",
    "INPUT_PDF_PATH = Path(\n",
    "    \"D:\\\\WorkSpace\\\\LOL-PaperReader\\\\backend\\\\src\\\\paperreader\\\\services\\\\parser\\\\1810.04805v2.pdf\"\n",
    ")\n",
    "OUTPUT_DIR = Path(\"output_parser_elements_analysis\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_text_elements(page, page_num):\n",
    "    \"\"\"Extract all text elements with their positions and formatting\"\"\"\n",
    "    text_elements = []\n",
    "\n",
    "    try:\n",
    "        layout = page.get_text(\"dict\", flags=fitz.TEXTFLAGS_DICT)\n",
    "\n",
    "        for block_num, block in enumerate(layout[\"blocks\"]):\n",
    "            if \"lines\" in block:  # Text block\n",
    "                block_bbox = block[\"bbox\"]\n",
    "                x0, y0, x1, y1 = block_bbox\n",
    "\n",
    "                block_text = []\n",
    "                font_info = []\n",
    "\n",
    "                for line_num, line in enumerate(block[\"lines\"]):\n",
    "                    line_text = \"\"\n",
    "                    line_fonts = []\n",
    "\n",
    "                    for span_num, span in enumerate(line[\"spans\"]):\n",
    "                        line_text += span[\"text\"]\n",
    "                        line_fonts.append(\n",
    "                            {\n",
    "                                \"font\": span.get(\"font\", \"\"),\n",
    "                                \"size\": span.get(\"size\", 0),\n",
    "                                \"flags\": span.get(\"flags\", 0),  # Bold, italic, etc.\n",
    "                                \"color\": span.get(\"color\", 0),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    if line_text.strip():\n",
    "                        block_text.append(line_text.strip())\n",
    "                        font_info.append(line_fonts)\n",
    "\n",
    "                if block_text:\n",
    "                    full_text = \" \".join(block_text)\n",
    "\n",
    "                    # Determine text type based on formatting\n",
    "                    text_type = \"paragraph\"\n",
    "                    is_bold = any(\n",
    "                        span.get(\"flags\", 0) & 2**4  # Bold flag\n",
    "                        for line in block[\"lines\"]\n",
    "                        for span in line[\"spans\"]\n",
    "                    )\n",
    "\n",
    "                    if is_bold and len(full_text) < 100:\n",
    "                        text_type = \"heading\"\n",
    "                    elif len(full_text) < 50:\n",
    "                        text_type = \"short_text\"\n",
    "\n",
    "                    text_element = {\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"block_id\": block_num,\n",
    "                        \"type\": \"text\",\n",
    "                        \"text_type\": text_type,\n",
    "                        \"content\": full_text,\n",
    "                        \"bbox\": [x0, y0, x1, y1],\n",
    "                        \"position\": {\n",
    "                            \"x\": x0,\n",
    "                            \"y\": y0,\n",
    "                            \"width\": x1 - x0,\n",
    "                            \"height\": y1 - y0,\n",
    "                        },\n",
    "                        \"font_info\": (\n",
    "                            font_info[0] if font_info else []\n",
    "                        ),  # Take first line's font info\n",
    "                        \"line_count\": len(block_text),\n",
    "                        \"char_count\": len(full_text),\n",
    "                        \"is_bold\": is_bold,\n",
    "                    }\n",
    "\n",
    "                    text_elements.append(text_element)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error extracting text from page {page_num + 1}: {e}\")\n",
    "\n",
    "    return text_elements\n",
    "\n",
    "\n",
    "def extract_image_elements(page, page_num, output_dir, pdf_stem):\n",
    "    \"\"\"Extract all image elements including embedded and vector figures\"\"\"\n",
    "    image_elements = []\n",
    "    images_saved = 0\n",
    "    global_image_id = 1  # Global counter for all images on this page\n",
    "\n",
    "    # --- Extract embedded images ---\n",
    "    try:\n",
    "        image_list = page.get_images()\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            try:\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                image_ext = base_image[\"ext\"]\n",
    "\n",
    "                # Save image\n",
    "                image_filename = f\"{pdf_stem}-p{page_num + 1}-img{global_image_id:03d}-embedded.{image_ext}\"\n",
    "                image_path = output_dir / image_filename\n",
    "                with open(image_path, \"wb\") as img_file:\n",
    "                    img_file.write(image_bytes)\n",
    "\n",
    "                # Get image info\n",
    "                img_bbox = img[1:5] if len(img) > 4 else [0, 0, 0, 0]\n",
    "                x0, y0, x1, y1 = img_bbox\n",
    "\n",
    "                image_element = {\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"global_id\": f\"p{page_num + 1}_img{global_image_id:03d}\",  # ✅ Unique global ID\n",
    "                    \"image_id\": f\"embedded_{img_index + 1}\",  # Original ID for reference\n",
    "                    \"type\": \"embedded_image\",\n",
    "                    \"filename\": image_filename,\n",
    "                    \"bbox\": img_bbox,\n",
    "                    \"position\": {\"x\": x0, \"y\": y0, \"width\": x1 - x0, \"height\": y1 - y0},\n",
    "                    \"xref\": xref,\n",
    "                    \"format\": image_ext,\n",
    "                    \"size_bytes\": len(image_bytes),\n",
    "                    \"order\": global_image_id,  # ✅ Order number\n",
    "                }\n",
    "\n",
    "                image_elements.append(image_element)\n",
    "                images_saved += 1\n",
    "                global_image_id += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Error extracting embedded image {img_index}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error in embedded image extraction for page {page_num + 1}: {e}\")\n",
    "\n",
    "    # --- Extract vector figures ---\n",
    "    try:\n",
    "        paths = page.get_drawings()\n",
    "\n",
    "        if paths:\n",
    "            # Group paths that are close together\n",
    "            path_groups = []\n",
    "            for path in paths:\n",
    "                path_rect = path.get(\"rect\", fitz.Rect(0, 0, 0, 0))\n",
    "                if path_rect.width > 0 and path_rect.height > 0:\n",
    "                    added_to_group = False\n",
    "                    for group in path_groups:\n",
    "                        group_rect = group[\"rect\"]\n",
    "                        expanded_rect = group_rect + (-30, -30, 30, 30)\n",
    "                        if path_rect.intersects(expanded_rect):\n",
    "                            group[\"rect\"] = group[\"rect\"] | path_rect\n",
    "                            group[\"paths\"].append(path)\n",
    "                            added_to_group = True\n",
    "                            break\n",
    "\n",
    "                    if not added_to_group:\n",
    "                        path_groups.append({\"rect\": path_rect, \"paths\": [path]})\n",
    "\n",
    "            for group_index, group in enumerate(path_groups):\n",
    "                rect = group[\"rect\"]\n",
    "                if (\n",
    "                    rect.width > 50\n",
    "                    and rect.height > 50\n",
    "                    and rect.width < page.rect.width * 0.95\n",
    "                    and rect.height < page.rect.height * 0.95\n",
    "                    and len(group[\"paths\"]) > 2\n",
    "                ):\n",
    "                    # Save vector figure\n",
    "                    clip_rect = rect + (-10, -10, 10, 10)\n",
    "                    clip_rect = clip_rect & page.rect\n",
    "\n",
    "                    if clip_rect.width > 30 and clip_rect.height > 30:\n",
    "                        # Use global ID in filename\n",
    "                        figure_filename = f\"{pdf_stem}-p{page_num + 1}-img{global_image_id:03d}-vector.png\"\n",
    "                        figure_path = output_dir / figure_filename\n",
    "                        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2), clip=clip_rect)\n",
    "                        pix.save(figure_path)\n",
    "\n",
    "                        image_element = {\n",
    "                            \"page\": page_num + 1,\n",
    "                            \"global_id\": f\"p{page_num + 1}_img{global_image_id:03d}\",  # ✅ Unique global ID\n",
    "                            \"image_id\": f\"vector_{group_index + 1}\",  # Original ID for reference\n",
    "                            \"type\": \"vector_figure\",\n",
    "                            \"filename\": figure_filename,\n",
    "                            \"bbox\": [rect.x0, rect.y0, rect.x1, rect.y1],\n",
    "                            \"position\": {\n",
    "                                \"x\": rect.x0,\n",
    "                                \"y\": rect.y0,\n",
    "                                \"width\": rect.width,\n",
    "                                \"height\": rect.height,\n",
    "                            },\n",
    "                            \"path_count\": len(group[\"paths\"]),\n",
    "                            \"complexity\": len(group[\"paths\"]),\n",
    "                            \"order\": global_image_id,  # ✅ Order number\n",
    "                        }\n",
    "\n",
    "                        image_elements.append(image_element)\n",
    "                        images_saved += 1\n",
    "                        global_image_id += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error in vector figure extraction for page {page_num + 1}: {e}\")\n",
    "\n",
    "    # --- Visual figure detection ---\n",
    "    try:\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(1.5, 1.5))\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        page_image = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "        img_array = np.array(page_image)\n",
    "        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "        edges = cv2.Canny(gray, 30, 100)\n",
    "\n",
    "        contours, _ = cv2.findContours(\n",
    "            edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        for contour_index, contour in enumerate(contours):\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            if (\n",
    "                w > 80\n",
    "                and h > 80\n",
    "                and w < page_image.width * 0.8\n",
    "                and h < page_image.height * 0.8\n",
    "                and 0.2 < w / h < 5.0\n",
    "            ):\n",
    "                roi = edges[y : y + h, x : x + w]\n",
    "                edge_density = np.sum(roi > 0) / (w * h)\n",
    "\n",
    "                if edge_density > 0.015:  # Has visual complexity\n",
    "                    # Convert back to page coordinates\n",
    "                    scale_factor = 1.5\n",
    "                    page_x = x / scale_factor\n",
    "                    page_y = y / scale_factor\n",
    "                    page_w = w / scale_factor\n",
    "                    page_h = h / scale_factor\n",
    "\n",
    "                    # Check for overlap with existing images\n",
    "                    new_rect = fitz.Rect(\n",
    "                        page_x, page_y, page_x + page_w, page_y + page_h\n",
    "                    )\n",
    "                    overlap = False\n",
    "\n",
    "                    for existing_img in image_elements:\n",
    "                        if existing_img[\"page\"] == page_num + 1:\n",
    "                            existing_bbox = existing_img[\"bbox\"]\n",
    "                            existing_rect = fitz.Rect(existing_bbox)\n",
    "                            if new_rect.intersects(existing_rect):\n",
    "                                overlap = True\n",
    "                                break\n",
    "\n",
    "                    if not overlap:\n",
    "                        # Save visual figure\n",
    "                        clip_rect = new_rect + (-5, -5, 5, 5)\n",
    "                        clip_rect = clip_rect & page.rect\n",
    "\n",
    "                        if clip_rect.width > 30 and clip_rect.height > 30:\n",
    "                            region_pix = page.get_pixmap(\n",
    "                                matrix=fitz.Matrix(2, 2), clip=clip_rect\n",
    "                            )\n",
    "                            # Use global ID in filename\n",
    "                            visual_filename = f\"{pdf_stem}-p{page_num + 1}-img{global_image_id:03d}-visual.png\"\n",
    "                            visual_path = output_dir / visual_filename\n",
    "                            region_pix.save(visual_path)\n",
    "\n",
    "                            image_element = {\n",
    "                                \"page\": page_num + 1,\n",
    "                                \"global_id\": f\"p{page_num + 1}_img{global_image_id:03d}\",  # ✅ Unique global ID\n",
    "                                \"image_id\": f\"visual_{contour_index + 1}\",  # Original ID for reference\n",
    "                                \"type\": \"visual_figure\",\n",
    "                                \"filename\": visual_filename,\n",
    "                                \"bbox\": [\n",
    "                                    new_rect.x0,\n",
    "                                    new_rect.y0,\n",
    "                                    new_rect.x1,\n",
    "                                    new_rect.y1,\n",
    "                                ],\n",
    "                                \"position\": {\n",
    "                                    \"x\": new_rect.x0,\n",
    "                                    \"y\": new_rect.y0,\n",
    "                                    \"width\": new_rect.width,\n",
    "                                    \"height\": new_rect.height,\n",
    "                                },\n",
    "                                \"edge_density\": edge_density,\n",
    "                                \"detection_method\": \"visual_analysis\",\n",
    "                                \"order\": global_image_id,  # ✅ Order number\n",
    "                            }\n",
    "\n",
    "                            image_elements.append(image_element)\n",
    "                            images_saved += 1\n",
    "                            global_image_id += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error in visual analysis for page {page_num + 1}: {e}\")\n",
    "\n",
    "    # Sort images by position for consistent ordering\n",
    "    image_elements.sort(key=lambda img: (img[\"position\"][\"y\"], img[\"position\"][\"x\"]))\n",
    "\n",
    "    print(\n",
    "        f\"[Page {page_num + 1}] Found {len(image_elements)} images, saved {images_saved} files\"\n",
    "    )\n",
    "    return image_elements\n",
    "\n",
    "\n",
    "def extract_table_elements(page, page_num, output_dir, pdf_stem):\n",
    "    \"\"\"Extract all table elements\"\"\"\n",
    "    table_elements = []\n",
    "    tables_saved = 0\n",
    "\n",
    "    try:\n",
    "        tables = page.find_tables()\n",
    "        for table_index, table in enumerate(tables):\n",
    "            try:\n",
    "                table_bbox = table.bbox\n",
    "                x0, y0, x1, y1 = table_bbox\n",
    "\n",
    "                df = table.to_pandas()\n",
    "\n",
    "                if not df.empty:\n",
    "                    # Save table files\n",
    "                    table_id = f\"table_{table_index + 1}\"\n",
    "                    csv_filename = f\"{pdf_stem}-p{page_num + 1}-{table_id}.csv\"\n",
    "                    html_filename = f\"{pdf_stem}-p{page_num + 1}-{table_id}.html\"\n",
    "\n",
    "                    csv_path = output_dir / csv_filename\n",
    "                    html_path = output_dir / html_filename\n",
    "\n",
    "                    df.to_csv(csv_path, index=False)\n",
    "                    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(df.to_html(index=False))\n",
    "\n",
    "                    # Get table content as text\n",
    "                    table_text = df.to_string(index=False)\n",
    "\n",
    "                    table_element = {\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"table_id\": table_id,\n",
    "                        \"type\": \"table\",\n",
    "                        \"bbox\": list(table_bbox),\n",
    "                        \"position\": {\n",
    "                            \"x\": x0,\n",
    "                            \"y\": y0,\n",
    "                            \"width\": x1 - x0,\n",
    "                            \"height\": y1 - y0,\n",
    "                        },\n",
    "                        \"dimensions\": {\"rows\": len(df), \"columns\": len(df.columns)},\n",
    "                        \"files\": {\"csv\": csv_filename, \"html\": html_filename},\n",
    "                        \"columns\": list(df.columns),\n",
    "                        \"content_preview\": (\n",
    "                            table_text[:500] + \"...\"\n",
    "                            if len(table_text) > 500\n",
    "                            else table_text\n",
    "                        ),\n",
    "                        \"cell_count\": len(df) * len(df.columns),\n",
    "                    }\n",
    "\n",
    "                    table_elements.append(table_element)\n",
    "                    tables_saved += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"[!] Error extracting table {table_index} from page {page_num + 1}: {e}\"\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error in table detection for page {page_num + 1}: {e}\")\n",
    "\n",
    "    print(\n",
    "        f\"[Page {page_num + 1}] Found {len(table_elements)} tables, saved {tables_saved} files\"\n",
    "    )\n",
    "    return table_elements\n",
    "\n",
    "\n",
    "def analyze_pdf_elements(pdf_path: Path, output_dir: Path):\n",
    "    \"\"\"Main function to extract and analyze all PDF elements\"\"\"\n",
    "\n",
    "    global doc  # Make doc global for image extraction\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pdf_stem = pdf_path.stem\n",
    "\n",
    "    # Create subdirectories\n",
    "    images_dir = output_dir / \"images\"\n",
    "    tables_dir = output_dir / \"tables\"\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Storage for all elements\n",
    "    all_text_elements = []\n",
    "    all_image_elements = []\n",
    "    all_table_elements = []\n",
    "\n",
    "    page_analysis = []\n",
    "\n",
    "    print(f\"Analyzing PDF: {pdf_path.name}\")\n",
    "    print(f\"Total pages: {len(doc)}\")\n",
    "\n",
    "    # Process each page\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        print(f\"\\n--- Processing Page {page_num + 1}/{len(doc)} ---\")\n",
    "\n",
    "        # Extract elements from this page\n",
    "        page_text_elements = extract_text_elements(page, page_num)\n",
    "        page_image_elements = extract_image_elements(\n",
    "            page, page_num, images_dir, pdf_stem\n",
    "        )\n",
    "        page_table_elements = extract_table_elements(\n",
    "            page, page_num, tables_dir, pdf_stem\n",
    "        )\n",
    "\n",
    "        # Add to global collections\n",
    "        all_text_elements.extend(page_text_elements)\n",
    "        all_image_elements.extend(page_image_elements)\n",
    "        all_table_elements.extend(page_table_elements)\n",
    "\n",
    "        # Page summary\n",
    "        page_summary = {\n",
    "            \"page\": page_num + 1,\n",
    "            \"text_blocks\": len(page_text_elements),\n",
    "            \"images\": len(page_image_elements),\n",
    "            \"tables\": len(page_table_elements),\n",
    "            \"total_elements\": len(page_text_elements)\n",
    "            + len(page_image_elements)\n",
    "            + len(page_table_elements),\n",
    "        }\n",
    "        page_analysis.append(page_summary)\n",
    "\n",
    "        print(f\"  Text blocks: {len(page_text_elements)}\")\n",
    "        print(f\"  Images: {len(page_image_elements)}\")\n",
    "        print(f\"  Tables: {len(page_table_elements)}\")\n",
    "\n",
    "    # Create comprehensive analysis\n",
    "    analysis_data = {\n",
    "        \"document_info\": {\n",
    "            \"filename\": pdf_path.name,\n",
    "            \"total_pages\": len(doc),\n",
    "            \"processing_time\": time.time() - start_time,\n",
    "            \"metadata\": dict(doc.metadata),\n",
    "        },\n",
    "        \"summary\": {\n",
    "            \"total_text_blocks\": len(all_text_elements),\n",
    "            \"total_images\": len(all_image_elements),\n",
    "            \"total_tables\": len(all_table_elements),\n",
    "            \"total_elements\": len(all_text_elements)\n",
    "            + len(all_image_elements)\n",
    "            + len(all_table_elements),\n",
    "        },\n",
    "        \"page_analysis\": page_analysis,\n",
    "        \"elements\": {\n",
    "            \"text_elements\": all_text_elements,\n",
    "            \"image_elements\": all_image_elements,\n",
    "            \"table_elements\": all_table_elements,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Save comprehensive analysis\n",
    "    analysis_path = output_dir / f\"{pdf_stem}-elements-analysis.json\"\n",
    "    with open(analysis_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(analysis_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Save individual element files for easier inspection\n",
    "    text_path = output_dir / f\"{pdf_stem}-text-elements.json\"\n",
    "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_text_elements, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    images_path = output_dir / f\"{pdf_stem}-image-elements.json\"\n",
    "    with open(images_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_image_elements, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    tables_path = output_dir / f\"{pdf_stem}-table-elements.json\"\n",
    "    with open(tables_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_table_elements, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"EXTRACTION COMPLETE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Processing time: {time.time() - start_time:.2f}s\")\n",
    "    print(f\"Total text blocks: {len(all_text_elements)}\")\n",
    "    print(f\"Total images: {len(all_image_elements)}\")\n",
    "    print(f\"Total tables: {len(all_table_elements)}\")\n",
    "    print(f\"\\nFiles saved:\")\n",
    "    print(f\"  - Complete analysis: {analysis_path}\")\n",
    "    print(f\"  - Text elements: {text_path}\")\n",
    "    print(f\"  - Image elements: {images_path}\")\n",
    "    print(f\"  - Table elements: {tables_path}\")\n",
    "    print(f\"  - Images saved to: {images_dir}\")\n",
    "    print(f\"  - Tables saved to: {tables_dir}\")\n",
    "\n",
    "\n",
    "# --- Run the analysis ---\n",
    "analyze_pdf_elements(INPUT_PDF_PATH, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c1ec96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import io\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def clean_text_blocks_in_figures_tables(\n",
    "    text_elements, image_elements, table_elements, overlap_threshold=0.8\n",
    "):\n",
    "    \"\"\"Clean text blocks that significantly overlap with figure or table regions\"\"\"\n",
    "\n",
    "    def get_overlap_ratio(text_bbox, other_bbox):\n",
    "        \"\"\"Calculate overlap ratio between two bounding boxes\"\"\"\n",
    "        text_rect = fitz.Rect(text_bbox)\n",
    "        other_rect = fitz.Rect(other_bbox)\n",
    "\n",
    "        if not text_rect.intersects(other_rect):\n",
    "            return 0.0\n",
    "\n",
    "        overlap_rect = text_rect & other_rect  # Intersection\n",
    "        overlap_area = overlap_rect.get_area()\n",
    "        text_area = text_rect.get_area()\n",
    "\n",
    "        if text_area == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return overlap_area / text_area\n",
    "\n",
    "    cleaned_text_elements = []\n",
    "    removed_count = 0\n",
    "\n",
    "    for text_element in text_elements:\n",
    "        text_bbox = text_element[\"bbox\"]\n",
    "        page_num = text_element[\"page\"]\n",
    "        should_keep = True\n",
    "\n",
    "        # Check overlap with images on the same page\n",
    "        for image_element in image_elements:\n",
    "            if image_element[\"page\"] == page_num:\n",
    "                image_bbox = image_element[\"bbox\"]\n",
    "                overlap_ratio = get_overlap_ratio(text_bbox, image_bbox)\n",
    "\n",
    "                if overlap_ratio > overlap_threshold:\n",
    "                    print(\n",
    "                        f\"[REMOVE] Text block {text_element['block_id']} on page {page_num}: {overlap_ratio:.2f} overlap with image {image_element['image_id']}\"\n",
    "                    )\n",
    "                    should_keep = False\n",
    "                    break\n",
    "\n",
    "        # Check overlap with tables on the same page (if text wasn't already removed)\n",
    "        if should_keep:\n",
    "            for table_element in table_elements:\n",
    "                if table_element[\"page\"] == page_num:\n",
    "                    table_bbox = table_element[\"bbox\"]\n",
    "                    overlap_ratio = get_overlap_ratio(text_bbox, table_bbox)\n",
    "\n",
    "                    if overlap_ratio > overlap_threshold:\n",
    "                        print(\n",
    "                            f\"[REMOVE] Text block {text_element['block_id']} on page {page_num}: {overlap_ratio:.2f} overlap with table {table_element['table_id']}\"\n",
    "                        )\n",
    "                        should_keep = False\n",
    "                        break\n",
    "\n",
    "        if should_keep:\n",
    "            cleaned_text_elements.append(text_element)\n",
    "        else:\n",
    "            removed_count += 1\n",
    "\n",
    "    print(\n",
    "        f\"[CLEAN TEXT] Removed {removed_count} text blocks that overlap with figures/tables\"\n",
    "    )\n",
    "    print(f\"[CLEAN TEXT] Kept {len(cleaned_text_elements)} clean text blocks\")\n",
    "\n",
    "    return cleaned_text_elements\n",
    "\n",
    "\n",
    "def clean_overlapping_images(image_elements, overlap_threshold=0.3):\n",
    "    \"\"\"Clean overlapping images, keeping the largest one\"\"\"\n",
    "\n",
    "    def get_overlap_ratio(bbox1, bbox2):\n",
    "        \"\"\"Calculate overlap ratio between two bounding boxes\"\"\"\n",
    "        rect1 = fitz.Rect(bbox1)\n",
    "        rect2 = fitz.Rect(bbox2)\n",
    "\n",
    "        if not rect1.intersects(rect2):\n",
    "            return 0.0\n",
    "\n",
    "        overlap_rect = rect1 & rect2\n",
    "        overlap_area = overlap_rect.get_area()\n",
    "\n",
    "        # Use the smaller area as denominator for overlap ratio\n",
    "        area1 = rect1.get_area()\n",
    "        area2 = rect2.get_area()\n",
    "        smaller_area = min(area1, area2)\n",
    "\n",
    "        if smaller_area == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return overlap_area / smaller_area\n",
    "\n",
    "    def get_image_area(img):\n",
    "        \"\"\"Calculate image area\"\"\"\n",
    "        bbox = img[\"bbox\"]\n",
    "        return (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "\n",
    "    # Group images by page\n",
    "    page_images = {}\n",
    "    for img in image_elements:\n",
    "        page_num = img[\"page\"]\n",
    "        if page_num not in page_images:\n",
    "            page_images[page_num] = []\n",
    "        page_images[page_num].append(img)\n",
    "\n",
    "    cleaned_images = []\n",
    "    removed_count = 0\n",
    "\n",
    "    for page_num, images in page_images.items():\n",
    "        # Sort images by area (largest first)\n",
    "        images_by_size = sorted(images, key=get_image_area, reverse=True)\n",
    "\n",
    "        kept_images = []\n",
    "\n",
    "        for img in images_by_size:\n",
    "            should_keep = True\n",
    "            img_bbox = img[\"bbox\"]\n",
    "\n",
    "            # Check against all already kept images on this page\n",
    "            for kept_img in kept_images:\n",
    "                kept_bbox = kept_img[\"bbox\"]\n",
    "                overlap_ratio = get_overlap_ratio(img_bbox, kept_bbox)\n",
    "\n",
    "                if overlap_ratio > overlap_threshold:\n",
    "                    kept_area = get_image_area(kept_img)\n",
    "                    current_area = get_image_area(img)\n",
    "                    print(\n",
    "                        f\"[REMOVE] Image {img['image_id']} (area: {current_area:.0f}) on page {page_num}: {overlap_ratio:.2f} overlap with larger image {kept_img['image_id']} (area: {kept_area:.0f})\"\n",
    "                    )\n",
    "                    should_keep = False\n",
    "                    break\n",
    "\n",
    "            if should_keep:\n",
    "                kept_images.append(img)\n",
    "            else:\n",
    "                removed_count += 1\n",
    "\n",
    "        cleaned_images.extend(kept_images)\n",
    "\n",
    "    print(f\"[CLEAN IMAGES] Removed {removed_count} overlapping images (kept largest)\")\n",
    "    print(f\"[CLEAN IMAGES] Kept {len(cleaned_images)} unique images\")\n",
    "\n",
    "    return cleaned_images\n",
    "\n",
    "\n",
    "def merge_elements_by_reading_order(text_elements, image_elements, table_elements):\n",
    "    \"\"\"Merge text, images, and tables in proper reading order considering column layout\"\"\"\n",
    "\n",
    "    # Group elements by page\n",
    "    page_elements = {}\n",
    "\n",
    "    # Initialize page structure\n",
    "    for text_elem in text_elements:\n",
    "        page_num = text_elem[\"page\"]\n",
    "        if page_num not in page_elements:\n",
    "            page_elements[page_num] = {\n",
    "                \"text\": [],\n",
    "                \"images\": [],\n",
    "                \"tables\": [],\n",
    "                \"merged_content\": [],\n",
    "            }\n",
    "        page_elements[page_num][\"text\"].append(text_elem)\n",
    "\n",
    "    # Add images and tables to their respective pages\n",
    "    for img_elem in image_elements:\n",
    "        page_num = img_elem[\"page\"]\n",
    "        if page_num in page_elements:\n",
    "            page_elements[page_num][\"images\"].append(img_elem)\n",
    "\n",
    "    for table_elem in table_elements:\n",
    "        page_num = table_elem[\"page\"]\n",
    "        if page_num in page_elements:\n",
    "            page_elements[page_num][\"tables\"].append(table_elem)\n",
    "\n",
    "    # Process each page\n",
    "    all_merged_content = []\n",
    "\n",
    "    for page_num in sorted(page_elements.keys()):\n",
    "        page_data = page_elements[page_num]\n",
    "\n",
    "        print(f\"\\n--- Processing Page {page_num} ---\")\n",
    "        print(f\"Text blocks: {len(page_data['text'])}\")\n",
    "        print(f\"Images: {len(page_data['images'])}\")\n",
    "        print(f\"Tables: {len(page_data['tables'])}\")\n",
    "\n",
    "        # Keep text blocks in original order (already in reading order from PyMuPDF)\n",
    "        text_blocks = page_data[\"text\"]\n",
    "\n",
    "        # Sort images and tables by Y position, then by X position\n",
    "        images = page_data[\"images\"]\n",
    "        tables = page_data[\"tables\"]\n",
    "\n",
    "        page_content = []\n",
    "        page_content.append(\n",
    "            {\n",
    "                \"type\": \"page_header\",\n",
    "                \"content\": f\"# Page {page_num}\\n\\n\",\n",
    "                \"page\": page_num,\n",
    "                \"position\": {\"x\": 0, \"y\": 0},\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Initialize pointers for images and tables\n",
    "        img_index = 0\n",
    "        table_index = 0\n",
    "\n",
    "        # Loop through text elements in their original order\n",
    "        for text_i, text_block in enumerate(text_blocks):\n",
    "            text_bbox = text_block[\"bbox\"]\n",
    "            current_page = text_block[\"page\"]\n",
    "            current_y = text_bbox[1]  # Y position\n",
    "            x_end = text_bbox[2]\n",
    "\n",
    "            # Check if we should insert images before this text block\n",
    "            while img_index < len(images):\n",
    "                img = images[img_index]\n",
    "                img_bbox = img[\"bbox\"]\n",
    "                img_page = img[\"page\"]\n",
    "                img_x = img_bbox[0]\n",
    "                img_y = img_bbox[1]\n",
    "\n",
    "                # Insert image if it's on same page and positioned before current text block\n",
    "                if img_page == current_page and img_y <= current_y and img_x < x_end:\n",
    "                    # Create image content\n",
    "                    if \"filename\" in img:\n",
    "                        content = f\"![{img['image_id']}]({img['filename']})\\n\\n\"\n",
    "                    else:\n",
    "                        content = f\"*[Image: {img['image_id']}]*\\n\\n\"\n",
    "\n",
    "                    page_content.append(\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"content\": content,\n",
    "                            \"image_type\": img[\"type\"],\n",
    "                            \"page\": page_num,\n",
    "                            \"position\": {\"x\": img_bbox[0], \"y\": img_y},\n",
    "                            \"image_id\": img[\"image_id\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    print(\n",
    "                        f\"[INSERT] Image {img['image_id']} before text block {text_block['block_id']}\"\n",
    "                    )\n",
    "                    img_index += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Check if we should insert tables before this text block\n",
    "            while table_index < len(tables):\n",
    "                table = tables[table_index]\n",
    "                table_bbox = table[\"bbox\"]\n",
    "                table_page = table[\"page\"]\n",
    "                table_x = table_bbox[0]\n",
    "                table_y = table_bbox[1]\n",
    "\n",
    "                # Insert table if it's on same page and positioned before current text block\n",
    "                if (\n",
    "                    table_page == current_page\n",
    "                    and table_y <= current_y\n",
    "                    and table_x < x_end\n",
    "                ):\n",
    "                    # Create table content\n",
    "                    content = f\"### {table['table_id'].replace('_', ' ').title()}\\n\\n\"\n",
    "                    if \"files\" in table:\n",
    "                        content += f\"[CSV]({table['files']['csv']}) | [HTML]({table['files']['html']})\\n\\n\"\n",
    "                    if \"content_preview\" in table:\n",
    "                        content += f\"```\\n{table['content_preview'][:300]}...\\n```\\n\\n\"\n",
    "\n",
    "                    page_content.append(\n",
    "                        {\n",
    "                            \"type\": \"table\",\n",
    "                            \"content\": content,\n",
    "                            \"page\": page_num,\n",
    "                            \"position\": {\"x\": table_bbox[0], \"y\": table_y},\n",
    "                            \"table_id\": table[\"table_id\"],\n",
    "                            \"dimensions\": table.get(\"dimensions\", {}),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    print(\n",
    "                        f\"[INSERT] Table {table['table_id']} before text block {text_block['block_id']}\"\n",
    "                    )\n",
    "                    table_index += 1\n",
    "                else:\n",
    "                    break\n",
    "            # Add current text block\n",
    "            text_content = text_block[\"content\"].strip()\n",
    "            if text_content:\n",
    "                # Determine if heading or paragraph\n",
    "                if text_block.get(\"text_type\") == \"heading\":\n",
    "                    final_text = f\"## {text_content}\\n\\n\"\n",
    "                else:\n",
    "                    final_text = f\"{text_content}\\n\\n\"\n",
    "\n",
    "                page_content.append(\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"content\": final_text,\n",
    "                        \"text_type\": text_block.get(\"text_type\", \"paragraph\"),\n",
    "                        \"page\": page_num,\n",
    "                        \"position\": {\"x\": text_bbox[0], \"y\": text_bbox[1]},\n",
    "                        \"block_id\": text_block.get(\"block_id\", 0),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Add any remaining images and tables at the end of the page\n",
    "        # while img_index < len(images):\n",
    "        #     img = images[img_index]\n",
    "        #     if img[\"page\"] == current_page:\n",
    "        #         if \"filename\" in img:\n",
    "        #             content = f\"![{img['image_id']}]({img['filename']})\\n\\n\"\n",
    "        #         else:\n",
    "        #             content = f\"*[Image: {img['image_id']}]*\\n\\n\"\n",
    "\n",
    "        #         page_content.append(\n",
    "        #             {\n",
    "        #                 \"type\": \"image\",\n",
    "        #                 \"content\": content,\n",
    "        #                 \"image_type\": img[\"type\"],\n",
    "        #                 \"page\": page_num,\n",
    "        #                 \"position\": {\"x\": img[\"bbox\"][0], \"y\": img[\"bbox\"][1]},\n",
    "        #                 \"image_id\": img[\"image_id\"],\n",
    "        #             }\n",
    "        #         )\n",
    "        #         print(f\"[INSERT] Remaining image {img['image_id']} at end of page\")\n",
    "        #     img_index += 1\n",
    "\n",
    "        # while table_index < len(tables):\n",
    "        #     table = tables[table_index]\n",
    "        #     if table[\"page\"] == current_page:\n",
    "        #         content = f\"### {table['table_id'].replace('_', ' ').title()}\\n\\n\"\n",
    "        #         if \"files\" in table:\n",
    "        #             content += f\"[CSV]({table['files']['csv']}) | [HTML]({table['files']['html']})\\n\\n\"\n",
    "\n",
    "        #         page_content.append(\n",
    "        #             {\n",
    "        #                 \"type\": \"table\",\n",
    "        #                 \"content\": content,\n",
    "        #                 \"page\": page_num,\n",
    "        #                 \"position\": {\"x\": table[\"bbox\"][0], \"y\": table[\"bbox\"][1]},\n",
    "        #                 \"table_id\": table[\"table_id\"],\n",
    "        #             }\n",
    "        #         )\n",
    "        #         print(f\"[INSERT] Remaining table {table['table_id']} at end of page\")\n",
    "        #     table_index += 1\n",
    "\n",
    "        page_elements[page_num][\"merged_content\"] = page_content\n",
    "        all_merged_content.extend(page_content)\n",
    "\n",
    "        print(\n",
    "            f\"[MERGE] Page {page_num}: {len(page_content)} total elements merged in reading order\"\n",
    "        )\n",
    "\n",
    "    return all_merged_content, page_elements\n",
    "\n",
    "\n",
    "# Example usage function that combines all three\n",
    "def process_pdf_elements(\n",
    "    text_elements, image_elements, table_elements, output_dir, pdf_stem\n",
    "):\n",
    "    \"\"\"Complete pipeline to clean and merge PDF elements\"\"\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CLEANING AND MERGING PDF ELEMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Clean text blocks that overlap with figures/tables\n",
    "    print(\"\\n1. Cleaning text blocks overlapping with figures/tables...\")\n",
    "    cleaned_text = clean_text_blocks_in_figures_tables(\n",
    "        text_elements, image_elements, table_elements\n",
    "    )\n",
    "\n",
    "    # Step 2: Clean overlapping images\n",
    "    print(\"\\n2. Cleaning overlapping images...\")\n",
    "    cleaned_images = clean_overlapping_images(image_elements)\n",
    "\n",
    "    # Step 3: Merge elements in reading order\n",
    "    print(\"\\n3. Merging elements in reading order...\")\n",
    "    merged_content, page_structure = merge_elements_by_reading_order(\n",
    "        cleaned_text, cleaned_images, table_elements\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    print(\"\\n4. Saving results...\")\n",
    "\n",
    "    # Save merged content as markdown\n",
    "    markdown_content = []\n",
    "    for item in merged_content:\n",
    "        markdown_content.append(item[\"content\"])\n",
    "\n",
    "    md_path = output_dir / f\"{pdf_stem}-merged-document.md\"\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\".join(markdown_content))\n",
    "\n",
    "    # Save detailed structure as JSON\n",
    "    structure_path = output_dir / f\"{pdf_stem}-merged-structure.json\"\n",
    "    with open(structure_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"merged_content\": merged_content,\n",
    "                \"page_structure\": page_structure,\n",
    "                \"statistics\": {\n",
    "                    \"total_elements\": len(merged_content),\n",
    "                    \"text_blocks_kept\": len(cleaned_text),\n",
    "                    \"text_blocks_removed\": len(text_elements) - len(cleaned_text),\n",
    "                    \"images_kept\": len(cleaned_images),\n",
    "                    \"images_removed\": len(image_elements) - len(cleaned_images),\n",
    "                    \"tables_kept\": len(table_elements),\n",
    "                    \"pages_processed\": len(page_structure),\n",
    "                },\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "\n",
    "    print(f\"[✓] Merged document saved: {md_path}\")\n",
    "    print(f\"[✓] Structure data saved: {structure_path}\")\n",
    "    print(f\"[✓] Processing complete!\")\n",
    "\n",
    "    return merged_content, page_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d46b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PDF PROCESSING PIPELINE: 2303.14334v2.pdf\n",
      "================================================================================\n",
      "\n",
      "🔍 STEP 1: EXTRACTING ELEMENTS FROM PDF\n",
      "--------------------------------------------------\n",
      "Analyzing PDF: 2303.14334v2.pdf\n",
      "Total pages: 11\n",
      "\n",
      "--- Processing Page 1/11 ---\n",
      "[Page 1] Found 0 images, saved 0 files\n",
      "[Page 1] Found 0 tables, saved 0 files\n",
      "  Text blocks: 22\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 2/11 ---\n",
      "[Page 2] Found 2 images, saved 2 files\n",
      "[Page 2] Found 0 tables, saved 0 files\n",
      "  Text blocks: 8\n",
      "  Images: 2\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 3/11 ---\n",
      "[Page 3] Found 2 images, saved 2 files\n",
      "[Page 3] Found 0 tables, saved 0 files\n",
      "  Text blocks: 10\n",
      "  Images: 2\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 4/11 ---\n",
      "[Page 4] Found 4 images, saved 4 files\n",
      "[Page 4] Found 0 tables, saved 0 files\n",
      "  Text blocks: 11\n",
      "  Images: 4\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 5/11 ---\n",
      "[Page 5] Found 4 images, saved 4 files\n",
      "[Page 5] Found 0 tables, saved 0 files\n",
      "  Text blocks: 11\n",
      "  Images: 4\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 6/11 ---\n",
      "[Page 6] Found 2 images, saved 2 files\n",
      "[Page 6] Found 0 tables, saved 0 files\n",
      "  Text blocks: 9\n",
      "  Images: 2\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 7/11 ---\n",
      "[Page 7] Found 0 images, saved 0 files\n",
      "[Page 7] Found 0 tables, saved 0 files\n",
      "  Text blocks: 11\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 8/11 ---\n",
      "[Page 8] Found 0 images, saved 0 files\n",
      "[Page 8] Found 0 tables, saved 0 files\n",
      "  Text blocks: 8\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 9/11 ---\n",
      "[Page 9] Found 0 images, saved 0 files\n",
      "[Page 9] Found 0 tables, saved 0 files\n",
      "  Text blocks: 6\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 10/11 ---\n",
      "[Page 10] Found 0 images, saved 0 files\n",
      "[Page 10] Found 0 tables, saved 0 files\n",
      "  Text blocks: 3\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "--- Processing Page 11/11 ---\n",
      "[Page 11] Found 0 images, saved 0 files\n",
      "[Page 11] Found 0 tables, saved 0 files\n",
      "  Text blocks: 3\n",
      "  Images: 0\n",
      "  Tables: 0\n",
      "\n",
      "==================================================\n",
      "EXTRACTION COMPLETE\n",
      "==================================================\n",
      "Processing time: 14.79s\n",
      "Total text blocks: 102\n",
      "Total images: 14\n",
      "Total tables: 0\n",
      "\n",
      "Files saved:\n",
      "  - Complete analysis: output_merged_pipeline\\2303.14334v2\\2303.14334v2-elements-analysis.json\n",
      "  - Text elements: output_merged_pipeline\\2303.14334v2\\2303.14334v2-text-elements.json\n",
      "  - Image elements: output_merged_pipeline\\2303.14334v2\\2303.14334v2-image-elements.json\n",
      "  - Table elements: output_merged_pipeline\\2303.14334v2\\2303.14334v2-table-elements.json\n",
      "  - Images saved to: output_merged_pipeline\\2303.14334v2\\images\n",
      "  - Tables saved to: output_merged_pipeline\\2303.14334v2\\tables\n",
      "[✓] Extracted 102 text blocks\n",
      "[✓] Extracted 14 images\n",
      "[✓] Extracted 0 tables\n",
      "\n",
      "🧹 STEP 2: CLEANING AND MERGING ELEMENTS\n",
      "--------------------------------------------------\n",
      "============================================================\n",
      "CLEANING AND MERGING PDF ELEMENTS\n",
      "============================================================\n",
      "\n",
      "1. Cleaning text blocks overlapping with figures/tables...\n",
      "[CLEAN TEXT] Removed 0 text blocks that overlap with figures/tables\n",
      "[CLEAN TEXT] Kept 102 clean text blocks\n",
      "\n",
      "2. Cleaning overlapping images...\n",
      "[CLEAN IMAGES] Removed 0 overlapping images (kept largest)\n",
      "[CLEAN IMAGES] Kept 14 unique images\n",
      "\n",
      "3. Merging elements in reading order...\n",
      "\n",
      "--- Processing Page 1 ---\n",
      "Text blocks: 22\n",
      "Images: 0\n",
      "Tables: 0\n",
      "[MERGE] Page 1: 23 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 2 ---\n",
      "Text blocks: 8\n",
      "Images: 2\n",
      "Tables: 0\n",
      "[INSERT] Image visual_3490 before text block 2\n",
      "[MERGE] Page 2: 10 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 3 ---\n",
      "Text blocks: 10\n",
      "Images: 2\n",
      "Tables: 0\n",
      "[INSERT] Image visual_5231 before text block 2\n",
      "[MERGE] Page 3: 12 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 4 ---\n",
      "Text blocks: 11\n",
      "Images: 4\n",
      "Tables: 0\n",
      "[INSERT] Image visual_4296 before text block 2\n",
      "[INSERT] Image visual_4295 before text block 9\n",
      "[MERGE] Page 4: 14 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 5 ---\n",
      "Text blocks: 11\n",
      "Images: 4\n",
      "Tables: 0\n",
      "[INSERT] Image visual_4080 before text block 9\n",
      "[INSERT] Image visual_988 before text block 11\n",
      "[MERGE] Page 5: 14 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 6 ---\n",
      "Text blocks: 9\n",
      "Images: 2\n",
      "Tables: 0\n",
      "[INSERT] Image visual_4860 before text block 6\n",
      "[MERGE] Page 6: 11 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 7 ---\n",
      "Text blocks: 11\n",
      "Images: 0\n",
      "Tables: 0\n",
      "[MERGE] Page 7: 12 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 8 ---\n",
      "Text blocks: 8\n",
      "Images: 0\n",
      "Tables: 0\n",
      "[MERGE] Page 8: 9 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 9 ---\n",
      "Text blocks: 6\n",
      "Images: 0\n",
      "Tables: 0\n",
      "[MERGE] Page 9: 7 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 10 ---\n",
      "Text blocks: 3\n",
      "Images: 0\n",
      "Tables: 0\n",
      "[MERGE] Page 10: 4 total elements merged in reading order\n",
      "\n",
      "--- Processing Page 11 ---\n",
      "Text blocks: 3\n",
      "Images: 0\n",
      "Tables: 0\n",
      "[MERGE] Page 11: 4 total elements merged in reading order\n",
      "\n",
      "4. Saving results...\n",
      "[✓] Merged document saved: output_merged_pipeline\\2303.14334v2\\2303.14334v2-merged-document.md\n",
      "[✓] Structure data saved: output_merged_pipeline\\2303.14334v2\\2303.14334v2-merged-structure.json\n",
      "[✓] Processing complete!\n",
      "\n",
      "📊 STEP 3: GENERATING SUMMARY REPORT\n",
      "--------------------------------------------------\n",
      "[✓] Pipeline completed in 14.98 seconds\n",
      "[✓] Processed 11 pages\n",
      "[✓] Generated 120 merged elements\n",
      "[✓] Summary saved: output_merged_pipeline\\2303.14334v2\\2303.14334v2-pipeline-summary.json\n",
      "[✓] Main document: output_merged_pipeline\\2303.14334v2\\2303.14334v2-merged-document.md\n",
      "\n",
      "📁 All files saved to: output_merged_pipeline\\2303.14334v2\n",
      "\n",
      "✅ SUCCESS: PDF processed successfully!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def run_pdf_processing_pipeline(pdf_path_str: str, output_dir_str: str = None):\n",
    "    \"\"\"\n",
    "    Complete PDF processing pipeline that extracts, cleans, and merges elements\n",
    "\n",
    "    Args:\n",
    "        pdf_path_str: Path to PDF file as string\n",
    "        output_dir_str: Output directory path as string (optional)\n",
    "\n",
    "    Returns:\n",
    "        dict: Processing results with merged content and statistics\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert string paths to Path objects\n",
    "    pdf_path = Path(pdf_path_str)\n",
    "\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "    # Set output directory\n",
    "    if output_dir_str is None:\n",
    "        output_dir = Path(\"output_merged_pipeline\") / pdf_path.stem\n",
    "    else:\n",
    "        output_dir = Path(output_dir_str)\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pdf_stem = pdf_path.stem\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"PDF PROCESSING PIPELINE: {pdf_path.name}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Step 1: Extract all elements from PDF\n",
    "    print(\"\\n🔍 STEP 1: EXTRACTING ELEMENTS FROM PDF\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Run the element extraction (using your existing function)\n",
    "        analyze_pdf_elements(pdf_path, output_dir)\n",
    "\n",
    "        # Load the extracted elements\n",
    "        text_elements_path = output_dir / f\"{pdf_stem}-text-elements.json\"\n",
    "        image_elements_path = output_dir / f\"{pdf_stem}-image-elements.json\"\n",
    "        table_elements_path = output_dir / f\"{pdf_stem}-table-elements.json\"\n",
    "\n",
    "        # Load elements from JSON files\n",
    "        with open(text_elements_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_elements = json.load(f)\n",
    "\n",
    "        with open(image_elements_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            image_elements = json.load(f)\n",
    "\n",
    "        with open(table_elements_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            table_elements = json.load(f)\n",
    "\n",
    "        print(f\"[✓] Extracted {len(text_elements)} text blocks\")\n",
    "        print(f\"[✓] Extracted {len(image_elements)} images\")\n",
    "        print(f\"[✓] Extracted {len(table_elements)} tables\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[✗] Error in extraction: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Step 2: Run the cleaning and merging pipeline\n",
    "    print(\"\\n🧹 STEP 2: CLEANING AND MERGING ELEMENTS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        merged_content, page_structure = process_pdf_elements(\n",
    "            text_elements, image_elements, table_elements, output_dir, pdf_stem\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[✗] Error in processing: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Step 3: Generate final summary report\n",
    "    print(\"\\n📊 STEP 3: GENERATING SUMMARY REPORT\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    total_time = time.time() - total_start_time\n",
    "\n",
    "    # Create comprehensive summary\n",
    "    summary_report = {\n",
    "        \"pipeline_info\": {\n",
    "            \"pdf_file\": pdf_path.name,\n",
    "            \"pdf_path\": str(pdf_path),\n",
    "            \"output_directory\": str(output_dir),\n",
    "            \"processing_time_seconds\": round(total_time, 2),\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        },\n",
    "        \"extraction_results\": {\n",
    "            \"original_text_blocks\": len(text_elements),\n",
    "            \"original_images\": len(image_elements),\n",
    "            \"original_tables\": len(table_elements),\n",
    "        },\n",
    "        \"cleaning_results\": {\n",
    "            \"cleaned_text_blocks\": sum(\n",
    "                1 for item in merged_content if item[\"type\"] == \"text\"\n",
    "            ),\n",
    "            \"cleaned_images\": sum(\n",
    "                1 for item in merged_content if item[\"type\"] == \"image\"\n",
    "            ),\n",
    "            \"tables_processed\": sum(\n",
    "                1 for item in merged_content if item[\"type\"] == \"table\"\n",
    "            ),\n",
    "            \"text_blocks_removed\": None,  # Will be calculated\n",
    "            \"images_removed\": None,  # Will be calculated\n",
    "        },\n",
    "        \"output_files\": {\n",
    "            \"merged_markdown\": f\"{pdf_stem}-merged-document.md\",\n",
    "            \"structure_json\": f\"{pdf_stem}-merged-structure.json\",\n",
    "            \"text_elements\": f\"{pdf_stem}-text-elements.json\",\n",
    "            \"image_elements\": f\"{pdf_stem}-image-elements.json\",\n",
    "            \"table_elements\": f\"{pdf_stem}-table-elements.json\",\n",
    "            \"images_folder\": \"images/\",\n",
    "            \"tables_folder\": \"tables/\",\n",
    "        },\n",
    "        \"pages_processed\": len(page_structure),\n",
    "        \"total_merged_elements\": len(merged_content),\n",
    "    }\n",
    "\n",
    "    # Calculate removal statistics from the structure file if it exists\n",
    "    structure_path = output_dir / f\"{pdf_stem}-merged-structure.json\"\n",
    "    if structure_path.exists():\n",
    "        with open(structure_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            structure_data = json.load(f)\n",
    "            stats = structure_data.get(\"statistics\", {})\n",
    "            summary_report[\"cleaning_results\"][\"text_blocks_removed\"] = stats.get(\n",
    "                \"text_blocks_removed\", 0\n",
    "            )\n",
    "            summary_report[\"cleaning_results\"][\"images_removed\"] = stats.get(\n",
    "                \"images_removed\", 0\n",
    "            )\n",
    "\n",
    "    # Save summary report\n",
    "    summary_path = output_dir / f\"{pdf_stem}-pipeline-summary.json\"\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Save human-readable summary\n",
    "    summary_text_path = output_dir / f\"{pdf_stem}-pipeline-summary.txt\"\n",
    "    with open(summary_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"PDF PROCESSING PIPELINE SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"📄 File: {pdf_path.name}\\n\")\n",
    "        f.write(f\"⏱️  Processing Time: {total_time:.2f} seconds\\n\")\n",
    "        f.write(f\"📅 Processed: {summary_report['pipeline_info']['timestamp']}\\n\\n\")\n",
    "\n",
    "        f.write(\"📊 EXTRACTION RESULTS:\\n\")\n",
    "        f.write(f\"   • Text blocks: {len(text_elements)}\\n\")\n",
    "        f.write(f\"   • Images: {len(image_elements)}\\n\")\n",
    "        f.write(f\"   • Tables: {len(table_elements)}\\n\\n\")\n",
    "\n",
    "        f.write(\"🧹 CLEANING RESULTS:\\n\")\n",
    "        f.write(\n",
    "            f\"   • Text blocks kept: {summary_report['cleaning_results']['cleaned_text_blocks']}\\n\"\n",
    "        )\n",
    "        f.write(\n",
    "            f\"   • Images kept: {summary_report['cleaning_results']['cleaned_images']}\\n\"\n",
    "        )\n",
    "        f.write(\n",
    "            f\"   • Tables kept: {summary_report['cleaning_results']['tables_processed']}\\n\"\n",
    "        )\n",
    "        if summary_report[\"cleaning_results\"][\"text_blocks_removed\"] is not None:\n",
    "            f.write(\n",
    "                f\"   • Text blocks removed: {summary_report['cleaning_results']['text_blocks_removed']}\\n\"\n",
    "            )\n",
    "            f.write(\n",
    "                f\"   • Images removed: {summary_report['cleaning_results']['images_removed']}\\n\"\n",
    "            )\n",
    "        f.write(f\"\\n\")\n",
    "\n",
    "        f.write(\"📁 OUTPUT FILES:\\n\")\n",
    "        f.write(\n",
    "            f\"   • Merged document: {summary_report['output_files']['merged_markdown']}\\n\"\n",
    "        )\n",
    "        f.write(\n",
    "            f\"   • Structure data: {summary_report['output_files']['structure_json']}\\n\"\n",
    "        )\n",
    "        f.write(\n",
    "            f\"   • Images folder: {summary_report['output_files']['images_folder']}\\n\"\n",
    "        )\n",
    "        f.write(\n",
    "            f\"   • Tables folder: {summary_report['output_files']['tables_folder']}\\n\"\n",
    "        )\n",
    "        f.write(f\"   • Pipeline summary: {pdf_stem}-pipeline-summary.json\\n\\n\")\n",
    "\n",
    "        f.write(f\"📖 FINAL DOCUMENT:\\n\")\n",
    "        f.write(f\"   • Total pages: {summary_report['pages_processed']}\\n\")\n",
    "        f.write(f\"   • Total elements: {summary_report['total_merged_elements']}\\n\")\n",
    "        f.write(\n",
    "            f\"   • Main output: {output_dir / summary_report['output_files']['merged_markdown']}\\n\"\n",
    "        )\n",
    "\n",
    "    # Print final summary\n",
    "    print(f\"[✓] Pipeline completed in {total_time:.2f} seconds\")\n",
    "    print(f\"[✓] Processed {summary_report['pages_processed']} pages\")\n",
    "    print(f\"[✓] Generated {summary_report['total_merged_elements']} merged elements\")\n",
    "    print(f\"[✓] Summary saved: {summary_path}\")\n",
    "    print(\n",
    "        f\"[✓] Main document: {output_dir / summary_report['output_files']['merged_markdown']}\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n📁 All files saved to: {output_dir}\")\n",
    "\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"summary\": summary_report,\n",
    "        \"merged_content\": merged_content,\n",
    "        \"page_structure\": page_structure,\n",
    "        \"output_directory\": str(output_dir),\n",
    "        \"main_document_path\": str(\n",
    "            output_dir / summary_report[\"output_files\"][\"merged_markdown\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage functions for different scenarios\n",
    "def process_single_pdf(pdf_path: str, output_dir: str = None):\n",
    "    \"\"\"Process a single PDF file\"\"\"\n",
    "    try:\n",
    "        result = run_pdf_processing_pipeline(pdf_path, output_dir)\n",
    "        print(\"\\n✅ SUCCESS: PDF processed successfully!\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR: {e}\")\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "def process_multiple_pdfs(pdf_directory: str, output_base_dir: str = None):\n",
    "    \"\"\"Process multiple PDF files in a directory\"\"\"\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    if not pdf_dir.exists():\n",
    "        raise ValueError(f\"Directory not found: {pdf_dir}\")\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {pdf_dir}\")\n",
    "        return {\"success\": False, \"error\": \"No PDF files found\"}\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    results = {}\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\n{'='*20} Processing {pdf_file.name} {'='*20}\")\n",
    "\n",
    "        try:\n",
    "            # Set individual output directory for each PDF\n",
    "            if output_base_dir:\n",
    "                individual_output = Path(output_base_dir) / pdf_file.stem\n",
    "            else:\n",
    "                individual_output = Path(\"output_batch_processing\") / pdf_file.stem\n",
    "\n",
    "            result = run_pdf_processing_pipeline(str(pdf_file), str(individual_output))\n",
    "            results[pdf_file.name] = result\n",
    "            successful += 1\n",
    "            print(f\"✅ {pdf_file.name} processed successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            results[pdf_file.name] = {\"success\": False, \"error\": str(e)}\n",
    "            failed += 1\n",
    "            print(f\"❌ {pdf_file.name} failed: {e}\")\n",
    "\n",
    "    print(f\"\\n📊 BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"✅ Successful: {successful}/{len(pdf_files)}\")\n",
    "    print(f\"❌ Failed: {failed}/{len(pdf_files)}\")\n",
    "\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"total_files\": len(pdf_files),\n",
    "        \"successful\": successful,\n",
    "        \"failed\": failed,\n",
    "        \"results\": results,\n",
    "    }\n",
    "\n",
    "\n",
    "# Quick test function\n",
    "# def quick_test():\n",
    "#     \"\"\"Quick test with your current PDF\"\"\"\n",
    "#     pdf_path = \"D:\\\\WorkSpace\\\\LOL-PaperReader\\\\backend\\\\src\\\\paperreader\\\\services\\\\parser\\\\1810.04805v2.pdf\"\n",
    "#     return process_single_pdf(pdf_path)\n",
    "\n",
    "\n",
    "# Run the pipeline (uncomment one of these)\n",
    "\n",
    "# Option 1: Process single PDF with default settings\n",
    "result = process_single_pdf(\n",
    "    \"D:\\\\WorkSpace\\\\LOL-PaperReader\\\\backend\\\\src\\\\paperreader\\\\services\\\\parser\\\\2303.14334v2.pdf\"\n",
    ")\n",
    "\n",
    "# Option 2: Process single PDF with custom output directory\n",
    "# result = process_single_pdf(\n",
    "#     \"D:\\\\WorkSpace\\\\LOL-PaperReader\\\\backend\\\\src\\\\paperreader\\\\services\\\\parser\\\\1810.04805v2.pdf\",\n",
    "#     \"my_custom_output\"\n",
    "# )\n",
    "\n",
    "# Option 3: Process multiple PDFs in a directory\n",
    "# result = process_multiple_pdfs(\n",
    "#     \"D:\\\\WorkSpace\\\\LOL-PaperReader\\\\backend\\\\src\\\\paperreader\\\\services\\\\parser\\\\\",\n",
    "#     \"batch_output\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f70f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da83f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperreader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
